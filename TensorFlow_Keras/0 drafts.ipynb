{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3 face=微软雅黑>在微积分中，函数  在点 $f(x)$ 上的导数定义为: $f'(x) = \\lim\\limits_{x \\to x_0}$ <font style=\"font-size:150%\">$\\frac{f(x) - f(x_0)}{x - x_0}$</font>，这在几何上指的是函数 $f(x)$ 在点 $x_0$ 处的切线方向。\n",
    "    \n",
    "当 $f'(x) = 0$ 时得到函数的极值点(临界点)。\n",
    "<br>\n",
    "<br>\n",
    "**但是临界点并不一定是全局最大值或者全局最小值，甚至不是局部的最大值或者局部最小值（如鞍点）。**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>从 Taylor 级数的角度来看，$f(x)$ 在 $x_0$ 附近的 Taylor 级数（皮亚诺余项）是：</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>若 $x_0$ 为临界点，则其满足条件：$f'(x) = 0$ 。\n",
    "<br><br>\n",
    "* 当 $f''(x) > 0$ 时，可以得到 $x_0$ 是 $f(x)$ 的局部最小值；\n",
    "<br><br>\n",
    "* 当 $f''(x) < 0$ 时，可以得到 $x_0$ 是 $f(x)$ 的局部最大值。\n",
    "<br><br>\n",
    "\n",
    "而对于例子 $f(x) = x^3$ 而言，临界点 0 的二阶导数则是 $f''(0) = 0$，因此使用上面的方法则无法判断临界点 0 是否是局部极值。\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>对于多元函数 $f(x) = f(x_1, ... , x_n)$ 而言，同样可以计算它们的“**导数**”，也就是偏导数和梯度。梯度可以定义为：</font>\n",
    "\n",
    "<font size=4 color=maroon>$$∇f(x) = (\\frac{∂f}{∂x_1}(x), ... , \\frac{∂f}{∂x_n}(x))$$</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>而多元函数 $f(x)$ 在点 $x_0$ 上的 Taylor 级数（皮亚诺余项）为：</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>其中 $H$ 表示<font color=maroon>黑塞矩阵（Hessian Matrix）</font>。如果 $x_0$ 为临界点，并且黑塞矩阵为<font color=maroon>正定矩阵</font>时，$f(x)$ 在 $x_0$ 处达到局部极小值。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>**梯度下降法基于以下定义**：如果实值函数 $f(x)$ 在点 $a$ 处可微且有定义，那么函数 $f(x)$ 在 $a$ 点沿着梯度相反的方向 <font color=maroon>$-∇f(a)$</font> 下降最多。\n",
    "\n",
    "<br>\n",
    "<br>  \n",
    "因而，如果 $b = a - γ∇f(a)$ 对于 $γ>0$ 且 $γ\\to0$ 时成立，那么 $f(a)\\ge f(b)$ 。值得注意的是<font color=maroon>迭代步长 $γ$ 并不是定值</font>。\n",
    "<br>\n",
    "<br>\n",
    "故我们可以从函数 $f$ 的局部极小值的<b>初始估计</b> $x_0$ 出发，并依次可得到如下序列 <font color=maroon>$(x_0, x_1, x_2, ...)$</font> 使得 $x_{n+1} = x_n - γ_n∇f(x_n), \\ \\ n \\ge 0$ 因此可得到 <font color=maroon>$f(x_0)\\ge f(x_1)\\ge f(x_2)\\ge ...$</font> ，如果顺利的话序列 $(x_n)$ 收敛到期望的局部极小值。</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3> 在神经网络中，梯度下降给调整网络参数提供了一种可行的方法。初始权重和偏置可解释为单个向量 <b><i>$θ_0$</i></b> ，理论上迭代步骤可以用来确定模型的最佳参数 <b><i>$θ^*$</i></b> 。实际上我们试图最小化的函数是根据整个数据集 $D$ 来定义的：\n",
    "<font color=maroon size=5>$$J(θ) = \\frac{1}{|D|}\\sum \\limits_{x ∈ D} f(x|θ)$$</font>\n",
    "\n",
    "其中 <font color=maroon>$f(x|θ)$ 表示使用参数向量 $θ$ 时，数据集元素 $x$ 的损失。</font>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "总的来说，梯度下降是一种通过更新模型中参数 $θ∈R^d$ 来最小化 $J(θ)$ 的方法（通过计算目标函数梯度 $∇_θJ(θ)$，并反向更新参数）。<font color=maroon>学习率 $η$ 决定了我们为了达到（局部）最小数而采取的跨度大小。</font></font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>其中：\n",
    "\n",
    "* $G_t ∈ ℝ^{d×d}$ 是一个**对角矩阵**，对角线上的元素 $(i,i)$ 是直到 $t$ 时刻为止，所有关于 $θ_i$ 的梯度平方和。\n",
    "\n",
    "* $ϵ$ 是一个平滑项，可以避免分母为零（通常 $ϵ$ 在 $10^{-8}$ 的数量级上）。\n",
    "\n",
    "<font color=maroon>Duchi 等人将该矩阵作为包含所有先前梯度的**外积**的**全矩阵** $G_t$ 的替代，因为计算全矩阵的平方根是不切实际的，尤其是在非常高的维度上（即使对于中等参数量，矩阵的均方根的计算都是不切实际的）。从另一方面看，计算平方根和对角线 $G_t$ 的倒数却能轻易实现。</font></font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3></font>\n",
    "\n",
    "<font size=3>$$</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x_0$\n",
    "$f(x)$\n",
    "$f'(x) = 0$\n",
    "\n",
    "$-∇f(a)$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
