{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=center><font color=maroon size=6 style=\"line-height:40px;\"><b>Visualizing Data using the Embedding Projector<br> in TensorBoard</b></font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4><b>References:</b></font>\n",
    "1. TensorFlow > <a href=\"https://www.tensorflow.org/resources\" style=\"text-decoration:none;\">Resources</a> \n",
    "    * `TensorFlow > Resources > TensorBoard > Guide > `<a href=\"https://www.tensorflow.org/tensorboard/tensorboard_projector_plugin\" style=\"text-decoration:none;\">Visualizing Data using the Embedding Projector in TensorBoard</a>\n",
    "        * Run in <a href=\"https://colab.research.google.com/github/tensorflow/tensorboard/blob/master/docs/tensorboard_projector_plugin.ipynb\" style=\"text-decoration:none;\">Google Colab</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Using the **TensorBoard Embedding Projector**, you can graphically represent high dimensional embeddings. This can be helpful in visualizing, examining, and understanding your embedding layers.\n",
    "\n",
    "<!-- <img src=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/images/embedding.jpg?raw=\\\" alt=\"Screenshot of the embedding projector\" width=\"400\"/> -->\n",
    "\n",
    "In this tutorial, you will learn how visualize this type of trained layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "For this tutorial, we will be using TensorBoard to visualize an embedding layer generated for classifying movie review data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\Anaconda3\\envs\\tfg\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorboard.plugins import projector\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB Data \n",
    "\n",
    "We will be using a dataset of 25,000 IMDB movie reviews, each of which has a sentiment label (positive/negative). Each review is preprocessed and encoded as a sequence of word indices (integers). <font style=\"color:maroon;font-size:120%\">**For simplicity, words are indexed by overall frequency in the dataset, for instance the integer \"3\" encodes the 3rd most frequent word appearing in all reviews**. This allows for quick filtering operations such as:</font> \"only consider the top 10,000 most common words, but eliminate the top 20 most common words\".\n",
    "\n",
    "<font style=\"color:maroon;font-size:110%\">As a convention, **\"0\"** does not stand for any specific word, but instead is used to encode any unknown word</font>. Later in the tutorial, we will remove the row for \"0\" in the visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:TFDS datasets with text encoding are deprecated and will be removed in a future version. Instead, you should use the plain text version and tokenize the text using `tensorflow_text` (See: https://www.tensorflow.org/tutorials/tensorflow_text/intro#tfdata_example)\n"
     ]
    }
   ],
   "source": [
    "(train_data, test_data), info = tfds.load(\"imdb_reviews/subwords8k\",\n",
    "                                          split=(tfds.Split.TRAIN, tfds.Split.TEST),\n",
    "                                          with_info=True,\n",
    "                                          as_supervised=True,\n",
    "                                          data_dir=\"D:/KeepStudy/0_Coding/0_dataset/tensorflow_datasets/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SubwordTextEncoder vocab_size=8185>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = info.features[\"text\"].encoder\n",
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle and pad the data\n",
    "train_batches = train_data.shuffle(1000).padded_batch(10, \n",
    "                                                      padded_shapes=((None, ), ())  # () 对应 labels\n",
    "                                                     )\n",
    "\n",
    "test_batches = test_data.shuffle(1000).padded_batch(10,\n",
    "                                                    padded_shapes=((None, ), ())\n",
    "                                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch, train_labels = next(iter(train_batches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Embedding Layer\n",
    "\n",
    "<font style=\"color:maroon;font-size:110%\">A [Keras Embedding Layer](https://keras.io/layers/embeddings/) can be used to train an embedding for each word in your vocabulary. Each word (or sub-word in this case) will be associated with a 16-dimensional vector (or embedding) that will be trained by the model.\n",
    "\n",
    "See [this tutorial](https://www.tensorflow.org/tutorials/text/word_embeddings?hl=en) to learn more about word embeddings.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an embedding layer.\n",
    "embedding_dim = 16\n",
    "embedding = tf.keras.layers.Embedding(encoder.vocab_size, embedding_dim)\n",
    "\n",
    "\n",
    "# Configure the embedding layer as part of a keras model.\n",
    "model = tf.keras.Sequential([embedding,  # The embedding layer should be the first layer in a model.\n",
    "                             tf.keras.layers.GlobalAveragePooling1D(),\n",
    "                             tf.keras.layers.Dense(16, activation='relu'),\n",
    "                             tf.keras.layers.Dense(1)])\n",
    "\n",
    "\n",
    "# Compile model.\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500/2500 [==============================] - 11s 4ms/step - loss: 0.5057 - accuracy: 0.6979 - val_loss: 0.3415 - val_accuracy: 0.8350\n"
     ]
    }
   ],
   "source": [
    "# Train model for one epoch.\n",
    "history = model.fit(train_batches, \n",
    "                    epochs=1, \n",
    "                    validation_data=test_batches, \n",
    "                    validation_steps=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Data for TensorBoard\n",
    "\n",
    "<font style=\"color:maroon;font-size:110%\">TensorBoard reads tensors and metadata from the logs of your tensorflow projects.</font> The path to the log directory is specified with `log_dir` below. For this tutorial, we will be using `/logs/imdb-example/`.\n",
    "\n",
    "<font style=\"color:maroon;font-size:110%\">In order to load the data into Tensorboard, we need to save a training checkpoint to that directory, along with metadata that allows for visualization of a specific layer of interest in the model. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected object of type bytes or bytearray, got: <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# pip install chardet\n",
    "\n",
    "import chardet\n",
    "\n",
    "try:\n",
    "    for subwords in encoder.subwords[10]:\n",
    "        print(chardet.detect(subwords))\n",
    "        # 报错：\n",
    "        # TypeError: Expected object of type bytes or bytearray, got: <class 'str'>\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a logs directory, so Tensorboard knows where to look for files.\n",
    "log_dir = './logs/imdb-example/'\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(encoder)\n",
    "# help(open)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Labels separately on a line-by-line manner.\n",
    "with open(os.path.join(log_dir, 'metadata.tsv'), 'w', encoding='utf-8') as f:\n",
    "    for subwords in encoder.subwords:\n",
    "        f.write(\"{}\\n\".format(subwords))\n",
    "        \n",
    "    # Fill in the rest of the labels with 'unknown'.\n",
    "    for unknown in range(1, encoder.vocab_size - len(encoder.subwords)):\n",
    "        f.write(\"unknown #{}\\n\".format(unknown))\n",
    "\n",
    "\n",
    "# 需要在 open() 里加上参数 encoding='utf-8'\n",
    "# 否则会报错：\n",
    "# UnicodeEncodeError: 'gbk' codec can't encode character '\\x96' in position 1: illegal multibyte sequence\n",
    "# \n",
    "# 参考链接：https://www.cnblogs.com/schut/p/10579955.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./logs/imdb-example/embedding.ckpt-1'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the weights we want to analyze as a variable.\n",
    "# Note that the first value represents any unknown word, which is not in the metadata,\n",
    "# here we will remove this value.\n",
    "weights = tf.Variable(model.layers[0].get_weights()[0][1:])\n",
    "\n",
    "# Create a checkpoint from embedding, the filename and key are the name of the tensor.\n",
    "checkpoint = tf.train.Checkpoint(embedding=weights)\n",
    "checkpoint.save(os.path.join(log_dir, \"embedding.ckpt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up config.\n",
    "config = projector.ProjectorConfig()\n",
    "embedding = config.embeddings.add()\n",
    "\n",
    "# The name of the tensor will be suffixed by `/.ATTRIBUTES/VARIABLE_VALUE`\n",
    "embedding.tensor_name = \"embedding/.ATTRIBUTES/VARIABLE_VALUE\"\n",
    "embedding.metadata_path = \"metadata.tsv\"\n",
    "projector.visualize_embeddings(log_dir, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls -al ./logs/imdb-example/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 5820), started 0:27:41 ago. (Use '!kill 5820' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-93342ac8bafb6cfc\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-93342ac8bafb6cfc\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now run tensorboard against on log data we just saved.\n",
    "\n",
    "%tensorboard --logdir log_dir\n",
    "\n",
    "# %tensorboard --logdir logs/imdb-example/\n",
    "# 也可在浏览器中输入 http://localhost:6006 查看\n",
    "\n",
    "# %tensorboard --logdir=logs/imdb-example/\n",
    "# 也可在浏览器中输入 http://localhost:6006 查看\n",
    "\n",
    "# 或者下面这句：\n",
    "# %tensorboard --logdir=logs/imdb-example/ --port=8008\n",
    "# 也可在浏览器中输入 http://localhost:8008 查看"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "The TensorBoard Projector is a great tool for interpreting and visualzing embedding. The dashboard allows users to search for specific terms, and highlights words that are adjacent to each other in the embedding (low-dimensional) space. From this example we can see that Wes **Anderson** and Alfred **Hitchcock** are both rather neutral terms, but that they are referenced in different contexts.\n",
    "\n",
    "<!-- <img class=\"tfo-display-only-on-site\" src=\"images/embedding_projector_hitchcock.png?raw=1\"/> -->\n",
    "\n",
    "In this space, Hitchcock is closer to words like `nightmare`, which is likely due to the fact that he is known as the \"Master of Suspense\", whereas Anderson is closer to the word `heart`, which is consistent with his relentlessly detailed and heartwarming style.\n",
    "\n",
    "<!-- <img class=\"tfo-display-only-on-site\" src=\"images/embedding_projector_anderson.png?raw=1\"/> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font style=\"color:maroon;font-size:110%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Copyright 2019 The TensorFlow Authors.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tfg]",
   "language": "python",
   "name": "conda-env-tfg-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
