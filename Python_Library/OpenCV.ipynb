{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenCV is a library used for computer vision. It has more functionality than the `PIL` library but is more difficult to use. We can import `OpenCV` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imgcodecs module: Image file reading and writing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load an image\n",
    "\n",
    "<br>\n",
    "\n",
    "**`cv2.imread(filename[, flags])`**\n",
    "* Loads an image from a file. \n",
    "* The result is a **numpy array** with intensity values as 8-bit unsigned integers.\n",
    "\n",
    "\n",
    "* `filename`: Name of file to be loaded.\n",
    "* `flags`: Flag that can take values of `cv2::ImreadModes`, and the default value is `cv2.IMREAD_COLOR`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(\"./images/lenna.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(image)    # numpy.ndarray\n",
    "image.shape\n",
    "image.max()\n",
    "image.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also load in a grayscale image we have to set flag parameter to gray color conversation code: **`cv2.COLOR_BGR2GRAY`**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_gray = cv2.imread('./images/barbara.png', cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_gray = cv2.imread('./images/barbara.png', cv2.IMREAD_GRAYSCALE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## numpy.ndarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Âõ†‰∏∫ cv2.imread() ÁöÑËøîÂõûÂÄºÊòØ <font style=\"color:magenta;font-size:120%;\">numpy.ndarray</font> Á±ªÂûã„ÄÇÊâÄ‰ª•ÂèØ‰ª•Âà©Áî® numpy.ndarray ÁöÑÁâπÊÄßÊù•ËøõË°åÂõæÂÉèÁöÑÔºö\n",
    "* **Copy**\n",
    "* **Indexing**\n",
    "* **Slicing**\n",
    "* **Changing Specific Image Pixels**\n",
    "* **Cropping**\n",
    "* **Flipping**\n",
    "* **Intensity Transformations**\n",
    "* ËØ∏Â¶ÇÊ≠§Á±ªÔºàËØ¶ËßÅ‰∏ãÈù¢4.1 ÊàñËÄÖ PIL.ipynbÔºö1.4 PIL Images into Numpy ArraysÔºâ\n",
    "\n",
    "<font style=\"color:red;font-size:120%;\">ÁâπÂà´Âú∞Ôºö</font> If you want to reassign an array to another variable, you should use the **`copy()`** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper = 150\n",
    "lower = 400\n",
    "left = 150\n",
    "right = 400\n",
    "\n",
    "# Changing Specific Image Pixels\n",
    "array_sq = np.copy(image)\n",
    "array_sq[upper:lower,left:right,:] = 0\n",
    "\n",
    "plt.imshow( cv2.cvtColor(array_sq, cv2.COLOR_BGR2RGB) )\n",
    "plt.title(\"Altered Image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cropping\n",
    "crop_top = image[upper:lower, :, :]\n",
    "plt.figure(figsize=(3,3))\n",
    "plt.imshow(cv2.cvtColor(crop_top, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font style=\"font-size:140%;\">Image Negatives</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider an image with $L$ intensity values ranging from $[0,L-1]$.  We can reverse the intensity levels by applying the following:\n",
    "$$\n",
    "g(x,y)=L-1-f(x,y)\n",
    "$$\n",
    "\n",
    "Using the intensity transformation function notation\n",
    "$$\n",
    "s = L - 1 - r\n",
    "$$\n",
    "\n",
    "This is called the image negative. For $L= 256$ the formulas simplifys to:\n",
    "$$\n",
    "g(x,y)=255-f(x,y) \\qquad \\mbox{and} \\qquad s=255-r\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reversing image intensity has many applications, including making it simpler to analyze medical images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_image = np.array([[0,2,2],[1,1,1],[1,1,2]], dtype=np.uint8)\n",
    "\n",
    "plt.imshow(toy_image, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(\"toy_image:\",toy_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intensity Transformations ‰πã Image Negatives\n",
    "neg_toy_image = -1 * toy_image + 255\n",
    "\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.subplot(1, 2, 1) \n",
    "plt.imshow(toy_image,cmap=\"gray\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(neg_toy_image,cmap=\"gray\")\n",
    "plt.show()\n",
    "print(\"toy_image:\",toy_image)\n",
    "\n",
    "# Intensity Transformations ‰πã Histogram Equalization ËßÅ 3.3.2\n",
    "# Intensity Transformations ‰πã Brightness and contrast adjustments ËßÅ 4.1.4\n",
    "#  Intensity Transformations ‰πã Thresholding and Simple Segmentation ËßÅ 3.4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add noise\n",
    "cols, rows, _ = image.shape\n",
    "noise = np.random.normal(0, 20, (rows, cols, 3)).astype(np.uint8)\n",
    "noisy_image = image + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Singular Value Decomposition of an Image\n",
    "im_gray = cv2.imread('./images/barbara.png', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "U, s, V = np.linalg.svd(im_gray , full_matrices=True)\n",
    "\n",
    "# convert s to a diagonal matrix S:\n",
    "S = np.zeros((im_gray.shape[0], im_gray.shape[1]))\n",
    "S[:image.shape[0], :image.shape[0]] = np.diag(s)\n",
    "\n",
    "B = S.dot(V)\n",
    "A = U.dot(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_component in [1,10,100,200, 500]:\n",
    "    S_new = S[:, :n_component]\n",
    "    V_new = V[:n_component, :]\n",
    "    A = U.dot(S_new.dot(V_new))\n",
    "    \n",
    "    plt.figure(figsize=(3,3))\n",
    "    plt.imshow(A,cmap='gray')\n",
    "    plt.title(\"Number of Components:\"+str(n_component))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Color Channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can obtain the different **`RGB`** colors and assign them to the variables `blue`, `green`, and `red`, in **`(B, G, R)`** format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blue, green, red = image[:, :, 0], image[:, :, 1], image[:, :, 2]\n",
    "\n",
    "# ÊàñËÄÖ\n",
    "blue, green, red = cv2.split(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can concatenate each image channel the images using the function **`cv2.vconcat()`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_bgr = cv2.vconcat([blue, green, red])\n",
    "\n",
    "\n",
    "# Plotting the color image next to the red channel in grayscale.\n",
    "# We see that regions with red have higher intensity values.\n",
    "plt.figure(figsize=(4,4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "plt.title(\"RGB image\")\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow(im_bgr,cmap='gray')\n",
    "plt.title(\"Different color channels  blue (top), green (middle), red (bottom)  \")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save an image\n",
    "\n",
    "<br>\n",
    "\n",
    "**`cv2.imwrite(filename, img[, params])`**\n",
    "* Saves an image to a specified file.\n",
    "* `filename`: Name of the file.\n",
    "* `img`: (Mat or vector of Mat) Image or Images to be saved.\n",
    "* `params`: Format-specific parameters encoded as pairs (paramId_1, paramValue_1, paramId_2, paramValue_2, ... .), see `cv::ImwriteFlags`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save the image as in jpg format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite(\"./images/lenna.jpg\", image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# highgui module: High-level GUI\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting an Image\n",
    "\n",
    "<br>\n",
    "\n",
    "### cv2.imshow()\n",
    "You can use OpenCV's **`cv2.imshow()`** function to open the image in a `new window`, but this may give you some issues in Jupyter.\n",
    "\n",
    "<br>\n",
    "\n",
    "**`cv2.imshow(winname, mat)`**\n",
    "* Displays an image in the specified window.\n",
    "* `winname`: Name of the window.\n",
    "* `mat`: Image to be shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('image', image)    # Âè¶ÂºπÂá∫‰∏Ä‰∏™ÊòæÁ§∫Á™óÂè£„ÄÇÊòæÁ§∫Âá∫Êù•ÁöÑÊòØÊ≠£Â∏∏ÁöÑ RGB ÂõæÂÉè\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### plt.imshow()\n",
    "\n",
    "We can also use the **`pyplot.imshow()`** function from the matplotlib library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(3,3))\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image output doesn't look natural. This is because the order of RGB Channels are different. We can change the color space with conversion code and the function **`cv2.cvtColor`** from the `cv2` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_image=cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.figure(figsize=(3,3))\n",
    "plt.imshow(new_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ê≥®ÊÑèÔºö\n",
    "# ‰ΩøÁî® cv2.imshow('image', image) Êù•ÊòæÁ§∫ÁöÑËØù‰∏çÈúÄË¶Å cvtColor Êù•ËΩ¨Êç¢\n",
    "# ‰ΩøÁî® plt.imshow(new_image) Êù•ÊòæÁ§∫ÁöÑËØùÔºåÂ∞±ÈúÄË¶Å cvtColor Êù•Â∞Ü BGR ËΩ¨Êç¢‰∏∫ RGB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some other functions in the highhui module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font style=\"font-family:Consolas;font-size:130%\">cv2.waitKey()</font>\n",
    "<br>\n",
    "\n",
    "* <font style=\"font-family:Consolas;font-size:130%\">cv2.destroyAllWindows()</font>\n",
    "<br>\n",
    "\n",
    "* <font style=\"font-family:Consolas;font-size:130%\">cv2.getWindowImageRect()</font>\n",
    "<br>\n",
    "* <font style=\"font-family:Consolas;font-size:130%\">cv2.resizeWindow()</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imgproc module: Image Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Color Space Conversions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cv2.cvtColor()\n",
    "<br>\n",
    "\n",
    "**`\tcv2.cvtColor(src, code[, dst[, dstCn]])`**\n",
    "* Converts an image from one color space to another.\n",
    "* `src`: input image: 8-bit unsigned, 16-bit unsigned ( CV_16UC... ), or single-precision floating-point.\n",
    "* `dst`: output image of the same size and depth as src.\n",
    "* `code`: color space conversion code (see **`3.1.2 ColorConversionCodes`**).\n",
    "* `dstCn`: number of channels in the destination image; if the parameter is 0, the number of the channels is derived automatically from src and code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conventional ranges for R, G, and B channel values are:\n",
    "* 0 to 255 for CV_8U images\n",
    "* 0 to 65535 for CV_16U images\n",
    "* 0 to 1 for CV_32F images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code for RGB to gray is `cv2.COLOR_BGR2GRAY`, we apply the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can plot the image using imshow but we have to specify the color map is gray:\n",
    "plt.figure(figsize=((3,3)))\n",
    "plt.imshow(image_gray, cmap='gray')   # cmap='gray' Â¶ÇÊûú‰∏çÂä†ÁöÑËØù‰ºöÂØπÊòæÁ§∫ÊúâÂΩ±Âìç\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save the image as a grayscale image, let's save it as a jpg as well, in the working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite('./images/lena_gray_cv.jpg', image_gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Color Conversion Codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ËøôÈáåÂè™ÊëòÂΩï‰∏ÄÈÉ®ÂàÜÔºö"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* cv2.COLOR_BGR2RGB\n",
    "* cv2.COLOR_RGB2BGR\n",
    "* \n",
    "* cv2.COLOR_BGR2GRAY\n",
    "* cv2.COLOR_RGB2GRAY\n",
    "* \n",
    "* cv2.COLOR_GRAY2BGR\n",
    "* cv2.COLOR_GRAY2RGB\n",
    "* \n",
    "* cv2.COLOR_BGR2XYZ\n",
    "* cv2.COLOR_RGB2XYZ\n",
    "* \n",
    "* cv2.COLOR_BGR2HSV\n",
    "* cv2.COLOR_RGB2HSV\n",
    "* \n",
    "* cv2.COLOR_BGR2Luv\n",
    "* cv2.COLOR_RGB2Luv\n",
    "* \n",
    "* cv2.COLOR_BGR2HLS\n",
    "* cv2.COLOR_RGB2HLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drawing Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cv2.rectangle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`cv2.rectangle( img, pt1, pt2, color[, thickness[, lineType[, shift]]] )  -> img`**\n",
    "* Draws a simple, thick, or filled up-right rectangle whose two opposite corners are pt1 and pt2.\n",
    "* `pt1`: Vertex of the rectangle.\n",
    "* `pt2`: Vertex of the rectangle opposite to pt1 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_point, end_point = (left, upper),(right, lower)\n",
    "image_draw = np.copy(image)\n",
    "cv2.rectangle(image_draw, \n",
    "              pt1=start_point, pt2=end_point,\n",
    "              color=(0, 255, 0), thickness=3) \n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow( cv2.cvtColor(image_draw, cv2.COLOR_BGR2RGB) )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cv2.putText()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`cv2.putText( img, text, org, fontFace, fontScale, color`**<br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; **`[, thickness[, lineType[, bottomLeftOrigin]]] ) -> img`**\n",
    "* Draws a text string. The functions renders the specified text string in the image. Symbols that cannot be rendered using the specified font are replaced by question marks. See `getTextSize` for a text rendering code example.\n",
    "* `org`: Bottom-left corner of the text string in the image.\n",
    "* `fontFace`: Font type, see `HersheyFonts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_draw = cv2.putText(img=image, text='Stuff',\n",
    "                         org=(10,500), color=(255,255,255),\n",
    "                         fontFace=4, fontScale=5, thickness=2)\n",
    "# ÊòØÁõ¥Êé•Âú®ÂéüÂõæÂÉè‰∏ä‰øÆÊîπÁöÑÔºåÊïÖ image ÊòæÁ§∫Âá∫Êù•ÁöÑÂíå image_draw ‰∏ÄÊ†∑„ÄÇ\n",
    "plt.figure(figsize=(3,3))\n",
    "plt.imshow( cv2.cvtColor(image_draw, cv2.COLOR_BGR2RGB) )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \tHistograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cv2.calcHist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`cv2.calcHist( images, channels, mask, histSize, ranges[, hist[, accumulate]] ) -> hist`**\n",
    "* Calculates a histogram of a set of arrays. The elements of a tuple used to increment a histogram bin are taken from the corresponding input arrays at the same location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `images`: Source arrays. They all should have the same depth, CV_8U, CV_16U or CV_32F , and the same size. Each of them can have an arbitrary number of channels.\n",
    "* `channels`: List of the dims channels used to compute the histogram. \n",
    "    * The first array channels are numerated from 0 to `images[0].channels() - 1`.\n",
    "    * The second array channels are counted from `images[0].channels()` to `images[0].channels() + images[1].channels() - 1`, and so on.\n",
    "* `mask`: Optional mask. If the matrix is not empty, it must be an 8-bit array of the same size as images[i] . The non-zero mask elements mark the array elements counted in the histogram. *(default?: None)*\n",
    "* `histSize`: Array of histogram sizes in each dimension. *(ÊàñËÄÖ?Ôºöthe number of bins: [L])*\n",
    "* `ranges`: Array of the dims arrays of the histogram bin boundaries in each dimension. *(ÊàñËÄÖ?Ôºöthe range of index of bins: [0,L-1])*\n",
    "* `hist`: Output histogram, which is a dense or sparse dims -dimensional array.\n",
    "\n",
    "\n",
    "For real images, L is 256."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histograms are used in grayscale images. Grayscale images are used in many applications, including medical and industrial. Color images are split into luminance and chrominance. The luminance is the grayscale portion and is usually processed in many applications. Consider the following \"Gold Hill\" image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goldhill = cv2.imread(\"images/goldhill.bmp\", cv2.IMREAD_GRAYSCALE)  #goldhillÊòØÁÅ∞Â∫¶ÁÖßÁâá\n",
    "\n",
    "hist = cv2.calcHist([goldhill], [0], None, [256], [0,256])\n",
    "\n",
    "intensity_values = np.array([x for x in range(hist.shape[0])])\n",
    "plt.bar(intensity_values, hist[:,0], width = 5)\n",
    "plt.title(\"Bar histogram\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can convert it to a probability mass function by normalizing it by the number of pixels and plot as a continuous function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PMF = hist / (goldhill.shape[0] * goldhill.shape[1])  # Âõ† goldhill ÊòØ np.ndarray\n",
    "plt.plot(intensity_values, hist)   # ËøôÈáåÁî®ÁöÑÊòØ Plt.plotÔºå‰∏äÈù¢‰∏Ä‰∏™ cell Áî®ÁöÑÊòØ plt.bar\n",
    "plt.title(\"histogram\")\n",
    "plt.show()\n",
    "\n",
    "type(hist)     # np.ndarray \n",
    "type(PMF)      # np.ndarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also apply a histogram to each image color channel. In the loop, the value for `i` specifies what color channel `calcHist` is going to calculate the histogram for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baboon = cv2.imread(\"images/baboon.png\")\n",
    "color = ('blue','green','red')\n",
    "for i, col in enumerate(color):   # 3‰∏™È¢úËâ≤ÁöÑ histogram ÁîªÂú®Âêå‰∏ÄÂπÖÂõæ‰∏≠\n",
    "    histr = cv2.calcHist([baboon], [i], None, [256], [0,256])\n",
    "    plt.plot(intensity_values, histr, color = col, label=col+\" channel\")\n",
    "    \n",
    "plt.xlim([0,256])\n",
    "plt.legend()\n",
    "plt.title(\"Histogram Channels\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cv2.equalizeHist()\n",
    "\n",
    "&emsp;&emsp;&emsp;&ensp; <font style=\"font-size:120%;color:red;\">Histogram Equalization</font>\n",
    "\n",
    "Histogram Equalization increases the contrast of images, by stretching out the range of the grayscale pixels; It does this by flatting the histogram. We simply apply the function `cv2.equalizeHist`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`cv2.equalizeHist( src[, dst] ) -> dst`**\n",
    "* Equalizes the histogram of a grayscale image.\n",
    "* `src`: Source 8-bit single channel image.\n",
    "* `dst`: Destination image of the same size and type as src ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/cv2_equalizeHist.png\" width=640px; align=left>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zelda = cv2.imread(\"images/zelda.png\", cv2.IMREAD_GRAYSCALE)\n",
    "zelda_new = cv2.equalizeHist(zelda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geometric Image Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cv2.resize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`cv2.resize( src, dsize[, dst[, fx[, fy[, interpolation]]]] ) -> dst`**\n",
    "* Resizes an image. *(Êõ¥ËØ¶ÁªÜÁöÑËØ¥ÊòéËßÅÂÆòÊñπÊñáÊ°£)*\n",
    "* \n",
    "* `dsize`: output image size; if it equals zero (None in Python), it is computed as: \n",
    "    * \n",
    "    * **`dsize = Size( round(fx*src.cols),  round(fy*src.rows) )`**\n",
    "    *\n",
    "    * Either dsize or both fx and fy must be non-zero.\n",
    "* \n",
    "* `fx`: scale factor along the horizontal axis; when it equals 0, it is computed as: **`(double)dsize.width/src.cols`**\n",
    "* \n",
    "* `fy`: scale factor along the vertical axis; when it equals 0, it is computed as: **`(double)dsize.height/src.rows`**\n",
    "* \n",
    "* `interpolation`: interpolation method, see **`InterpolationFlags`**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To shrink an image, it will generally look best with **`cv2.INTER_AREA`** interpolation, whereas to enlarge an image, it will generally look best with **`cv2.INTER_CUBIC`**` (slow)` or **`cv2.INTER_LINEAR`**` (faster but still looks OK)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_image = np.zeros((6,6))\n",
    "toy_image[1:5,1:5]=255\n",
    "toy_image[2:4,2:4]=0\n",
    "\n",
    "new_toy = cv2.resize(toy_image, None, fx=2, fy=1, \n",
    "                     interpolation = cv2.INTER_NEAREST )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also specify the number of rows and columns:\n",
    "new_image = cv2.resize(image, (100, 200), interpolation=cv2.INTER_CUBIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### InterpolationFlags\n",
    "&emsp;&emsp;&emsp;&ensp; <font style=\"font-size:120%;color:red;\">Interpolation algorithm</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **cv2.INTER_NEAREST**\n",
    "* **cv2.INTER_LINEAR**\n",
    "* **cv2.INTER_CUBIC**\n",
    "* **cv2.INTER_AREA**\n",
    "* **cv2.INTER_MAX**\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cv2.warpAffine()\n",
    "&emsp;&emsp;&emsp;&ensp; <font style=\"font-size:120%;color:red;\">Translation</font>\n",
    "\n",
    "Translation is when you shift the location of the image. We can create the transformation matrix  ùëÄ  to shift the image:\n",
    "* `tx` is the number of pixels you shift the location in the horizontal direction.\n",
    "* `ty` is the number of pixels you shift in the vertical direction.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx = 100\n",
    "ty = 0\n",
    "M = np.float32([[1, 0, tx], [0, 1, ty]])\n",
    "\n",
    "rows, cols, _ = image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`cv2.warpAffine(src, M, dsize[, dst[, flags[, borderMode[, borderValue]]]]) -> dst`**\n",
    "\n",
    "<br>\n",
    "\n",
    "* Applies an affine transformation to an image. The function warpAffine transforms the source image using the specified matrix: \n",
    "    * \n",
    "    * $dst(x,y) = src ( M_{11}x+M_{12}y+M_{13}, M_{21}x+M_{22}y+M_{23} )$ &ensp; when the flag **`WARP_INVERSE_MAP`** is set. \n",
    "    * \n",
    "    * Otherwise, the transformation is first inverted with **`invertAffineTransform`** and then put in the formula above instead of M. The function cannot operate in-place.\n",
    "* \n",
    "* `src`: input image.\n",
    "* `dst`: output image that has the size *dsize* and the same type as *src* .\n",
    "* `M`: 2√ó3 transformation matrix.\n",
    "* `dsize`: size of the output image.\n",
    "* `flags`: combination of interpolation methods (see **`InterpolationFlags`**) and the optional flag **`WARP_INVERSE_MAP`** that means that M is the inverse transformation ( dst‚Üísrc ).\n",
    "* `borderMode`: pixel extrapolation method (see **`BorderTypes`**); when `borderMode=`**`cv2.BORDER_TRANSPARENT`**, it means that the pixels in the destination image corresponding to the \"outliers\" in the source image are not modified by the function.\n",
    "* `borderValue`: value used in case of a constant border; by default, it is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_image = cv2.warpAffine(image, M, (cols, rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cv2.getRotationMatrix2D()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`cv2.getRotationMatrix2D( center, angle, scale ) ->  retval`**\n",
    "* Calculates an affine matrix of 2D rotation.\n",
    "* `center`: Center of the rotation in the source image.\n",
    "* `angle`: Rotation angle in degrees. Positive values mean counter-clockwise rotation (the coordinate origin is assumed to be the top-left corner).\n",
    "* `scale`: Isotropic scale factor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/cv2_getRotationMatrix2D.png\" width=780px; align=left>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = 45.0\n",
    "M = cv2.getRotationMatrix2D(center=(3, 3), angle=theta, scale=1)\n",
    "new_toy_image = cv2.warpAffine(toy_image, M, (6, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can perform the same operation on color images:\n",
    "cols, rows, _ = image.shape\n",
    "M = cv2.getRotationMatrix2D(center=(cols // 2 - 1, rows // 2 - 1), \n",
    "                            angle=theta, scale=1)\n",
    "new_image = cv2.warpAffine(image, M, (cols, rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Filtering\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/cv2_Image_Filtering_Detailed_Description.png\">\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cv2.filter2D()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`cv.filter2D( src, ddepth, kernel[, dst[, anchor[, delta[, borderType]]]] ) -> dst`**\n",
    "* Convolves an image with the kernel.\n",
    "* `src`: input image.\n",
    "* `dst`: output image of the same size and the same number of channels as src.\n",
    "* `ddepth`: desired depth of the destination image, see **`combinations`**\n",
    "* `kernel`: convolution kernel (or rather a correlation kernel), a single-channel floating point matrix; if you want to apply different kernels to different channels, split the image into separate color planes using split and process them individually.\n",
    "* `anchor`: anchor of the kernel that indicates the relative position of a filtered point within the kernel; the anchor should lie within the kernel; default value (-1,-1) means that the anchor is at the kernel center.\n",
    "* `delta`: optional value added to the filtered pixels before storing them in dst.\n",
    "* `borderType`: pixel extrapolation method, see **`BorderTypes`**. **`BORDER_WRAP`** is not supported."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function applies an arbitrary linear filter to an image. In-place operation is supported. When the aperture is partially outside the image, the function interpolates outlier pixel values according to the specified border mode.</br>\n",
    "\n",
    "The function does actually compute correlation, not the convolution:</br>\n",
    "\n",
    "<img src=\"images/cv2_filter2D.png\" width=720px; align=center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, the kernel is not mirrored around the anchor point. If you need a real convolution, flip the kernel using flip and set the new anchor to **`(kernel.cols - anchor.x - 1, kernel.rows - anchor.y - 1)`**.\n",
    "\n",
    "The function uses the DFT-based algorithm in case of sufficiently large kernels (~`11 x 11` or larger) and the direct algorithm for small kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = np.ones((6,6))/36\n",
    "image_filtered = cv2.filter2D(src=noisy_image, ddepth=-1, kernel=kernel)\n",
    "# ÂèÇÊï∞ noisy_image ËßÅ 1.2 ÈáåÁöÑ # add noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Sharpening \n",
    "# Image Sharpening involves smoothing the image and calculating the derivatives. \n",
    "# We can accomplish image sharpening by applying the following Kernel.\n",
    "\n",
    "# Common Kernel for image sharpening\n",
    "kernel = np.array([[-1,-1,-1], \n",
    "                   [-1, 9,-1],\n",
    "                   [-1,-1,-1]])\n",
    "# Applys the sharpening filter using kernel on the original image without noise\n",
    "sharpened = cv2.filter2D(image, -1, kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cv2.medianBlur()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`cv.medianBlur( src, ksize[, dst] ) -> dst`**\n",
    "* Blurs an image using the median filter. The function smoothes an image using the median filter with the ksize√óksize aperture. Each channel of a multi-channel image is processed independently. In-place operation is supported.\n",
    "* `src`: input 1-, 3-, or 4-channel image; when ksize is 3 or 5, the image depth should be CV_8U, CV_16U, or CV_32F, for larger aperture sizes, it can only be CV_8U.\n",
    "* `dst`: destination array of the same size and type as src.\n",
    "* `ksize`: aperture linear size; it must be odd and greater than 1, for example: 3, 5, 7 ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See also: `bilateralFilter, blur, boxFilter, GaussianBlur`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the image using Median Blur with a kernel of size 5\n",
    "filtered_image = cv2.medianBlur(image, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cv2.GaussianBlur()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`cv.GaussianBlur(src, ksize, sigmaX[, dst[, sigmaY[, borderType]]]) -> dst`**\n",
    "* Blurs an image using a Gaussian filter. The function convolves the source image with the specified Gaussian kernel. In-place filtering is supported.\n",
    "* `src`: input image; the image can have any number of channels, which are processed independently, but the depth should be CV_8U, CV_16U, CV_16S, CV_32F or CV_64F.\n",
    "* `dst`: output image of the same size and type as src.\n",
    "* `ksize`: Gaussian kernel size. ksize.width and ksize.height can differ but they both must be positive and odd. Or, they can be zero's and then they are computed from sigma.\n",
    "* `sigmaX`: Gaussian kernel standard deviation in X direction.\n",
    "* `sigmaY`: Gaussian kernel standard deviation in Y direction; if sigmaY is zero, it is set to be equal to sigmaX, if both sigmas are zeros, they are computed from ksize.width and ksize.height, respectively (see **`getGaussianKernel`** for details); to fully control the result regardless of possible future modifications of all this semantics, it is recommended to specify all of ksize, sigmaX, and sigmaY.\n",
    "* `borderType`: pixel extrapolation method, see **`BorderTypes`**. **`BORDER_WRAP`** is not supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_filtered = cv2.GaussianBlur(noisy_image, (5,5), sigmaX=4, sigmaY=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigma behaves like the size of the mean filter, a larger value of sigma will make the image blurry, but you are still constrained by the size of the filter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cv2.Sobel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`cv2.Sobel( src, ddepth, dx, dy[, dst[, ksize[, scale[, delta[, borderType]]]]] ) -> dst`**\n",
    "* Calculates the first, second, third, or mixed image derivatives using an extended Sobel operator.\n",
    "* `src`: input image.\n",
    "* `ddepth`: output image depth, see **`combinations`** *`(ËßÅ‰∏ãÈù¢Ë°®Ê†ºÔºöDepth combinations)`*; in the case of 8-bit input images it will result in truncated derivatives.\n",
    "* `dx`: order of the derivative x.\n",
    "* `dy`: order of the derivative y.\n",
    "* `dst`: output image of the same size and the same number of channels as src .\n",
    "* `ksize`: size of the extended Sobel kernel; it must be 1, 3, 5, or 7.\n",
    "* `scale`: optional scale factor for the computed derivative values; by default, no scaling is applied. (see **`getDerivKernels`** for details)\n",
    "* `delta`: optional delta value that is added to the results prior to storing them in dst.\n",
    "* `borderType`: pixel extrapolation method, see **`BorderTypes`**. **`BORDER_WRAP`** is not supported."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all cases except one, the **`ksize`**`√ó`**`ksize`** separable kernel is used to calculate the derivative. When **ksize = 1**, the **3√ó1** or **1√ó3** kernel is used (that is, no Gaussian smoothing is done). `ksize = 1` can only be used for the first or the second x- or y- derivatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also the special value `ksize = `**`FILTER_SCHARR`**` (-1)` that corresponds to the 3√ó3 Scharr filter that may give more accurate results than the 3√ó3 Sobel. The Scharr aperture below is for the x-derivative, or <font color=magenta>transposed for the y-derivative</font>:<br>\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "-3  & 0 & 3\\\\\n",
    "-10 & 0 & 10\\\\\n",
    "-3  & 0 & 3\\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function calculates an image derivative by convolving the image with the appropriate kernel:</br>\n",
    "\n",
    "$$\n",
    "dst = \\frac{\\partial ^{xorder+yorder}src} {\\partial x^{xorder} \\partial y^{yorder}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Sobel operators combine Gaussian smoothing and differentiation, so the result is more or less resistant to the noise. Most often, the function is called with ( xorder = 1, yorder = 0, ksize = 3) or ( xorder = 0, yorder = 1, ksize = 3) to calculate the first x- or y- image derivative. The first case corresponds to a kernel of:</br>\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "-1 & 0 & 1\\\\\n",
    "-2 & 0 & 2\\\\\n",
    "-1 & 0 & 1\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The second case corresponds to a kernel of:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "-1 & -2 & 1\\\\\n",
    "0  &  0 & 0\\\\\n",
    "1  &  2 & 1\\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/depth_combinations.png\" width=420px; align=left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See also: `Scharr, Laplacian, sepFilter2D, filter2D, GaussianBlur, cartToPolar`\n",
    "<br>\n",
    "\n",
    "Core functionality ¬ª Hardware Acceleration Layer ¬ª Interface ‰∏≠ÂÆö‰πâÔºö**`define CV_16S   3`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddepth = cv2.CV_16S   # 3\n",
    "\n",
    "# Applys the filter on the image in the X direction\n",
    "grad_x = cv2.Sobel(src=im_gray, ddepth=ddepth, dx=1, dy=0, ksize=3)\n",
    "# Applys the filter on the image in the Y direction\n",
    "grad_y = cv2.Sobel(src=im_gray, ddepth=ddepth, dx=0, dy=1, ksize=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can approximate the gradient by calculating absolute values, and converts the result to 8-bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the values back to a number between 0 and 255\n",
    "abs_grad_x = cv2.convertScaleAbs(grad_x)\n",
    "abs_grad_y = cv2.convertScaleAbs(grad_y)\n",
    "\n",
    "# ËßÅ 4.1.4  cv2.convertScaleAbs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then apply the function addWeighted to calculates the sum of two arrays as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adds the derivative in the X and Y direction\n",
    "grad = cv2.addWeighted(abs_grad_x, 0.5, abs_grad_y, 0.5, 0)\n",
    "plt.imshow(grad, cmap='gray')\n",
    "plt.show()\n",
    "# ËßÅ 4.1.5  cv2.addWeighted()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Miscellaneous Image Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cv2.threshold()\n",
    "\n",
    "&emsp;&emsp;&emsp;&ensp; <font style=\"font-size:120%;color:red;\">Thresholding and Simple Segmentation</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thresholding is used in image segmentation this means extracting objects from an image. Image segmentation is used in many applications including extracting text, medical imaging, and industrial imaging. Thresholding an image takes a threshold; If a particular pixel (i,j) is greater than that threshold it will set that pixel to some value usually 1 or 255, otherwise, it will set it to another value, usually zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function cv2.threshold Applies a threshold to the **`gray image`**.\n",
    "\n",
    "<br>\n",
    "\n",
    "**`cv2.threshold( src, thresh, maxval, type[, dst] ) -> retval, dst`**\n",
    "* Applies a fixed-level threshold to each array element.\n",
    "* `src`: input array (multiple-channel, 8-bit or 32-bit floating point). *(ÈúÄË¶ÅÔºögrayscale image)*\n",
    "* `thresh`: threshold value.\n",
    "* `maxval`: maximum value to use with the `cv2.THRESH_BINARY` and `cv2.THRESH_BINARY_INV` thresholding types.\n",
    "* `type`: thresholding type (see `ThresholdTypes`).\n",
    "* `dst`: output array of the same size and type and the same number of channels as src.\n",
    "* \n",
    "* Returns: the computed threshold value if Otsu's or Triangle methods used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter thresholding `type` is the type of thresholding we would like to perform. For example:\n",
    "* **`cv2.THRESH_BINARY`** this is the basic thresholding, it is the type we implemented in the python function *`thresholding`* below, it just a number: (ËßÅ‰∏ãÊñπ cell)\n",
    "* **`cv2.THRESH_TRUNC`** will not change the values if the pixels are less than the threshold value.\n",
    "* **`cv2.THRESH_OTSU`** Otsu's method , it avoids having to choose a value and determines it automatically, using the histogram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type of the threshold operation:\n",
    "\n",
    "<table>\n",
    "    <td>\n",
    "        <img src=\"./images/cv2_threshold_operation__type.png\" width=750px; align=top>\n",
    "    </td>\n",
    "    <td>\n",
    "        <img src=\"./images/cv2_threshold_operation__type_table.png\" align=top>\n",
    "    </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.THRESH_BINARY     # 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 87\n",
    "max_value = 255\n",
    "min_value = 0\n",
    "cameraman = cv2.imread(\"cameraman.jpeg\", cv2.IMREAD_GRAYSCALE)\n",
    "retval, cameraman_threshold = cv2.threshold(cameraman, threshold, \n",
    "                                            max_value, cv2.THRESH_BINARY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret, outs = cv2.threshold(src = cameraman, thresh = 0, maxval = 255, \n",
    "                          type = cv2.THRESH_OTSU+cv2.THRESH_BINARY_INV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "We can write a Python function that will perform thresholding and output a new image `given some input `**`grayscale image`**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thresholding(input_img, threshold, max_value=255, min_value=0):\n",
    "    N,M = input_img.shape\n",
    "    image_out = np.zeros((N,M), dtype=np.uint8)\n",
    "        \n",
    "    for i  in range(N):\n",
    "        for j in range(M):\n",
    "            if input_img[i,j] > threshold:\n",
    "                image_out[i,j] = max_value\n",
    "            else:\n",
    "                image_out[i,j] = min_value\n",
    "                \n",
    "    return image_out          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying thresholding, by setting all the values less than two to zero.\n",
    "threshold = 1\n",
    "max_value = 2\n",
    "min_value = 0\n",
    "thresholding_toy = thresholding(toy_image,    # toy_image ËßÅ 1.3 ÁöÑ Image Negatives\n",
    "                                threshold=threshold, \n",
    "                                max_value=max_value, min_value=min_value)\n",
    "thresholding_toy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core module: Core functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operations on arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cv2.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`cv2.split( m[, mv] ) -> mv`**\n",
    "* Divides a multi-channel array into several single-channel arrays.\n",
    "* `m`: input multi-channel array.\n",
    "* `mv`: output vector of arrays; the arrays themselves are reallocated, if needed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blue, green, red = cv2.split(image)   # Âè¶ËßÅ 1.3 Color Channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cv2.flip()\n",
    "\n",
    "<br>\n",
    "\n",
    "**`cv2.flip(src, flipCode[, dst])`**\n",
    "* Flips a 2D array around vertical, horizontal, or both axes.\n",
    "* `src`: input array.\n",
    "* `dst`: output array of the same size and type as src.\n",
    "* `flipCode`: a flag to specify how to flip the array:\n",
    "    * 0 means flipping around the x-axis\n",
    "    * Positive value (>0, for example, 1) means flipping around y-axis.\n",
    "    * Negative value (<0, for example, -1) means flipping around both axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for flipcode in [0,1,-1]:\n",
    "    \n",
    "    im_flip =  cv2.flip(image,flipcode )\n",
    "    \n",
    "    plt.figure(figsize=(3,3))\n",
    "    plt.imshow( cv2.cvtColor(im_flip,cv2.COLOR_BGR2RGB) )\n",
    "    plt.title(\"flipcode: \"+str(flipcode))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÊàñËÄÖÔºöÈÄöËøá np.ndarray Êìç‰Ωú\n",
    "width, height,C = image.shape\n",
    "array_flip = np.zeros((width, height,C), dtype=np.uint8)\n",
    "\n",
    "for i,row in enumerate(image):\n",
    "        array_flip[width-1-i,:,:]=row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cv2.rotate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`cv2.rotate(src, rotateCode[, dst])`**\n",
    "* Rotates a 2D array in multiples of 90 degrees.\n",
    "* `src`: input array.\n",
    "* `dst`: output array of the same type as src. The size is dependent of enum RotateFlags.\n",
    "* `rotateCode`: an enum to specify how to rotate the array; see the enum RotateFlags.\n",
    "    * rotateCode = `cv2.ROTATE_90_CLOCKWISE`: Rotate by 90 degrees clockwise.\n",
    "    * rotateCode = `cv2.ROTATE_180`: Rotate by 180 degrees clockwise.\n",
    "    * rotateCode = `cv2.ROTATE_90_COUNTERCLOCKWISE`: Rotate by 270 degrees clockwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_flip = cv2.rotate(image, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "enum &emsp; **cv::RotateFlags** {<br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp; cv::ROTATE_90_CLOCKWISE = 0,<br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp; cv::ROTATE_180 = 1,<br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp; cv::ROTATE_90_COUNTERCLOCKWISE = 2<br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp; }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flip = {\"ROTATE_90_CLOCKWISE\":        cv2.ROTATE_90_CLOCKWISE,\n",
    "        \"ROTATE_90_COUNTERCLOCKWISE\": cv2.ROTATE_90_COUNTERCLOCKWISE,\n",
    "        \"ROTATE_180\":                 cv2.ROTATE_180}\n",
    "\n",
    "flip[\"ROTATE_90_CLOCKWISE\"]   # 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot each of the outputs using the different parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in flip.items():\n",
    "    # ÂéüÂõæÂÉè\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(\"orignal\")\n",
    "    \n",
    "    # rotated ÂõæÂÉè\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow( cv2.cvtColor( cv2.rotate(image,value), cv2.COLOR_BGR2RGB ) )\n",
    "    plt.title(key)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cv2.convertScaleAbs()\n",
    "\n",
    "&emsp;&emsp;&emsp;&ensp; <font style=\"font-size:120%;color:red;\">Brightness and contrast adjustments</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`cv2.convertScaleAbs( src[, dst[, alpha[, beta]]] ) -> dst`**\n",
    "* Scales, calculates absolute values, and converts the result to 8-bit.\n",
    "* `src`: input array.\n",
    "* `dst`: output array.\n",
    "* `alpha`: optional scale factor.\n",
    "* `beta`: optional delta added to the scaled values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/cv2_convertScaleAbs.png\"  align=left>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 3        # Simple contrast control\n",
    "beta = -200      # Simple brightness control   \n",
    "new_image = cv2.convertScaleAbs(goldhill, alpha=alpha, beta=beta) #ÂèòÈáègoldhillËßÅ 3.3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cv2.addWeighted()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`cv2.addWeighted( src1, alpha, src2, beta, gamma[, dst[, dtype]] ) -> dst`**\n",
    "* Calculates the weighted sum of two arrays.\n",
    "* `src1`: first input array.\n",
    "* `alpha`: weight of the first array elements.\n",
    "* `src2`: second input array of the same size and channel number as src1.\n",
    "* `beta`: weight of the second array elements.\n",
    "* `gamma`: scalar added to each sum.\n",
    "* `dst`: output array that has the same size and number of channels as the input arrays.\n",
    "* `dtype`: optional depth of the output array; when both input arrays have the same depth, dtype can be set to -1, which will be equivalent to src1.depth()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/cv2_addWeighted.png\"  align=left>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Á®ãÂ∫èÁ§∫‰æãËßÅ 3.5.4  cv2.Sobel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "295.44px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
