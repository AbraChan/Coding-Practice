{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=maroon>**Performance Measures**</font>: sklearn.**metrics** & sklearn.**model_selection**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "## <font color=maroon>Accuracy</font>: using Cross-Validation\n",
    "\n",
    "### cross_val_score()\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")\n",
    "\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "### StratifiedKFold() & clone()\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import clone\n",
    "\n",
    "skfolds = StratifiedKFold(n_splits=3) \n",
    "# add shuffle=True if the dataset is not already shuffled\n",
    "\n",
    "for train_index, test_index in skfolds.split(X_train, y_train_5):\n",
    "    clone_clf = clone(sgd_clf)\n",
    "    X_train_folds = X_train[train_index]\n",
    "    y_train_folds = y_train_5[train_index]\n",
    "    X_test_fold = X_train[test_index]\n",
    "    y_test_fold = y_train_5[test_index]\n",
    "    \n",
    "    clone_clf.fit(X_train_folds, y_train_folds)\n",
    "    y_pred = clone_clf.predict(X_test_fold)\n",
    "    n_correct = sum(y_pred == y_test_fold)\n",
    "    print(n_correct / len(y_pred))         # prints 0.95035, 0.96035, and 0.9604\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "## <font color=maroon>confusion_matrix</font>\n",
    "\n",
    "### cross_val_predict() & confusion_matrix()\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_train_5, y_train_pred)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "## <font color=maroon>Precision</font> and <font color=maroon>Recall</font>\n",
    "\n",
    "### precision_score() & recall_score()\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "precision_score(y_train_5, y_train_pred) # == 3530 / (687 + 3530)\n",
    "\n",
    "recall_score(y_train_5, y_train_pred)    # == 3530 / (1891 + 3530)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "## <font color=maroon>F<sub>1</sub> score</font>\n",
    "\n",
    "### f1_score()\n",
    "\n",
    "The **F1 score** is the `harmonic mean` (调和平均数) of precision and recall. Whereas the `regular mean` treats all values equally, the harmonic mean gives much more weight to low values. As a result, the\n",
    "classifier will only get a high F1 score if both recall and precision are high.\n",
    "```python\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_score(y_train_5, y_train_pred)\n",
    "\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "## <font color=magenta>The Precision/Recall Trade-off</font>\n",
    "\n",
    "The F1 score favors classifiers that have similar precision and recall. This is not always what you want: in some contexts you mostly care about precision, and in other contexts you really care about recall.\n",
    "\n",
    "Unfortunately, you can’t have it both ways: increasing precision reduces recall, and vice versa. This is called the <font color=maroon>**precision/recall trade-off**</font>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### decision_function()\n",
    "\n",
    "Scikit-Learn does not let you set the threshold directly, but it does give you access to the decision scores that it uses to make predictions. Instead of calling the classifier’s `predict()` method, you can call its **`decision_function()`** method, which returns a score for each instance,\n",
    "and then use any **threshold** you want to make predictions based on those scores:\n",
    "\n",
    "<font color=maroon size=5>Lowering the threshold increases recall and reduces precision.</font>\n",
    "\n",
    "```python\n",
    "y_scores = sgd_clf.decision_function([some_digit])  # array([2164.22030239])    \n",
    "threshold = 0\n",
    "y_some_digit_pred = (y_scores > threshold)          # array([ True])\n",
    "\n",
    "\n",
    "threshold = 3000\n",
    "y_some_digit_pred = (y_scores > threshold)\n",
    "y_some_digit_pred                                   # array([False])\n",
    "# This confirms that raising the threshold decreases recall.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "### <font color=magenta>How do you decide which **threshold** to use?</font>\n",
    "\n",
    "\n",
    "#### precision_recall_curve()\n",
    "\n",
    "First, use the **cross_val_predict()** function to get the scores of all instances in the\n",
    "training set, but this time specify that you want to return `decision scores` instead of `predictions`:\n",
    "\n",
    "```python\n",
    "y_scores = cross_val_predict(sgd_clf, \n",
    "                             X_train, y_train_5, \n",
    "                             cv=3, \n",
    "                             method=\"decision_function\")\n",
    "```\n",
    "\n",
    "> The **RandomForestClassifier** class does not have a `decision_function()` method, due to the way it works (we will cover this in Chapter 7). Luckily, it has a `predict_proba()` method that returns class probabilities for each instance, and we can just use the probability of the positive class as a score, so it will work fine. \n",
    "> ```python\n",
    "y_probas_forest = cross_val_predict(forest_clf, \n",
    "                                    X_train, y_train_5, \n",
    "                                    cv=3,\n",
    "                                    method=\"predict_proba\")```\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "With these scores, use the **precision_recall_curve()** function to compute precision and recall for all possible thresholds (the function adds a last precision of 0 and a last recall of 1, corresponding to an infinite threshold):\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)\n",
    "\n",
    "plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\", linewidth=2)\n",
    "plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\", linewidth=2)\n",
    "plt.vlines(threshold, 0, 1.0, \"k\", \"dotted\", label=\"threshold\")\n",
    "[...] # beautify the figure: add grid, legend, axis, labels, and circles\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "#### Plot precision directly against recall\n",
    "\n",
    "Another way to select a good precision/recall trade-off is to plot precision\n",
    "directly against recall.\n",
    "\n",
    "```python\n",
    "plt.plot(recalls, precisions, linewidth=2, label=\"Precision/Recall curve\")\n",
    "[...] # beautify the figure: add labels, grid, legend, arrow, and text\n",
    "plt.show()\n",
    "\n",
    "\n",
    "idx_for_90_precision = (precisions >= 0.90).argmax()\n",
    "threshold_for_90_precision = thresholds[idx_for_90_precision]\n",
    "threshold_for_90_precision\n",
    "\n",
    "\n",
    "y_train_pred_90 = (y_scores >= threshold_for_90_precision)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "## <font color=maroon>ROC Curve</font>\n",
    "\n",
    "The **receiver operating characteristic (ROC) curve** is another common tool\n",
    "used with `binary classifiers`. It is very similar to the `precision/recall curve`,\n",
    "but instead of <font color=maroon>plotting **precision** versus **recall**</font>, the `ROC curve` <font color=maroon>plots the **true positive rate** (another name for recall) against the **false positive rate** (FPR)</font>.\n",
    "\n",
    "......\n",
    "\n",
    "Hence, the ROC curve plots `sensitivity (recall)` versus `1 – specificity`.\n",
    "\n",
    "<br>\n",
    "\n",
    "### roc_curve()\n",
    "\n",
    "To plot the `ROC curve`, you first use the **roc_curve()** function to <font color=maroon>compute the `TPR` and `FPR` for various threshold values</font>:\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_train_5, y_scores)\n",
    "\n",
    "# Since thresholds are listed in decreasing order in this case, \n",
    "# we use <= instead of >= on the first line:\n",
    "idx_for_threshold_at_90 = (thresholds <= threshold_for_90_precision).argmax()\n",
    "tpr_90, fpr_90 = tpr[idx_for_threshold_at_90], fpr[idx_for_threshold_at_90]\n",
    "\n",
    "plt.plot(fpr, tpr, linewidth=2, label=\"ROC curve\")\n",
    "plt.plot([0, 1], [0, 1], 'k:', label=\"Random classifier's ROC curve\")\n",
    "plt.plot([fpr_90], [tpr_90], \"ko\", label=\"Threshold for 90% precision\")\n",
    "[...] # beautify the figure: add labels, grid, legend, arrow, and text\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "\n",
    "Once again there is a trade-off: the higher the **recall (TPR)**, the more **false positives (FPR)** the classifier produces. The dotted line represents the ROC curve of a purely random classifier; a good classifier stays as far away from that line as possible (toward the **top-left corner**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC <font color=maroon>AUC</font>: roc_auc_score()\n",
    "\n",
    "One way to compare classifiers is to measure the **area under the curve\n",
    "(AUC)**. A perfect classifier will have a `ROC AUC equal to 1`, whereas a\n",
    "purely random classifier will have a ROC AUC equal to 0.5. Scikit-Learn\n",
    "provides a function to estimate the ROC AUC:\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "roc_auc_score(y_train_5, y_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "# sklearn.**dummy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dummy_clf = DummyClassifier()\n",
    "dummy_clf.fit(X_train, y_train_5)\n",
    "print(any(dummy_clf.predict(X_train)))  # prints False: no 5s detected\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "KNeighborsClassifier\n",
      "SVC\n",
      "DecisionTreeClassifier\n",
      "RandomForestClassifier\n",
      "AdaBoostClassifier\n",
      "GradientBoostingClassifier\n",
      "GaussianNB\n",
      "LinearDiscriminantAnalysis\n",
      "QuadraticDiscriminantAnalysis\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, \\\n",
    "                             AdaBoostClassifier, \\\n",
    "                             GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, \\\n",
    "                                          QuadraticDiscriminantAnalysis\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "classifiers = [LogisticRegression(),\n",
    "               KNeighborsClassifier(3),\n",
    "               SVC(probability=True),\n",
    "               DecisionTreeClassifier(),\n",
    "               RandomForestClassifier(),\n",
    "               AdaBoostClassifier(),\n",
    "               GradientBoostingClassifier(),\n",
    "               GaussianNB(),\n",
    "               LinearDiscriminantAnalysis(),\n",
    "               QuadraticDiscriminantAnalysis(),\n",
    "               ]\n",
    "\n",
    "\n",
    "for clf in classifiers:\n",
    "    name = clf.__class__.__name__\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfg",
   "language": "python",
   "name": "tfg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "295.99px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
