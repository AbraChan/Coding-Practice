{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=center><font color=maroon size=6><b>Learn the Basics</b></font></div>\n",
    "<br>\n",
    "<div align=center><font color=maroon size=4><b>(CS231n PyTorch Tutorial)</b></font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4><b>References:</b></font>\n",
    "1. Lecture 8 Pytorch slides: CS231n PyTorch Tutorial<br><font size=2>`D:\\KeepStudy\\Stanford\\CS231n_Convolutional-Neural-Networks-for-Visual-Recognition\\0 lecture slides\\lecture_8 - 2 Pytorch.pdf`</font>\n",
    "\n",
    "\n",
    "2. Pytorch official tutorials: [WELCOME TO PYTORCH TUTORIALS](https://pytorch.org/tutorials/index.html)\n",
    "* [Learn the Basics](https://pytorch.org/tutorials/beginner/basics/intro.html)\n",
    "    * [Quickstart](https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html)&ensp;(略)\n",
    "    * [TENSORS](https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html) (Tutorials > Tensors)\n",
    "    * [Datasets and DataLoaders](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) (Tutorials > Datasets & DataLoaders)\n",
    "    * [Build Model](https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html) (Tutorials > Build the Neural Network)\n",
    "    * [Automatic Differentiation](https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html) (Tutorials > Automatic Differentiation with torch.autograd)\n",
    "    * [Optimization Loop](https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html) (Tutorials > Optimizing Model Parameters)\n",
    "    * [Save, Load and Use Model](https://pytorch.org/tutorials/beginner/basics/saveloadrun_tutorial.html) (Tutorials > Save and Load the Model)\n",
    "\n",
    "\n",
    "* [PyTorch Recipes](https://pytorch.org/tutorials/recipes/recipes_index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:blue;font-size:110%;font-weight:bold\">TENSORS</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors are a specialized data structure that are very similar to arrays and matrices. In PyTorch, we use tensors to encode the inputs and outputs of a model, as well as the model’s parameters.\n",
    "\n",
    "Tensors are similar to `NumPy's` ndarrays, except that tensors can run on GPUs or other hardware accelerators. In fact, tensors and NumPy arrays can often share the same underlying memory, eliminating the need to copy data (see `Bridge with NumPy`). \n",
    "\n",
    "Tensors are also optimized for automatic differentiation (we’ll see more about that later in the [Autograd](https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html) section). If you’re familiar with ndarrays, you’ll be right at home with the Tensor API. If not, follow along!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing a Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directly from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [[1,2], [3,4]]\n",
    "x_data = torch.tensor(data)\n",
    "x_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From a Numpy array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors can be created from NumPy arrays (and vice versa - see [Bridge with NumPy](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#bridge-to-np-label))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]], dtype=torch.int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)\n",
    "x_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From another tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new tensor retains the properties (shape, datatype) of the argument tensor, unless explicitly overridden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1],\n",
       "        [1, 1]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retains the properties of x_data\n",
    "x_ones = torch.ones_like(x_data)\n",
    "x_ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3310, 0.9394],\n",
       "        [0.7755, 0.8797]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# overrides the datatype of x_data\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float)\n",
    "x_rand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With random or constant values:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shape is a tuple of tensor dimensions. In the functions below, it determines the dimensionality of the output tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (2,3,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4039, 0.0701, 0.4876],\n",
       "        [0.0564, 0.4843, 0.8168]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_tensor = torch.rand(shape)\n",
    "rand_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ones_tensor = torch.ones(shape)\n",
    "ones_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zeros_tensor = torch.zeros(shape)\n",
    "zeros_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attributes of a Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor attributes describe their shape, datatype, and the device on which they are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5730, 0.3064, 0.8686, 0.3647],\n",
       "        [0.5309, 0.1777, 0.3743, 0.7102],\n",
       "        [0.7722, 0.2021, 0.7823, 0.7325]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.rand(3,4)\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operations on Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over 100 tensor operations, including arithmetic, linear algebra, matrix manipulation (transposing, indexing, slicing), sampling and more are comprehensively described `here`([torch](https://pytorch.org/docs/stable/torch.html)).\n",
    "\n",
    "Each of these operations can be run on the GPU (at typically higher speeds than on a CPU). If you’re using **Colab**, allocate a GPU by going to `Runtime > Change runtime type > GPU`.\n",
    "\n",
    "By default, tensors are created on the CPU. We need to explicitly move tensors to the GPU using **`.to`** method (after checking for GPU availability). <font size=3 color=maroon>Keep in mind that copying large tensors across devices can be expensive in terms of time and memory!</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We move our tensor to the GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    tensor = tensor.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard numpy-like indexing and slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9880, 0.8574, 0.5132, 0.0198],\n",
       "        [0.2757, 0.0768, 0.1185, 0.0267],\n",
       "        [0.5538, 0.9278, 0.1123, 0.1449],\n",
       "        [0.6735, 0.6554, 0.4960, 0.3828]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.rand(4,4)\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9880, 0.8574, 0.5132, 0.0198])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor[0]        # first row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9880, 0.2757, 0.5538, 0.6735])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor[:, 0]    # first column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0198, 0.0267, 0.1449, 0.3828])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor[..., -1] # last column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9880, 0.0000, 0.5132, 0.0198],\n",
       "        [0.2757, 0.0000, 0.1185, 0.0267],\n",
       "        [0.5538, 0.0000, 0.1123, 0.1449],\n",
       "        [0.6735, 0.0000, 0.4960, 0.3828]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor[:, 1] = 0\n",
    "tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use **`torch.cat`** to concatenate a sequence of tensors along a given dimension. See also torch.stack, another tensor joining op that is subtly different from **`torch.cat`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 0.],\n",
       "        [1., 1., 0.],\n",
       "        [1., 1., 0.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.ones(3,3)\n",
    "tensor[:,2] = 0\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 0., 1., 1., 0., 1., 1., 0.],\n",
       "        [1., 1., 0., 1., 1., 0., 1., 1., 0.],\n",
       "        [1., 1., 0., 1., 1., 0., 1., 1., 0.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
    "t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arithmetic operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[6., 6., 6.],\n",
       "         [6., 6., 6.],\n",
       "         [6., 6., 6.]]),\n",
       " tensor([[6., 6., 6.],\n",
       "         [6., 6., 6.],\n",
       "         [6., 6., 6.]]),\n",
       " tensor([[6., 6., 6.],\n",
       "         [6., 6., 6.],\n",
       "         [6., 6., 6.]]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This computes the matrix multiplication between two tensors. y1, y2, y3 will have the same value\n",
    "y1 = t1 @ t1.T\n",
    "y2 = t1.matmul(t1.T)\n",
    "\n",
    "y3 = torch.rand_like(t1)\n",
    "torch.matmul(t1, t1.T, out=y3)\n",
    "\n",
    "y1, y2, y3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1., 0., 1., 1., 0., 1., 1., 0.],\n",
       "         [1., 1., 0., 1., 1., 0., 1., 1., 0.],\n",
       "         [1., 1., 0., 1., 1., 0., 1., 1., 0.]]),\n",
       " tensor([[1., 1., 0., 1., 1., 0., 1., 1., 0.],\n",
       "         [1., 1., 0., 1., 1., 0., 1., 1., 0.],\n",
       "         [1., 1., 0., 1., 1., 0., 1., 1., 0.]]),\n",
       " tensor([[1., 1., 0., 1., 1., 0., 1., 1., 0.],\n",
       "         [1., 1., 0., 1., 1., 0., 1., 1., 0.],\n",
       "         [1., 1., 0., 1., 1., 0., 1., 1., 0.]]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This computes the element-wise product. z1, z2, z3 will have the same value\n",
    "z1 = t1 * t1\n",
    "z2 = t1.mul(t1)\n",
    "\n",
    "z3 = torch.rand_like(t1)\n",
    "torch.mul(t1, t1, out=z3)\n",
    "\n",
    "z1, z2, z3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single-element tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a one-element tensor, for example by aggregating all values of a tensor into one value, you can convert it to a Python numerical value using **`item()`**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(18.)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg = t1.sum()\n",
    "agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_item = agg.item()\n",
    "agg_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(agg_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n"
     ]
    }
   ],
   "source": [
    "print(type(agg_item))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-place operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Operations that store the result into the operand are called in-place. They are denoted by a **`_`** suffix. For example: **`x.copy_(y)`**, **`x.t_()`**, will change x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 0., 1., 1., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 1., 1., 0., 1., 1., 0.],\n",
      "        [1., 1., 0., 1., 1., 0., 1., 1., 0.]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"{t1} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6., 6., 5., 6., 6., 5., 6., 6., 5.],\n",
       "        [6., 6., 5., 6., 6., 5., 6., 6., 5.],\n",
       "        [6., 6., 5., 6., 6., 5., 6., 6., 5.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.add_(5)\n",
    "t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red size=3><b>Note:</b></font>\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">In-place operations save some memory, but can be problematic when computing derivatives because of an immediate loss of history. Hence, their use is discouraged.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bridge with NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see [Bridge with NumPy](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#bridge-to-np-label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:blue;font-size:110%;font-weight:bold\">Datasets & DataLoaders</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for processing data samples can get messy and hard to maintain; we ideally want our dataset code to be decoupled from our model training code for better readability and modularity. PyTorch provides two data primitives: \n",
    "<font size=3 color=maroon><b>torch.utils.data.DataLoader</b></font> and <font size=3 color=maroon><b>torch.utils.data.Dataset</b></font> that allow you to `use pre-loaded datasets as well as your own data`. \n",
    "* **`Dataset`** stores the <font color=maroon>samples</font> and their corresponding <font color=maroon>labels</font>.\n",
    "* **`DataLoader`** wraps an iterable around the **`Dataset`** to enable easy access to the samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch domain libraries provide a number of pre-loaded datasets (such as FashionMNIST) that subclass `torch.utils.data.Dataset` and implement functions specific to the particular data. They can be used to prototype and benchmark your model. You can find them here: [Image Datasets](https://pytorch.org/vision/stable/datasets.html), [Text Datasets](https://pytorch.org/text/stable/datasets.html), and [Audio Datasets](https://pytorch.org/audio/stable/datasets.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of how to load the [Fashion-MNIST](https://research.zalando.com/project/fashion_mnist/fashion_mnist/)  dataset from <font size=3 color=maroon><b>TorchVision</b></font>. Fashion-MNIST is a dataset of Zalando’s article images consisting of 60,000 training examples and 10,000 test examples. Each example comprises a 28×28 grayscale image and an associated label from one of 10 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the [FashionMNIST Dataset](https://pytorch.org/vision/stable/datasets.html#fashion-mnist) with the following parameters:\n",
    "* **`root`** is the path where the train/test data is stored,\n",
    "* **`train`** specifies training or test dataset,\n",
    "* **`download=True`** downloads the data from the internet if it’s not available at root.\n",
    "* **`transform`** and **`target_transform`** specify the *feature* and *label* transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "# from torchvision.transforms import ToPILImage\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterating and Visualizing the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can index **`Datasets`** manually like a list: **`training_data[index]`**. We use **`matplotlib`** to visualize some samples in our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels_map = {\n",
    "    0: \"T-Shirt\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle Boot\",\n",
    "}\n",
    "\n",
    "figure = plt.figure(figsize=(6, 6))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
    "    img, label = training_data[sample_idx]\n",
    "    \n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(str(label) + \" \" + labels_map[label])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:red;font-size:110%;font-weight:bold\">Creating a Custom Dataset for your files</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A custom Dataset class must implement three functions: <font size=3 color=maroon><b><u>_</u><u>_</u>init<u>_</u><u>_</u></b></font>, <font size=3 color=maroon><b><u>_</u><u>_</u>len<u>_</u><u>_</u></b></font>, and <font size=3 color=maroon><b><u>_</u><u>_</u>getitem<u>_</u><u>_</u></b></font>. \n",
    "\n",
    "Take a look at this implementation; the FashionMNIST images are stored in a directory `img_dir`, and their labels are stored separately in a CSV file `annotations_file`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next sections, we’ll break down what’s happening in each of these functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, \n",
    "                       img_dir, \n",
    "                       transform=None, \n",
    "                       target_transform=None):\n",
    "        \n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0] )\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "            \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font size=4 color=maroon><b><u>_</u><u>_</u>init<u>_</u><u>_</u></b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **`__init__`** function is run once when instantiating the Dataset object. We initialize the directory containing the images, the annotations file, and both transforms (covered in more detail in the next section).\n",
    "\n",
    "The labels.csv file looks like:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tshirt1.jpg, 0\n",
    "tshirt2.jpg, 0\n",
    "......\n",
    "ankleboot999.jpg, 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "    self.img_labels = pd.read_csv(annotations_file)\n",
    "    self.img_dir = img_dir\n",
    "    self.transform = transform\n",
    "    self.target_transform = target_transform\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font size=4 color=maroon><b><u>_</u><u>_</u>len<u>_</u><u>_</u></b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **`__len__`** function returns the number of samples in our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def __len__(self):\n",
    "    return len(self.img_labels)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font size=4 color=maroon><b><u>_</u><u>_</u>getitem<u>_</u><u>_</u></b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **`__getitem__`** function loads and returns a sample from the dataset at the given index `idx`. Based on the index, it identifies the image’s location on disk, converts that to a <font color=maroon><b>tensor</b></font> using `read_image`, retrieves the corresponding label from the csv data in `self.img_labels`, calls the transform functions on them (if applicable), and returns the tensor image and corresponding label in a tuple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def __getitem__(self, idx):\n",
    "    img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "    image = read_image(img_path)\n",
    "    label = self.img_labels.iloc[idx, 1]\n",
    "    if self.transform:\n",
    "        image = self.transform(image)\n",
    "    if self.target_transform:\n",
    "        label = self.target_transform(label)\n",
    "    return image, label\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing your data for training with DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <font size=3 color=maroon><b>Dataset</b></font> retrieves our dataset’s features and labels one sample at a time. While training a model, we typically want to pass samples in “minibatches”, reshuffle the data at every epoch to reduce model overfitting, and use Python’s <font size=3 color=maroon>multiprocessing</font> to speed up data retrieval.\n",
    "\n",
    "<font size=3 color=maroon><b>DataLoader</b></font> is an iterable that abstracts this complexity for us in an easy API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "# from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate through the DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have loaded that dataset into the **`DataLoader`** and can iterate through the dataset as needed. Each iteration below returns a batch of **`train_features`** and **`train_labels`** (containing **`batch_size=64`** features and labels respectively). Because we specified **`shuffle=True`**, after we iterate over all batches the data is shuffled (for finer-grained control over the data loading order, take a look at [Samplers](https://pytorch.org/docs/stable/data.html#data-loading-order-and-sampler))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([64, 1, 28, 28])\n",
      "Labels batch shape: torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 0\n"
     ]
    }
   ],
   "source": [
    "# Display image and label\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:blue;font-size:110%;font-weight:bold\">Build Model</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5>BUILD THE NEURAL NETWORK</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>Neural networks comprise of layers/modules that perform operations on data. \n",
    "\n",
    "\n",
    "* The [<font color=maroon><b>torch.nn</b></font>](https://pytorch.org/docs/stable/nn.html) namespace provides all the building blocks you need to build your own neural network. \n",
    "\n",
    "\n",
    "* Every module in PyTorch subclasses the [<font color=maroon><b>nn.Module</b></font>](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). \n",
    "\n",
    "\n",
    "* A neural network is a module itself that consists of other modules (layers). This nested structure allows for building and managing complex architectures easily.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue size=5><b>torch.nn Module</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Device for Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to be able to train our model on a hardware accelerator like the GPU, if it is available. Let’s check to see if [<font color=maroon size=3><b>torch.cuda</b></font>](https://pytorch.org/docs/stable/notes/cuda.html) is available, else we continue to use the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our neural network by subclassing **`nn.Module`**, and initialize the neural network layers in **`__init__`**. Every **`nn.Module`** subclass implements the operations on input data in the **`forward`** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "                nn.Linear(28*28, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(512, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(512, 10)\n",
    "        )\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create an instance of **`NeuralNetwork`**, and move it to the device, and print its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>To use the model, we pass it the input data. This executes the model’s forward, along with some [<font size=3 color=maroon><b>background operations</b></font>](https://github.com/pytorch/pytorch/blob/270111b7b611d174967ed204776985cefca9c144/torch/nn/modules/module.py#L866). \n",
    "    \n",
    "<font color=maroon>Do not call <b>model.forward()</b> directly!</font></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand(1, 28, 28, device=device)\n",
    "logits = model(X)\n",
    "print(logits.shape)\n",
    "\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s break down the layers in the FashionMNIST model. To illustrate it, we will take a sample minibatch of 3 images of size 28x28 and see what happens to it as we pass it through the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = torch.rand(3,28,28)\n",
    "input_image.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize the [<font size=3 color=maroon><b>nn.Flatten</b></font>]() layer to convert each 2D 28x28 image into a contiguous array of 784 pixel values ( the minibatch dimension (at dim=0) is maintained)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(input_image)\n",
    "flat_image.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [<font size=3 color=maroon><b>linear layer</b></font>](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) is a module that applies a linear transformation on the input <font color=maroon>using its stored weights and biases</font>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
    "hidden1 = layer1(flat_image)\n",
    "\n",
    "hidden1.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-linear activations are what create the complex mappings between the model’s inputs and outputs. They are applied after linear transformations to introduce nonlinearity, helping neural networks learn a wide variety of phenomena.\n",
    "\n",
    "In this model, we use [<font size=3 color=maroon><b>nn.ReLU</b></font>](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html) between our linear layers, but there’s other activations to introduce non-linearity in your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Before ReLu:\\n {hidden1}\\n\\n\")\n",
    "\n",
    "hidden1 = nn.ReLU()(hidden1)\n",
    "\n",
    "print(f\"After ReLu:\\n{hidden1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<font size=3 color=maroon><b>nn.Sequential</b></font>]() is an ordered container of modules. The data is passed through all the modules in the same order as defined. You can use sequential containers to put together a quick network like `seq_modules`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_modules = nn.Sequential(\n",
    "        flatten,\n",
    "        layer1,\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(20,10)\n",
    ")\n",
    "\n",
    "input_image = torch.rand(3,28,28)\n",
    "logits = seq_modules(input_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last linear layer of the neural network returns *`logits`*` - raw values in [-infty, infty]` - which are passed to the [<font size=3 color=maroon><b>nn.Softmax</b></font>]() module. The logits are scaled to values [0, 1] representing the model’s predicted probabilities for each class. dim parameter indicates the dimension along which the values must sum to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim=1)\n",
    "pred_probab = softmax(logits)\n",
    "pred_probab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many layers inside a neural network are ***parameterized***, i.e. have associated weights and biases that are optimized during training. Subclassing **`nn.Module`** automatically tracks all fields defined inside your model object, and makes all parameters accessible using your model’s **`parameters()`** or **`named_parameters()`** methods.\n",
    "\n",
    "In this example, we iterate over each parameter, and print its size and a preview of its values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Model structure: {model}\\n\\n\")\n",
    "\n",
    "\n",
    "for name,param in model.named_parameters():\n",
    "    print(f\"Layer: {name} \\nSize: {param.size()} \\nValues : \\n{param[:2]} \\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Differentiation  <a href=\"https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html\" style=\"text-decoration:none;\"><font size=2>[link]</font></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "详见:\n",
    "* 上面蓝色 [link] 的链接，\n",
    "* 或者见自己笔记：`D:\\KeepStudy\\0_Coding\\Pytorch\\Pytorch_Docs\\0 Autograd.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:blue;font-size:110%;font-weight:bold\">Optimization Loop</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5>OPTIMIZING MODEL PARAMETERS</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a model and data it’s time to train, validate and test our model by optimizing its parameters on our data. \n",
    "\n",
    "<font color=maroon>Training a model is an iterative process; in each <b>iteration</b> (called an <b><i>epoch</i></b>) the model makes a guess about the output, calculates the error in its guess (loss), collects the derivatives of the error with respect to its parameters (as we saw in the [previous section](https://pytorch.org/tutorials/beginner/basics/autograd_tutorial.html)), and optimizes these parameters using gradient descent. </font>\n",
    "\n",
    "For a more detailed walkthrough of this process, check out this video on [backpropagation from 3Blue1Brown](https://www.youtube.com/watch?v=tIeHLnjs5U8)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4><b>Prerequisite Code</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the code from the previous sections on [Datasets & DataLoaders](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) and [Build Model](https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(略)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4><b>Hyperparameters</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters are adjustable parameters that let you control the model optimization process. Different hyperparameter values can impact model training and convergence rates ([read more](https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html) about hyperparameter tuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the following hyperparameters for training:\n",
    "\n",
    "* **Number of Epochs** - the number times to iterate over the dataset\n",
    "\n",
    "\n",
    "* **Batch Size** - the number of data samples propagated through the network before the parameters are updated\n",
    "\n",
    "\n",
    "* **Learning Rate** - how much to update models parameters at each batch/epoch. Smaller values yield slow learning speed, while large values may result in unpredictable behavior during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When presented with some training data, our untrained network is likely not to give the correct answer. **Loss function** measures the degree of dissimilarity of obtained result to the target value, and it is the loss function that we want to minimize during training. To calculate the loss we make a prediction using the inputs of our given data sample and compare it against the true data label value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common loss functions include [<font size=3 color=maroon><b>nn.MSELoss</b></font>](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss) (Mean Square Error) for regression tasks, and [<font size=3 color=maroon><b>nn.NLLLoss</b></font>](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss) (Negative Log Likelihood) for classification. [<font size=3 color=maroon><b>nn.CrossEntropyLoss</b></font>](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss) combines **`nn.LogSoftmax`** and **`nn.NLLLoss`**.\n",
    "\n",
    "We pass our model’s output logits to **`nn.CrossEntropyLoss`**, which will normalize the logits and compute the prediction error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization is the process of adjusting model parameters to reduce model error in each training step. **Optimization algorithms** define how this process is performed (in this example we use `Stochastic Gradient Descent`). \n",
    "\n",
    "All optimization logic is encapsulated in the **`optimizer`** object. Here, we use the SGD optimizer; additionally, there are many [<font size=3 color=maroon>different optimizers</font>](https://pytorch.org/docs/stable/optim.html) available in PyTorch such as ADAM and RMSProp, that work better for different kinds of models and data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "    \n",
    "Inside the training loop, optimization happens in three steps:\n",
    "* Call <font color=maroon>optimizer.zero_grad()</font> to reset the gradients of model parameters. <font color=red><i>Gradients by default add up; to prevent double-counting, we explicitly zero them at each iteration.</i></font>\n",
    "\n",
    "\n",
    "* Backpropagate the prediction loss with a call to <font color=maroon>loss.backward()</font>. PyTorch deposits the gradients of the loss w.r.t. each parameter.\n",
    "\n",
    "\n",
    "* Once we have our gradients, we call <font color=maroon>optimizer.step()</font> to adjust the parameters by the gradients collected in the backward pass.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define **`train_loop`** that loops over our optimization code, and **`test_loop`** that evaluates the model’s performance against our test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    \n",
    "    for batch, (X,y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X.to(device))\n",
    "        loss = loss_fn(pred, y.to(device))\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            \n",
    "            \n",
    "            \n",
    "def test_loop(dataloader, model, loos_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X,y in dataloader:\n",
    "            pred = model(X.to(device))\n",
    "            test_loss += loss_fn(pred, y.to(device)).item()\n",
    "            correct += (pred.argmax(1) == y.to(device)).type(torch.float).sum().item()\n",
    "            \n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, \\\n",
    "            Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize the loss function and optimizer, and pass it to train_loop and test_loop. Feel free to increase the number of epochs to track the model’s improving performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.478181  [    0/60000]\n",
      "loss: 1.603208  [ 6400/60000]\n",
      "loss: 1.510848  [12800/60000]\n",
      "loss: 1.421172  [19200/60000]\n",
      "loss: 1.419490  [25600/60000]\n",
      "loss: 1.355895  [32000/60000]\n",
      "loss: 1.368680  [38400/60000]\n",
      "loss: 1.408450  [44800/60000]\n",
      "loss: 1.261227  [51200/60000]\n",
      "loss: 1.220700  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.8%,             Avg loss: 1.279551 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.246270  [    0/60000]\n",
      "loss: 1.218567  [ 6400/60000]\n",
      "loss: 1.164149  [12800/60000]\n",
      "loss: 1.110192  [19200/60000]\n",
      "loss: 1.105657  [25600/60000]\n",
      "loss: 1.222122  [32000/60000]\n",
      "loss: 1.035296  [38400/60000]\n",
      "loss: 1.177889  [44800/60000]\n",
      "loss: 1.202076  [51200/60000]\n",
      "loss: 0.983081  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.3%,             Avg loss: 1.104755 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.138592  [    0/60000]\n",
      "loss: 1.010925  [ 6400/60000]\n",
      "loss: 1.104472  [12800/60000]\n",
      "loss: 1.059325  [19200/60000]\n",
      "loss: 1.129629  [25600/60000]\n",
      "loss: 1.020354  [32000/60000]\n",
      "loss: 0.995613  [38400/60000]\n",
      "loss: 1.195210  [44800/60000]\n",
      "loss: 0.931632  [51200/60000]\n",
      "loss: 1.016347  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 65.5%,             Avg loss: 0.991968 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.970414  [    0/60000]\n",
      "loss: 1.038270  [ 6400/60000]\n",
      "loss: 0.996947  [12800/60000]\n",
      "loss: 1.158988  [19200/60000]\n",
      "loss: 0.877816  [25600/60000]\n",
      "loss: 0.945225  [32000/60000]\n",
      "loss: 0.970804  [38400/60000]\n",
      "loss: 1.018119  [44800/60000]\n",
      "loss: 1.010220  [51200/60000]\n",
      "loss: 0.922017  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 67.0%,             Avg loss: 0.917076 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.890037  [    0/60000]\n",
      "loss: 0.891259  [ 6400/60000]\n",
      "loss: 0.893880  [12800/60000]\n",
      "loss: 0.771294  [19200/60000]\n",
      "loss: 0.825110  [25600/60000]\n",
      "loss: 0.927488  [32000/60000]\n",
      "loss: 0.686355  [38400/60000]\n",
      "loss: 0.769025  [44800/60000]\n",
      "loss: 1.003782  [51200/60000]\n",
      "loss: 0.851441  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 67.6%,             Avg loss: 0.861872 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.832149  [    0/60000]\n",
      "loss: 0.834087  [ 6400/60000]\n",
      "loss: 0.960057  [12800/60000]\n",
      "loss: 0.981051  [19200/60000]\n",
      "loss: 0.793077  [25600/60000]\n",
      "loss: 0.794752  [32000/60000]\n",
      "loss: 0.816567  [38400/60000]\n",
      "loss: 0.791244  [44800/60000]\n",
      "loss: 0.801192  [51200/60000]\n",
      "loss: 0.815386  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.3%,             Avg loss: 0.821069 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.788761  [    0/60000]\n",
      "loss: 0.800611  [ 6400/60000]\n",
      "loss: 0.877878  [12800/60000]\n",
      "loss: 0.668536  [19200/60000]\n",
      "loss: 0.935412  [25600/60000]\n",
      "loss: 0.760195  [32000/60000]\n",
      "loss: 0.661545  [38400/60000]\n",
      "loss: 0.839829  [44800/60000]\n",
      "loss: 0.794543  [51200/60000]\n",
      "loss: 0.728601  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.2%,             Avg loss: 0.790163 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.735816  [    0/60000]\n",
      "loss: 0.873352  [ 6400/60000]\n",
      "loss: 0.746170  [12800/60000]\n",
      "loss: 0.702707  [19200/60000]\n",
      "loss: 0.716742  [25600/60000]\n",
      "loss: 0.758223  [32000/60000]\n",
      "loss: 0.788897  [38400/60000]\n",
      "loss: 0.772801  [44800/60000]\n",
      "loss: 0.745542  [51200/60000]\n",
      "loss: 0.561856  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 71.9%,             Avg loss: 0.762598 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.635256  [    0/60000]\n",
      "loss: 0.849839  [ 6400/60000]\n",
      "loss: 0.697234  [12800/60000]\n",
      "loss: 0.958799  [19200/60000]\n",
      "loss: 0.723617  [25600/60000]\n",
      "loss: 0.788915  [32000/60000]\n",
      "loss: 0.627391  [38400/60000]\n",
      "loss: 0.765804  [44800/60000]\n",
      "loss: 0.594406  [51200/60000]\n",
      "loss: 0.726936  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 72.9%,             Avg loss: 0.739794 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.761789  [    0/60000]\n",
      "loss: 0.754411  [ 6400/60000]\n",
      "loss: 0.781381  [12800/60000]\n",
      "loss: 0.794758  [19200/60000]\n",
      "loss: 0.813151  [25600/60000]\n",
      "loss: 0.691985  [32000/60000]\n",
      "loss: 0.715515  [38400/60000]\n",
      "loss: 0.645677  [44800/60000]\n",
      "loss: 0.708220  [51200/60000]\n",
      "loss: 0.644050  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 74.0%,             Avg loss: 0.720889 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_addmm)</font>\n",
    "\n",
    "\n",
    "我的解决思路，参照[文献](https://blog.csdn.net/qq_28935065/article/details/119007334)，因为前面已经 有`model = NeuralNetwork().to(device)`，故我在train_loop和test_loop这两个函数的相关 Tensor 变量处都增加了 **`.to(device)`**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Loss Functions](https://pytorch.org/docs/stable/nn.html#loss-functions)\n",
    "* [torch.optim](https://pytorch.org/docs/stable/optim.html)\n",
    "* [Warmstart Training a Model](https://pytorch.org/tutorials/recipes/recipes/warmstarting_model_using_parameters_from_a_different_model.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Save, Load and Use Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5>SAVE AND LOAD THE MODEL</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will look at how to persist model state with saving, loading and running model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading Model Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch models store the learned parameters in an internal state dictionary, called **`state_dict`**. These can be persisted via the **`torch.save`** method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.vgg16(pretrained=True)\n",
    "torch.save(model.state_dict(), \"./model_weights/vgg16_weights.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load model weights, you need to create an instance of the same model first, and then load the parameters using **`load_state_dict()`** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we do not specify pretrained=True, i.e. do not load default weights\n",
    "model = models.vgg16()\n",
    "\n",
    "model.load_state_dict(torch.load(\"./model_weights/vgg16_weights.pth\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3 color=dodgerblue><b>Note:</b></font>\n",
    "<div class=\"alert alert-block alert-info\">Be sure to call <font color=maroon>model.eval()</font> method before inferencing to set the dropout and batch normalization layers to evaluation mode. Failing to do this will yield inconsistent inference results.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading Models with Shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When loading model weights, we needed to instantiate the model class first, because the class defines the structure of a network. We might want to save the structure of this class together with the model, in which case we can pass **`model`** (and not **`model.state_dict()`**) to the saving function:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torch.save(model, 'model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then load the model like this:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = torch.load('model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3 color=dodgerblue><b>Note:</b></font>\n",
    "<div class=\"alert alert-block alert-info\">This approach uses Python <font color=maroon>pickle</font> module when serializing the model, thus it relies on the actual class definition to be available when loading the model.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4><b>Related Tutorials</b></font>:\n",
    "* [Saving and Loading a General Checkpoint in PyTorch](https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ptg]",
   "language": "python",
   "name": "conda-env-ptg-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "258px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
