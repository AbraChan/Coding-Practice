{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=center><font color=maroon size=6><b>PyTorch Recipes - 1</b></font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4><b>References:</b></font>\n",
    "\n",
    "Pytorch official tutorials: <a href=\"https://pytorch.org/tutorials/index.html\" style=\"text-decoration:none;\">WELCOME TO PYTORCH TUTORIALS</a>\n",
    "\n",
    "\n",
    "* <a href=\"https://pytorch.org/tutorials/beginner/basics/intro.html\" style=\"text-decoration:none;\">Learn the Basics</a>\n",
    "\n",
    "\n",
    "* <a href=\"https://pytorch.org/tutorials/recipes/recipes_index.html\" style=\"text-decoration:none;\">PyTorch Recipes</a>\n",
    "    * `以下文字红色字体表示已经学习过并将笔记整理进这个 notebook`\n",
    "    * <a href=\"https://pytorch.org/tutorials/recipes/recipes/loading_data_recipe.html\" style=\"text-decoration:none;\">Loading data in PyTorch</a>\n",
    "    \n",
    "    * <a href=\"https://pytorch.org/tutorials/recipes/recipes/defining_a_neural_network.html\" style=\"text-decoration:none;\">Defining a Neural Network in PyTorch</a>\n",
    "    \n",
    "    * <a href=\"https://pytorch.org/tutorials/recipes/recipes/what_is_state_dict.html\" style=\"text-decoration:none;\">What is a state_dict in PyTorch</a>\n",
    "    \n",
    "    * <a href=\"https://pytorch.org/tutorials/recipes/recipes/zeroing_out_gradients.html\" style=\"text-decoration:none;\">Zeroing out gradients in PyTorch</a> \n",
    "    \n",
    "    * <a href=\"https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_models_for_inference.html\" style=\"text-decoration:none;\">Saving and loading models for inference in PyTorch</a>\n",
    "    \n",
    "    * <a href=\"https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html\" style=\"text-decoration:none;\">Saving and loading a general checkpoint in PyTorch</a>\n",
    "    \n",
    "    * \n",
    "    \n",
    "    * <a href=\"https://pytorch.org/tutorials/recipes/recipes/warmstarting_model_using_parameters_from_a_different_model.html\" style=\"text-decoration:none;\"><font color=maroon>Warmstarting model using parameters from a different model in PyTorch</font></a>\n",
    "    \n",
    "    * <a href=\"https://pytorch.org/tutorials/recipes/recipes/saving_multiple_models_in_one_file.html\" style=\"text-decoration:none;\"><font color=maroon>Saving and loading multiple models in one file using PyTorch</font></a> \n",
    "    \n",
    "    * <a href=\"https://pytorch.org/tutorials/recipes/recipes/save_load_across_devices.html\" style=\"text-decoration:none;\">Saving and loading models across devices in PyTorch</a>\n",
    "    \n",
    "    *\n",
    "    \n",
    "    * <a href=\"https://pytorch.org/tutorials/recipes/recipes/benchmark.html\" style=\"text-decoration:none;\">PyTorch Benchmark</a>\n",
    "    \n",
    "    * <a href=\"https://pytorch.org/tutorials/recipes/recipes/timer_quick_start.html\" style=\"text-decoration:none;\">PyTorch Benchmark (quick start)</a> 【Timer quick start】\n",
    "    \n",
    "    *\n",
    "    \n",
    "    * <a href=\"https://pytorch.org/tutorials/recipes/recipes/Captum_Recipe.html\" style=\"text-decoration:none;\">Model Interpretability using Captum</a>\n",
    "    \n",
    "    * <a href=\"https://pytorch.org/tutorials/recipes/recipes/tensorboard_with_pytorch.html\" style=\"text-decoration:none;\">How to use TensorBoard with PyTorch</a>\n",
    "\n",
    "    * \n",
    "    * \n",
    "    \n",
    "    * <a href=\"https://pytorch.org/tutorials/recipes/fuse.html\" style=\"text-decoration:none;\">Fuse Modules Recipe</a>\n",
    "    \n",
    "    * <a href=\"https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html\" style=\"text-decoration:none;\">PyTorch Profiler</a>\n",
    "    \n",
    "    * <a href=\"https://pytorch.org/tutorials/recipes/recipes/dynamic_quantization.html\" style=\"text-decoration:none;\">Dynamic Quantization</a>\n",
    "    \n",
    "    *\n",
    "    \n",
    "    * <a href=\"https://pytorch.org/tutorials/recipes/deployment_with_flask.html\" style=\"text-decoration:none;\"><font color=maroon><b>Deploying with Flask</b></font></a>\n",
    "    \n",
    "    * <a href=\"https://pytorch.org/tutorials/recipes/torchscript_inference.html\" style=\"text-decoration:none;\"><font color=maroon><b>TorchScript for Deployment</b></font></a>\n",
    "    \n",
    "    * <a href=\"https://pytorch.org/tutorials/recipes/script_optimized.html\" style=\"text-decoration:none;\">Script and Optimize for Mobile Recipe</a>\n",
    "    \n",
    "    * <a href=\"https://pytorch.org/tutorials/recipes/quantization.html\" style=\"text-decoration:none;\">Quantization for Mobile Recipe</a> 【Quantization Recipe】\n",
    "    \n",
    "    * <a href=\"https://pytorch.org/tutorials/recipes/mobile_perf.html\" style=\"text-decoration:none;\">Pytorch Mobile Performance Recipes</a> \n",
    "    \n",
    "    * <a href=\"https://pytorch.org/tutorials/recipes/model_preparation_android.html\" style=\"text-decoration:none;\">Model Preparation for Android Recipe</a>\n",
    "    \n",
    "    * <a href=\"https://pytorch.org/tutorials/recipes/model_preparation_ios.html\" style=\"text-decoration:none;\">Model Preparation for iOS Recipe</a>\n",
    "    \n",
    "    * <a href=\"https://pytorch.org/tutorials/recipes/mobile_interpreter.html\" style=\"text-decoration:none;\"> Mobile Interpreter Workflow in Android and iOS</a> 【(beta) Efficient mobile interpreter in Android and iOS】\n",
    "    \n",
    "    * <a href=\"https://pytorch.org/tutorials/recipes/android_native_app_with_custom_op.html\" style=\"text-decoration:none;\">Making Native Android Application that uses PyTorch prebuilt libraries</a>\n",
    "    \n",
    "    *\n",
    "    \n",
    "    * <a href=\"https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html\" style=\"text-decoration:none;\"><font color=maroon><b>Performance Tuning Guide</b></font></a>\n",
    "    \n",
    "    * <a href=\"https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html\" style=\"text-decoration:none;\">Automatic Mixed Precision</a>\n",
    "    \n",
    "    * <a href=\"https://pytorch.org/tutorials/recipes/distributed_rpc_profiling.html\" style=\"text-decoration:none;\">Profiling PyTorch RPC-Based Workloads</a>\n",
    "    \n",
    "    * <a href=\"https://pytorch.org/tutorials/recipes/intel_extension_for_pytorch.html\" style=\"text-decoration:none;\">Intel® Extension for PyTorch*</a>\n",
    "    \n",
    "    * <a href=\"https://pytorch.org/tutorials/recipes/intel_neural_compressor_for_pytorch.html\" style=\"text-decoration:none;\">Intel® Neural Compressor for PyTorch</a>  【Ease-of-use quantization for PyTorch with Intel® Neural Compressor】\n",
    "    \n",
    "    * <a href=\"https://pytorch.org/tutorials/recipes/zero_redundancy_optimizer.html\" style=\"text-decoration:none;\">Shard Optimizer States with ZeroRedundancyOptimizer</a>\n",
    "    \n",
    "    * <a href=\"https://pytorch.org/tutorials/recipes/cuda_rpc.html\" style=\"text-decoration:none;\">Direct Device-to-Device Communication with TensorPipe CUDA RPC</a>\n",
    "    \n",
    "    * <a href=\"https://pytorch.org/tutorials/recipes/distributed_optim_torchscript.html\" style=\"text-decoration:none;\">Distributed Optimizer with TorchScript support</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# <font style=\"font-size:120%;font-weight:bold\"></font> <a href=\"\" style=\"text-decoration:none;\"><font size=3>[ link ]</font></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"font-size:120%;font-weight:bold\">Warmstarting model using parameters from a different model in PyTorch</font> <a href=\"https://pytorch.org/tutorials/recipes/recipes/warmstarting_model_using_parameters_from_a_different_model.html\" style=\"text-decoration:none;\"><font size=3>[ link ]</font></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partially loading a model or loading a partial model are common scenarios when **transfer learning** or training a new complex model. Leveraging trained parameters, even if only a few are usable, will help to warmstart the training process and hopefully help your model converge much faster than training from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whether you are loading from a partial `state_dict`, which is missing some keys, or loading a `state_dict` with more keys than the model that you are loading into, you can set the strict argument to `False` in the `load_state_dict()` function to ignore non-matching keys. In this recipe, we will experiment with warmstarting a model using parameters of a different model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps\n",
    "* Import all necessary libraries for loading our data\n",
    "* Define and intialize the neural network A and B\n",
    "* Save model A\n",
    "* Load into model B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries for loading our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and intialize the neural network A and B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For sake of example, we will create a neural network for training images. To learn more see the `Defining a Neural Network recipe`. We will create two neural networks for sake of loading one parameter of type A into type B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetA(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NetA, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16*5*5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16*5*5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetB(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NetB, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16*5*5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16*5*5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))     \n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "netA = NetA()\n",
    "netB = NetB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify a path to save to\n",
    "PATH = \"./model_weights/model_A.pt\"\n",
    "\n",
    "torch.save(netA.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load into model B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to load parameters from one layer to another, but some keys do not match, simply change the name of the parameter keys in the state_dict that you are loading to match the keys in the model that you are loading into.\n",
    "\n",
    "* <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.state_dict\" style=\"text-decoration:none;\">torch.nn.Module.state_dict(destination=None, prefix='', keep_vars=False)</a>\n",
    "* <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.load_state_dict\" style=\"text-decoration:none;\">torch.nn.Module.load_state_dict(state_dict, strict=True)</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netB.load_state_dict(torch.load(PATH), strict=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You have successfully warmstarted a model using parameters from a different model in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn More"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at these other recipes to continue your learning:\n",
    "\n",
    "* <a href=\"https://pytorch.org/tutorials/recipes/recipes/saving_multiple_models_in_one_file.html\" style=\"text-decoration:none;\">Saving and loading multiple models in one file using PyTorch</a>\n",
    "* <a href=\"https://pytorch.org/tutorials/recipes/recipes/save_load_across_devices.html\" style=\"text-decoration:none;\">Saving and loading models across devices in PyTorch</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"font-size:120%;font-weight:bold\"> Saving and loading multiple models in one file using PyTorch</font> <a href=\"https://pytorch.org/tutorials/recipes/recipes/saving_multiple_models_in_one_file.html\" style=\"text-decoration:none;\"><font size=3>[ link ]</font></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving and loading multiple models can be helpful for reusing models that you have previously trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>When saving a model comprised of multiple `torch.nn.Modules`, such as a GAN, a sequence-to-sequence model, or an ensemble of models, you must save a dictionary of each model’s state_dict and corresponding optimizer. You can also save any other items that may aid you in resuming training by simply appending them to the dictionary. \n",
    "</font>\n",
    "\n",
    "\n",
    "<font size=3>To load the models, first initialize the models and optimizers, then load the dictionary locally using `torch.load()`. From here, you can easily access the saved items by simply querying the dictionary as you would expect.</font>\n",
    "\n",
    "In this recipe, we will demonstrate how to save multiple models to one file using PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps\n",
    "* Import all necessary libraries for loading our data\n",
    "* Define and intialize the neural network\n",
    "* Initialize the optimizer\n",
    "* Save multiple models\n",
    "* Load multiple models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries for loading our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and intialize the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "netA = Net()\n",
    "netB = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use SGD with momentum to build an optimizer for each model we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizerA = optim.SGD(netA.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizerB = optim.SGD(netB.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save multiple models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect all relevant information and build your dictionary.\n",
    "\n",
    "* <a href=\"https://pytorch.org/docs/stable/generated/torch.save.html\" style=\"text-decoration:none;\"> torch.save(obj, f, pickle_module=pickle, pickle_protocol=DEFAULT_PROTOCOL, _use_new_zipfile_serialization=True)</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify a path to save to\n",
    "PATH = \"./model_weights/models_A_B.pt\"\n",
    "\n",
    "torch.save({\n",
    "            'modelA_state_dict': netA.state_dict(),\n",
    "            'modelB_state_dict': netB.state_dict(),\n",
    "            'optimizerA_state_dict': optimizerA.state_dict(),\n",
    "            'optimizerB_state_dict': optimizerB.state_dict(),\n",
    "            }, PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load multiple models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=maroon>Remember to first initialize the models and optimizers, then load the dictionary locally.</font>\n",
    "\n",
    "* <a href=\"https://pytorch.org/docs/stable/generated/torch.load.html\" style=\"text-decoration:none;\">torch.load(f, map_location=None, pickle_module=pickle, **pickle_load_args)</a>\n",
    "* <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.eval\" style=\"text-decoration:none;\">torch.nn.Module.eval()</a>\n",
    "* <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.train\" style=\"text-decoration:none;\">torch.nn.Module.train(mode=True)</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelA = Net()\n",
    "modelB = Net()\n",
    "optimModelA = optim.SGD(modelA.parameters(), lr=0.001, momentum=0.9)\n",
    "optimModelB = optim.SGD(modelB.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "checkpoint = torch.load(PATH)\n",
    "modelA.load_state_dict(checkpoint['modelA_state_dict'])\n",
    "modelB.load_state_dict(checkpoint['modelB_state_dict'])\n",
    "optimizerA.load_state_dict(checkpoint['optimizerA_state_dict'])\n",
    "optimizerB.load_state_dict(checkpoint['optimizerB_state_dict'])\n",
    "\n",
    "modelA.eval()\n",
    "modelB.eval()\n",
    "\n",
    "# - or -\n",
    "# modelA.train()\n",
    "# modelB.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=maroon>You must call `model.eval()` to set dropout and batch normalization layers to evaluation mode before running inference. Failing to do this will yield inconsistent inference results.\n",
    "\n",
    "If you wish to resuming training, call `model.train()` to ensure these layers are in training mode.</font>\n",
    "\n",
    "Congratulations! You have successfully saved and loaded multiple models in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:red;font-size:120%;font-weight:bold\">Performance Tuning Guide</font> <a href=\"https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html\" style=\"text-decoration:none;\"><font size=3>[ link ]</font></a>\n",
    "\n",
    "<br>\n",
    "\n",
    "**Author:** \n",
    "<a href=\"https://github.com/szmigacz\" style=\"text-decoration:none;\"><b>Szymon Migacz</b></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance Tuning Guide is a set of optimizations and best practices which can accelerate training and inference of deep learning models in PyTorch. \n",
    "\n",
    "Presented techniques often can be implemented by changing only a few lines of code and can be applied to a wide range of deep learning models across all domains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:red;font-size:110%\">General optimizations</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enable async data loading and augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\" style=\"text-decoration:none;\"><font size=3 color=maroon><b>torch.utils.data.DataLoader</b></font></a> supports asynchronous data loading and data augmentation in separate worker subprocesses. The default setting for **`DataLoader`** is <font color=maroon>num_workers=0</font>, which means that the data loading is synchronous and done in the main process. As a result the main training process has to wait for the data to be available to continue the execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Setting <font color=maroon>num_workers > 0</font> enables asynchronous data loading and overlap between the training and data loading. <font color=maroon>num_workers</font> should be tuned depending on the workload, CPU, GPU, and location of training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **`DataLoader`** accepts <font color=maroon>pin_memory</font> argument, which defaults to <font color=maroon>False</font>. When using a GPU it’s better to set <font color=maroon>pin_memory=True</font>, this instructs **`DataLoader`** to use pinned memory and enables faster and asynchronous memory copy from the host to the GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disable gradient calculation for validation or inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch saves intermediate buffers from all operations which involve tensors that require gradients. Typically gradients aren’t needed for validation or inference. \n",
    "\n",
    "* <a href=\"https://pytorch.org/docs/stable/generated/torch.no_grad.html#torch.no_grad\" style=\"text-decoration:none;\"><font color=maroon><b>torch.no_grad()</b></font></a> context manager can be applied to disable gradient calculation within a specified block of code, this accelerates execution and reduces the amount of required memory. \n",
    "    \n",
    "    \n",
    "* **`torch.no_grad()`** can also be used as a function decorator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disable bias for convolutions directly followed by a batch norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d\" style=\"text-decoration:none;\"><font color=maroon><b>torch.nn.Conv2d()</b></font></a> has <font color=maroon>bias</font> parameter which defaults to <font color=maroon>True</font> (the same is true for **`Conv1d`** and **`Conv3d`** )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a **`nn.Conv2d`** layer is directly followed by a **`nn.BatchNorm2d`** layer, then the bias in the convolution is not needed, instead use **`nn.Conv2d(..., bias=False, ....)`**. Bias is not needed because in the first step **`BatchNorm`** subtracts the mean, which effectively cancels out the effect of bias.</font>\n",
    "\n",
    "This is also applicable to 1d and 3d convolutions as long as **`BatchNorm`** (or other normalization layer) normalizes on the same dimension as convolution’s bias.\n",
    "\n",
    "Models available from <a href=\"https://github.com/pytorch/vision\" style=\"text-decoration:none;\">torchvision</a> torchvision already implement this optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use **`parameter.grad = None`** instead of **`model.zero_grad()`** or **`optimizer.zero_grad()`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of calling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.zero_grad()\n",
    "\n",
    "# or\n",
    "\n",
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to zero out gradients, use the following method instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.grad = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second code snippet does not zero the memory of each individual parameter, also the subsequent backward pass uses assignment instead of addition to store gradients, this reduces the number of memory operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting gradient to `None` has a slightly different numerical behavior than setting it to `zero`, for more details refer to the <a href=\"https://pytorch.org/docs/master/optim.html#torch.optim.Optimizer.zero_grad\" style=\"text-decoration:none;\">documentation</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, starting from PyTorch 1.7, call **`model`** or **`optimizer.zero_grad(set_to_none=True)`**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuse pointwise operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pointwise operations (`elementwise addition, multiplication,` `math functions - `**`sin()`**, **`cos()`**, **`sigmoid()`** etc.) can be fused into a single kernel to amortize memory access time and kernel launch time.\n",
    "\n",
    "<a href=\"https://pytorch.org/docs/stable/jit.html\" style=\"text-decoration:none;\">PyTorch JIT</a> can fuse kernels automatically, although there could be additional fusion opportunities not yet implemented in the compiler, and not all device types are supported equally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pointwise operations are memory-bound, for each operation PyTorch launches a separate kernel. Each kernel loads data from the memory, performs computation (this step is usually inexpensive) and stores results back into the memory.\n",
    "\n",
    "Fused operator launches only one kernel for multiple fused pointwise ops and loads/stores data only once to the memory. This makes JIT very useful for activation functions, optimizers, custom RNN cells etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the simplest case fusion can be enabled by applying <a href=\"https://pytorch.org/docs/stable/generated/torch.jit.script.html#torch.jit.script\" style=\"text-decoration:none;\">torch.jit.script</a> decorator to the function definition, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.jit.script\n",
    "def fused_gelu(x):\n",
    "    return x * 0.5 * (1.0 + torch.erf(x / 1.41421))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to <a href=\"https://pytorch.org/docs/stable/jit.html\" style=\"text-decoration:none;\">TorchScript documentation</a> for more advanced use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enable channels_last memory format for computer vision models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch 1.5 introduced support for **`channels_last`** memory format for convolutional networks. This format is meant to be used in conjunction with <a href=\"https://pytorch.org/docs/stable/amp.html\" style=\"text-decoration:none;\">AMP</a> to further accelerate convolutional neural networks with <a href=\"https://www.nvidia.com/en-us/data-center/tensor-cores/\" style=\"text-decoration:none;\">Tensor Cores</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support for **`channels_last`** is experimental, but it’s expected to work for standard computer vision models (e.g. ResNet-50, SSD). To convert models to **`channels_last`** format follow <a href=\"https://pytorch.org/tutorials/intermediate/memory_format_tutorial.html\" style=\"text-decoration:none;\">Channels Last Memory Format Tutorial</a>. The tutorial includes a section on <a href=\"https://pytorch.org/tutorials/intermediate/memory_format_tutorial.html#converting-existing-models\" style=\"text-decoration:none;\">converting existing models</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint intermediate buffers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Buffer checkpointing is a technique to mitigate the memory capacity burden of model training. Instead of storing inputs of all layers to compute upstream gradients in backward propagation, it stores the inputs of a few layers and the others are recomputed during backward pass. The reduced memory requirements enables increasing the batch size that can improve utilization.\n",
    "\n",
    "Checkpointing targets should be selected carefully. The best is not to store large layer outputs that have small re-computation cost. The example target layers are activation functions (e.g. **`ReLU`**, **`Sigmoid`**, **`Tanh`**), up/down sampling and matrix-vector operations with small accumulation depth.\n",
    "\n",
    "PyTorch supports a native <a href=\"https://pytorch.org/docs/stable/checkpoint.html\" style=\"text-decoration:none;\">torch.utils.checkpoint</a> API to automatically perform checkpointing and recomputation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disable debugging APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many PyTorch APIs are intended for debugging and should be disabled for regular training runs:\n",
    "\n",
    "* anomaly detection: <a href=\"https://pytorch.org/docs/stable/autograd.html#torch.autograd.detect_anomaly\" style=\"text-decoration:none;\">torch.autograd.detect_anomaly</a> or <a href=\"https://pytorch.org/docs/stable/autograd.html#torch.autograd.set_detect_anomaly\" style=\"text-decoration:none;\">torch.autograd.set_detect_anomaly(True)</a>\n",
    "\n",
    "\n",
    "* profiler related: <a href=\"https://pytorch.org/docs/stable/autograd.html#torch.autograd.profiler.emit_nvtx\" style=\"text-decoration:none;\">torch.autograd.profiler.emit_nvtx</a>, <a href=\"https://pytorch.org/docs/stable/autograd.html#torch.autograd.profiler.profile\" style=\"text-decoration:none;\">torch.autograd.profiler.profile</a>\n",
    "\n",
    "\n",
    "* autograd gradcheck: <a href=\"https://pytorch.org/docs/stable/autograd.html#torch.autograd.gradcheck\" style=\"text-decoration:none;\">torch.autograd.gradcheck</a> or <a href=\"https://pytorch.org/docs/stable/autograd.html#torch.autograd.gradgradcheck\" style=\"text-decoration:none;\">torch.autograd.gradgradcheck</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:red;font-size:110%\">CPU specific optimizations</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilize Non-Uniform Memory Access (NUMA) Controls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NUMA or non-uniform memory access is a memory layout design used in data center machines meant to take advantage of locality of memory in multi-socket machines with multiple memory controllers and blocks. Generally speaking, all deep learning workloads, training or inference, get better performance without accessing hardware resources across NUMA nodes. Thus, inference can be run with multiple instances, each instance runs on one socket, to raise throughput. For training tasks on single node, distributed training is recommended to make each training process run on one socket.\n",
    "\n",
    "In general cases the following command executes a PyTorch script on cores on the Nth node only, and avoids cross-socket memory access to reduce memory access overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numactl --cpunodebind=N --membind=N python <pytorch_script>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More detailed descriptions can be found <a href=\"https://software.intel.com/content/www/us/en/develop/articles/how-to-get-better-performance-on-pytorchcaffe2-with-intel-acceleration.html\" style=\"text-decoration:none;\">here</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilize OpenMP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenMP is utilized to bring better performance for parallel computation tasks. OMP_NUM_THREADS is the easiest switch that can be used to accelerate computations. It determines number of threads used for OpenMP computations. CPU affinity setting controls how workloads are distributed over multiple cores. It affects communication overhead, cache line invalidation overhead, or page thrashing, thus proper setting of CPU affinity brings performance benefits. GOMP_CPU_AFFINITY or KMP_AFFINITY determines how to bind OpenMP* threads to physical processing units. Detailed information can be found <a href=\"https://software.intel.com/content/www/us/en/develop/articles/how-to-get-better-performance-on-pytorchcaffe2-with-intel-acceleration.html\" style=\"text-decoration:none;\">here</a>.\n",
    "\n",
    "With the following command, PyTorch run the task on N OpenMP threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export OMP_NUM_THREADS=N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, the following environment variables are used to set for CPU affinity with GNU OpenMP implementation. OMP_PROC_BIND specifies whether threads may be moved between processors. Setting it to CLOSE keeps OpenMP threads close to the primary thread in contiguous place partitions. OMP_SCHEDULE determines how OpenMP threads are scheduled. GOMP_CPU_AFFINITY binds threads to specific CPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export OMP_SCHEDULE=STATIC\n",
    "# export OMP_PROC_BIND=CLOSE\n",
    "# export GOMP_CPU_AFFINITY=\"N-M\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intel OpenMP Runtime Library (libiomp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, PyTorch uses GNU OpenMP (GNU libgomp) for parallel computation. On Intel platforms, Intel OpenMP Runtime Library (libiomp) provides OpenMP API specification support. It sometimes brings more performance benefits compared to libgomp. Utilizing environment variable LD_PRELOAD can switch OpenMP library to libiomp:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export LD_PRELOAD=<path>/libiomp5.so:$LD_PRELOAD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to CPU affinity settings in GNU OpenMP, environment variables are provided in libiomp to control CPU affinity settings. KMP_AFFINITY binds OpenMP threads to physical processing units. KMP_BLOCKTIME sets the time, in milliseconds, that a thread should wait, after completing the execution of a parallel region, before sleeping. In most cases, setting KMP_BLOCKTIME to 1 or 0 yields good performances. The following commands show a common settings with Intel OpenMP Runtime Library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export KMP_AFFINITY=granularity=fine,compact,1,0\n",
    "# export KMP_BLOCKTIME=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Switch Memory allocator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For deep learning workloads, Jemalloc or TCMalloc can get better performance by reusing memory as much as possible than default malloc funtion. \n",
    "\n",
    "<a href=\"https://github.com/jemalloc/jemalloc\" style=\"text-decoration:none;\">Jemalloc</a> is a general purpose malloc implementation that emphasizes fragmentation avoidance and scalable concurrency support. \n",
    "\n",
    "<a href=\"https://google.github.io/tcmalloc/overview.html\" style=\"text-decoration:none;\">TCMalloc</a> also features a couple of optimizations to speed up program executions. \n",
    "\n",
    "One of them is holding memory in caches to speed up access of commonly-used objects. Holding such caches even after deallocation also helps avoid costly system calls if such memory is later re-allocated. Use environment variable LD_PRELOAD to take advantage of one of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export LD_PRELOAD=<jemalloc.so/tcmalloc.so>:$LD_PRELOAD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a model on CPU with PyTorch DistributedDataParallel(DDP) functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For small scale models or memory-bound models, such as DLRM, training on CPU is also a good choice. On a machine with multiple sockets, distributed training brings a high-efficient hardware resource usage to accelerate the training process. \n",
    "\n",
    "<a href=\"https://github.com/intel/torch-ccl\" style=\"text-decoration:none;\">Torch-ccl</a>, optimized with Intel(R) oneCCL (collective commnications library) for efficient distributed deep learning training implementing such collectives like allreduce, allgather, alltoall, implements PyTorch C10D ProcessGroup API and can be dynamically loaded as external ProcessGroup. \n",
    "\n",
    "Upon optimizations implemented in PyTorch DDP moduel, torhc-ccl accelerates communication operations. Beside the optimizations made to communication kernels, torch-ccl also features simultaneous computation-communication functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:red;font-size:110%\">GPU specific optimizations</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enable cuDNN auto-tuner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://developer.nvidia.com/cudnn\" style=\"text-decoration:none;\">NVIDIA cuDNN</a> supports many algorithms to compute a convolution. Autotuner runs a short benchmark and selects the kernel with the best performance on a given hardware for a given input size.\n",
    "\n",
    "For convolutional networks (other types currently not supported), enable cuDNN autotuner before launching the training loop by setting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "* the auto-tuner decisions may be non-deterministic; different algorithm may be selected for different runs. For more details see <a href=\"https://pytorch.org/docs/stable/notes/randomness.html?highlight=determinism\" style=\"text-decoration:none;\">PyTorch: Reproducibility</a>\n",
    "\n",
    "\n",
    "* in some rare cases, such as with highly variable input sizes, it’s better to run convolutional networks with autotuner disabled to avoid the overhead associated with algorithm selection for each input size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avoid unnecessary CPU-GPU synchronization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avoid unnecessary synchronizations, to let the CPU run ahead of the accelerator as much as possible to make sure that the accelerator work queue contains many operations.\n",
    "\n",
    "When possible, avoid operations which require synchronizations, for example:\n",
    "\n",
    "* **`print(cuda_tensor)`**\n",
    "\n",
    "\n",
    "* **`cuda_tensor.item()`**\n",
    "\n",
    "\n",
    "* memory copies: **`tensor.cuda()`**, **`uda_tensor.cpu()`** and equivalent **`ensor.to(device)`** calls\n",
    "\n",
    "\n",
    "* **`cuda_tensor.nonzero()`**\n",
    "\n",
    "\n",
    "* python control flow which depends on results of operations performed on cuda tensors e.g. **`if (cuda_tensor != 0).all()`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create tensors directly on the target device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of calling **`torch.rand(size).cuda()`** to generate a random tensor, produce the output directly on the target device: **`torch.rand(size, device=torch.device('cuda'))`**.\n",
    "\n",
    "This is applicable to all functions which create new tensors and accept **`device`**` argument`: <a href=\"https://pytorch.org/docs/stable/generated/torch.rand.html#torch.rand\" style=\"text-decoration:none;\">torch.rand()</a>, <a href=\"https://pytorch.org/docs/stable/generated/torch.zeros.html#torch.zeros\" style=\"text-decoration:none;\">torch.zeros()</a>, <a href=\"https://pytorch.org/docs/stable/generated/torch.full.html#torch.full\" style=\"text-decoration:none;\">torch.full()</a> and similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use mixed precision and AMP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mixed precision leverages <a href=\"https://www.nvidia.com/en-us/data-center/tensor-cores/\" style=\"text-decoration:none;\"><font size=3 color=maroon><b>Tensor Cores</b></font></a> and offers up to 3x overall speedup on Volta and newer GPU architectures. To use Tensor Cores AMP should be enabled and matrix/tensor dimensions should satisfy requirements for calling kernels that use Tensor Cores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use Tensor Cores:\n",
    "* set sizes to multiples of 8 (to map onto dimensions of Tensor Cores)\n",
    "    * see <a href=\"https://docs.nvidia.com/deeplearning/performance/index.html#optimizing-performance\" style=\"text-decoration:none;\">Deep Learning Performance Documentation</a> for more details and guidelines specific to layer type\n",
    "    * if layer size is derived from other parameters rather than fixed, it can still be explicitly padded e.g. vocabulary size in NLP models\n",
    "\n",
    "\n",
    "* enable AMP\n",
    "    * Introduction to Mixed Precision Training and AMP: <a href=\"https://www.youtube.com/watch?v=jF4-_ZK_tyc&feature=youtu.be\" style=\"text-decoration:none;\">video</a>, <a href=\"https://nvlabs.github.io/eccv2020-mixed-precision-tutorial/files/dusan_stosic-training-neural-networks-with-tensor-cores.pdf\" style=\"text-decoration:none;\">slides</a>\n",
    "    * native PyTorch AMP is available starting from PyTorch 1.6: <a href=\"https://pytorch.org/docs/stable/amp.html\" style=\"text-decoration:none;\">documentation</a>, <a href=\"https://pytorch.org/docs/stable/notes/amp_examples.html#amp-examples\" style=\"text-decoration:none;\">examples</a>, <a href=\"https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html\" style=\"text-decoration:none;\">tutorial</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-allocate memory in case of variable input length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models for speech recognition or for NLP are often trained on input tensors with variable sequence length. Variable length can be problematic for PyTorch caching allocator and can lead to reduced performance or to unexpected out-of-memory errors. If a batch with a short sequence length is followed by an another batch with longer sequence length, then PyTorch is forced to release intermediate buffers from previous iteration and to re-allocate new buffers. This process is time consuming and causes fragmentation in the caching allocator which may result in out-of-memory errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical solution is to implement pre-allocation. It consists of the following steps:\n",
    "1. generate a (usually random) batch of inputs with maximum sequence length (either corresponding to max length in the training dataset or to some predefined threshold)\n",
    "\n",
    "\n",
    "2. execute a forward and a backward pass with the generated batch, do not execute an optimizer or a learning rate scheduler, this step pre-allocates buffers of maximum size, which can be reused in subsequent training iterations\n",
    "\n",
    "\n",
    "3. zero out gradients\n",
    "\n",
    "\n",
    "4. proceed to regular training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:red;font-size:110%\">Distributed optimizations</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use efficient data-parallel backend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch has two ways to implement data-parallel training:\n",
    "\n",
    "* <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html#torch.nn.DataParallel\" style=\"text-decoration:none;\"><font color=maroon><b>torch.nn.DataParallel</b></font></a>\n",
    "\n",
    "\n",
    "* <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel\" style=\"text-decoration:none;\"><font color=maroon><b>torch.nn.parallel.DistributedDataParallel</b></font></a>\n",
    "\n",
    "\n",
    "**`DistributedDataParallel`** offers much better performance and scaling to multiple-GPUs. For more information refer to the relevant section of <a href=\"https://pytorch.org/docs/stable/notes/cuda.html#use-nn-parallel-distributeddataparallel-instead-of-multiprocessing-or-nn-dataparallel\" style=\"text-decoration:none;\">CUDA Best Practices</a> from PyTorch documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip unnecessary all-reduce if training with DistributedDataParallel and gradient accumulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel\" style=\"text-decoration:none;\">torch.nn.parallel.DistributedDataParallel</a> executes gradient all-reduce after every backward pass to compute the average gradient over all workers participating in the training. If training uses gradient accumulation over N steps, then all-reduce is not necessary after every training step, it’s only required to perform all-reduce after the last call to backward, just before the execution of the optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`DistributedDataParallel`** provides <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel.no_sync\" style=\"text-decoration:none;\"><font color=maroon>no_sync()</font></a> context manager which disables gradient all-reduce for particular iteration. **`no_sync()`** should be applied to first **`N-1`** iterations of gradient accumulation, the last iteration should follow the default execution and perform the required gradient all-reduce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match the order of layers in constructors and during the execution if using DistributedDataParallel`(find_unused_parameters=True)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel\" style=\"text-decoration:none;\">torch.nn.parallel.DistributedDataParallel</a> with `find_unused_parameters=True` uses the order of layers and parameters from model constructors to build buckets for `DistributedDataParallel` gradient all-reduce. `DistributedDataParallel` overlaps all-reduce with the backward pass. All-reduce for a particular bucket is asynchronously triggered only when all gradients for parameters in a given bucket are available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To maximize the amount of overlap, the order in model constructors should roughly match the order during the execution. If the order doesn’t match, then all-reduce for the entire bucket waits for the gradient which is the last to arrive, this may reduce the overlap between backward pass and all-reduce, all-reduce may end up being exposed, which slows down the training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DistributedDataParallel` with `find_unused_parameters=False` (which is the default setting) relies on automatic bucket formation based on order of operations encountered during the backward pass. With `find_unused_parameters=False` it’s not necessary to reorder layers or parameters to achieve optimal performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load-balance workload in a distributed setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load imbalance typically may happen for models processing sequential data (speech recognition, translation, language models etc.). If one device receives a batch of data with sequence length longer than sequence lengths for the remaining devices, then all devices wait for the worker which finishes last. Backward pass functions as an implicit synchronization point in a distributed setting with <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel\" style=\"text-decoration:none;\">DistributedDataParallel</a> backend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple ways to solve the load balancing problem. The core idea is to distribute workload over all workers as uniformly as possible within each global batch. For example Transformer solves imbalance by forming batches with approximately constant number of tokens (and variable number of sequences in a batch), other models solve imbalance by bucketing samples with similar sequence length or even by sorting dataset by sequence length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"font-size:120%;font-weight:bold\">Deploying with Flask</font> <a href=\"https://pytorch.org/tutorials/recipes/deployment_with_flask.html\" style=\"text-decoration:none;\"><font size=3>[ link ]</font></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this recipe, you will learn:\n",
    "\n",
    "* How to wrap your trained PyTorch model in a Flask container to expose it via a web API\n",
    "\n",
    "\n",
    "* How to translate incoming web requests into PyTorch tensors for your model\n",
    "\n",
    "\n",
    "* How to package your model’s output for an HTTP response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need a Python 3 environment with the following packages (and their dependencies) installed:\n",
    "\n",
    "* PyTorch 1.5\n",
    "* TorchVision 0.6.0\n",
    "* Flask 1.1\n",
    "\n",
    "Optionally, to get some of the supporting files, you’ll need `git`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The instructions for installing PyTorch and TorchVision are available at <a href=\"https://pytorch.org/\" style=\"text-decoration:none;\">pytorch.org</a>. Instructions for installing Flask are available on <a href=\"https://flask.palletsprojects.com/en/1.1.x/installation/\" style=\"text-decoration:none;\">the Flask site</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Flask?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flask is a lightweight web server written in Python. It provides a convenient way for you to quickly set up a web API for predictions from your trained PyTorch model, either for direct use, or as a web service within a larger system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Supporting Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’re going to create a web service that takes in images, and maps them to one of the 1000 classes of the ImageNet dataset. To do this, you’ll need an image file for testing. Optionally, you can also get a file that will map the class index output by the model to a human-readable class name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: To Get Both Files Quickly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can pull both of the supporting files quickly by checking out the TorchServe repository and copying them to your working folder. (NB: There is no dependency on TorchServe for this tutorial - it’s just a quick way to get the files.) Issue the following commands from your shell prompt:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "git clone https://github.com/pytorch/serve\n",
    "cp serve/examples/image_classifier/kitten.jpg .\n",
    "cp serve/examples/image_classifier/index_to_name.json ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And you’ve got them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Bring Your Own Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `index_to_name.json` file is optional in the Flask service below. You can test your service with your own image - just make sure it’s a 3-color JPEG."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:blue;font-size:110%\">Building Your Flask Service</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full Python script for the Flask service is shown at the end of this recipe; you can copy and paste that into your own `app.py` file. Below we’ll look at individual sections to make their functions clear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from flask import Flask, jsonify, request\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order:\n",
    "\n",
    "* We’ll be using a pre-trained DenseNet model from `torchvision.models`\n",
    "* `torchvision.transforms` contains tools for manipulating your image data\n",
    "* Pillow (`PIL`) is what we’ll use to load the image file initially\n",
    "* And of course we’ll need classes from `flask`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def transform_image(infile):\n",
    "    input_transforms = [transforms.Resize(255),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "            [0.229, 0.224, 0.225])]\n",
    "    my_transforms = transforms.Compose(input_transforms)\n",
    "    image = Image.open(infile)\n",
    "    timg = my_transforms(image)\n",
    "    timg.unsqueeze_(0)\n",
    "    return timg\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The web request gave us an image file, but our model expects a PyTorch tensor of shape `(N, 3, 224, 224)` where N is the number of items in the input batch. (We will just have a batch size of 1.) The first thing we do is compose a set of TorchVision transforms that resize and crop the image, convert it to a tensor, then normalize the values in the tensor. (For more information on this normalization, see the documentation for `torchvision.models_` (see codes below).)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, we open the file and apply the transforms. The transforms return a tensor of shape `(3, 224, 224)` - the 3 color channels of a 224x224 image. Because we need to make this single image a batch, we use the `unsqueeze_(0)` call to modify the tensor in place by adding a new first dimension. The tensor contains the same data, but now has shape `(1, 3, 224, 224)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3 color=maroon>In general, even if you’re not working with image data, you will need to transform the input from your HTTP request into a tensor that PyTorch can consume.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def get_prediction(input_tensor):\n",
    "    outputs = model.forward(input_tensor)\n",
    "    _, y_hat = outputs.max(1)\n",
    "    prediction = y_hat.item()\n",
    "    return prediction\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inference itself is the simplest part: When we pass the input tensor to them model, we get back a tensor of values that represent the model’s estimated likelihood that the image belongs to a particular class. The `max()` call finds the class with the maximum likelihood value, and returns that value with the ImageNet class index. Finally, we extract that class index from the tensor containing it with the `item()` call, and return it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def render_prediction(prediction_idx):\n",
    "    stridx = str(prediction_idx)\n",
    "    class_name = 'Unknown'\n",
    "    if img_class_map is not None:\n",
    "        if stridx in img_class_map is not None:\n",
    "            class_name = img_class_map[stridx][1]\n",
    "\n",
    "    return prediction_idx, class_name\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `render_prediction()` method maps the predicted class index to a human-readable class label. It’s typical, after getting the prediction from your model, to perform post-processing to make the prediction ready for either human consumption, or for another piece of software."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:magenta;font-size:110%\">Running The Full Flask App</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paste the following into a file called `app.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "import os\n",
    "\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from flask import Flask, jsonify, request\n",
    "\n",
    "\n",
    "app = Flask(__name__)\n",
    "model = models.densenet121(pretrained=True)    # rained on 1000 classes from ImageNet\n",
    "model.eval()                                   # Turns off autograd\n",
    "\n",
    "\n",
    "img_class_map = None\n",
    "mapping_file_path = 'index_to_name.json'       # Human-readable names for Imagenet classes\n",
    "if os.path.isfile(mapping_file_path):\n",
    "    with open(mapping_file_path) as f:\n",
    "        img_class_map = json.load(f)\n",
    "        \n",
    "\n",
    "\n",
    "# Transform input into the form our model expects\n",
    "# We use multiple TorchVision transforms to ready the image\n",
    "def transform_image(infile):\n",
    "    input_transforms = [transforms.Resize(255),\n",
    "                        transforms.CenterCrop(224),\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                                             [0.229, 0.224, 0.225])]\n",
    "    \n",
    "    my_transforms = transforms.Compose(input_transforms)\n",
    "    image = Image.open(infile)        # Open the image file\n",
    "    timg = my_transforms(image)       # Transform PIL image to appropriately-shaped PyTorch tensor\n",
    "    timg.unsqueeze_(0)                # PyTorch models expect batched input; create a batch of 1\n",
    "    \n",
    "    return timg\n",
    "\n",
    "\n",
    "\n",
    "# Get a prediction\n",
    "def get_prediction(input_tensor):\n",
    "    outputs = model.forward(input_tensor)     # Get likelihoods for all ImageNet classes\n",
    "    _, y_hat = outputs.max(1)                 # Extract the most likely class\n",
    "    prediction = y_hat.item()                 # Extract the int value from the PyTorch tensor\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "\n",
    "\n",
    "# Make the prediction human-readable\n",
    "def render_prediction(prediction_idx):\n",
    "    stridx = str(prediction_idx)\n",
    "    class_name = 'Unknown'\n",
    "    if img_class_map is not None:\n",
    "        if stridx in img_class_map is not None:\n",
    "            class_name = img_class_map[stridx][1]\n",
    "\n",
    "    return prediction_idx, class_name\n",
    "\n",
    "\n",
    "\n",
    "@app.route('/', methods=['GET'])\n",
    "def root():\n",
    "    return jsonify({'msg' : 'Try POSTing to the /predict endpoint with an RGB image attachment'})\n",
    "\n",
    "\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    if request.method == 'POST':\n",
    "        file = request.files['file']\n",
    "        if file is not None:\n",
    "            input_tensor = transform_image(file)\n",
    "            prediction_idx = get_prediction(input_tensor)\n",
    "            class_id, class_name = render_prediction(prediction_idx)\n",
    "            \n",
    "            return jsonify({'class_id': class_id, 'class_name': class_name})\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start the server from your shell prompt, issue the following command:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "FLASK_APP=app.py flask run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, your Flask server is listening on port 5000. Once the server is running, open another terminal window, and test your new inference server:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "curl -X POST -H \"Content-Type: multipart/form-data\" http://localhost:5000/predict -F \"file=@kitten.jpg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything is set up correctly, you should recevie a response similar to the following:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "{\"class_id\":285,\"class_name\":\"Egyptian_cat\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <a href=\"https://pytorch.org/\" style=\"text-decoration:none;\">pytorch.org</a> for installation instructions, and more documentation and tutorials\n",
    "* The <a href=\"https://flask.palletsprojects.com/en/1.1.x/\" style=\"text-decoration:none;\">Flask site</a> has a <a href=\"https://flask.palletsprojects.com/en/1.1.x/quickstart/\" style=\"text-decoration:none;\">Quick Start guide</a> that goes into more detail on setting up a simple Flask service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"font-size:120%;font-weight:bold\">TorchScript for Deployment</font> <a href=\"https://pytorch.org/tutorials/recipes/torchscript_inference.html\" style=\"text-decoration:none;\"><font size=3>[ link ]</font></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this recipe, you will learn:\n",
    "\n",
    "* What TorchScript is\n",
    "* How to export your trained model in TorchScript format\n",
    "* How to load your TorchScript model in C++ and do inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* PyTorch 1.5\n",
    "* TorchVision 0.6.0\n",
    "* libtorch 1.5\n",
    "* C++ compiler\n",
    "\n",
    "The instructions for installing the three PyTorch components are available at `pytorch.org`. The C++ compiler will depend on your platform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is TorchScript?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TorchScript** is an intermediate representation of a PyTorch model (subclass of nn.Module) that can then be run in a high-performance environment like C++. It’s a high-performance subset of Python that is meant to be consumed by the **PyTorch JIT Compiler**, which performs run-time optimization on your model’s computation. \n",
    "\n",
    "<font color=maroon size=3>TorchScript is the recommended model format for doing scaled inference with PyTorch models.</font> \n",
    "\n",
    "For more information, see the PyTorch <a href=\"https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html\" style=\"text-decoration:none;\">Introduction to TorchScript tutorial</a>, the <a href=\"https://pytorch.org/tutorials/advanced/cpp_export.html\" style=\"text-decoration:none;\">Loading A TorchScript Model in C++ tutorial</a>, and the <a href=\"https://pytorch.org/docs/stable/jit.html\" style=\"text-decoration:none;\">full TorchScript documentation</a>, all of which are available on `pytorch.org`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Export Your Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, let’s take a pretrained vision model. All of the pretrained models in TorchVision are compatible with TorchScript."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following Python 3 code, either in a script or from the REPL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to C:\\Users\\18617/.cache\\torch\\hub\\checkpoints\\resnet18-f37072fd.pth\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "r18 = models.resnet18(pretrained=True)    # We now have an instance of the pretrained model\n",
    "r18_scripted = torch.jit.script(r18)      # *** This is the TorchScript export\n",
    "\n",
    "dumy_input = torch.rand(1, 3, 224, 224)   # We should run a quick test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s do a sanity check on the equivalence of the two models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python model top 5 results:\n",
      "  tensor([[463, 600, 731, 412, 899]])\n",
      "TorchScript model top 5 results:\n",
      "  tensor([[463, 600, 731, 412, 899]])\n"
     ]
    }
   ],
   "source": [
    "unscripted_output = r18(dumy_input)         # Get the unscripted model's prediction...\n",
    "scripted_output = r18_scripted(dumy_input)  # ...and do the same for the scripted version\n",
    "\n",
    "unscripted_top5 = F.softmax(unscripted_output, dim=1).topk(5).indices\n",
    "scripted_top5 = F.softmax(scripted_output, dim=1).topk(5).indices\n",
    "\n",
    "print('Python model top 5 results:\\n  {}'.format(unscripted_top5))\n",
    "print('TorchScript model top 5 results:\\n  {}'.format(scripted_top5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that both versions of the model give the same results:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Python model top 5 results:\n",
    "  tensor([[463, 600, 731, 899, 898]])\n",
    "TorchScript model top 5 results:\n",
    "  tensor([[463, 600, 731, 899, 898]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([[0.0082, 0.0061, 0.0060, 0.0056, 0.0049]], grad_fn=<TopkBackward0>),\n",
       "indices=tensor([[463, 600, 731, 412, 899]]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for knowing\n",
    "F.softmax(unscripted_output, dim=1).topk(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that check confirmed, go ahead and save the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "r18_scripted.save('./model_weights/r18_scripted.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:red;font-size:110%\">Loading TorchScript Models in C++</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the following C++ file and name it `ts-infer.cpp`:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#include <torch/script.h>\n",
    "#include <torch/nn/functional/activation.h>\n",
    "\n",
    "\n",
    "int main(int argc, const char* argv[]) {\n",
    "    if (argc != 2) {\n",
    "        std::cerr << \"usage: ts-infer <path-to-exported-model>\\n\";\n",
    "        return -1;\n",
    "    }\n",
    "\n",
    "    std::cout << \"Loading model...\\n\";\n",
    "\n",
    "    // deserialize ScriptModule\n",
    "    torch::jit::script::Module module;\n",
    "    try {\n",
    "        module = torch::jit::load(argv[1]);\n",
    "    } catch (const c10::Error& e) {\n",
    "        std::cerr << \"Error loading model\\n\";\n",
    "        std::cerr << e.msg_without_backtrace();    # replace as e.what_without_backtrace()\n",
    "        return -1;\n",
    "    }\n",
    "\n",
    "    std::cout << \"Model loaded successfully\\n\";\n",
    "\n",
    "    torch::NoGradGuard no_grad; // ensures that autograd is off\n",
    "    module.eval(); // turn off dropout and other training-time layers/functions\n",
    "\n",
    "    // create an input \"image\"\n",
    "    std::vector<torch::jit::IValue> inputs;\n",
    "    inputs.push_back(torch::rand({1, 3, 224, 224}));\n",
    "\n",
    "    // execute model and package output as tensor\n",
    "    at::Tensor output = module.forward(inputs).toTensor();\n",
    "\n",
    "    namespace F = torch::nn::functional;\n",
    "    at::Tensor output_sm = F::softmax(output, F::SoftmaxFuncOptions(1));\n",
    "    std::tuple<at::Tensor, at::Tensor> top5_tensor = output_sm.topk(5);\n",
    "    at::Tensor top5 = std::get<1>(top5_tensor);\n",
    "\n",
    "    std::cout << top5[0] << \"\\n\";\n",
    "\n",
    "    std::cout << \"\\nDONE\\n\";\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This program:\n",
    "\n",
    "* Loads the model you specify on the command line\n",
    "* Creates a dummy “image” input tensor\n",
    "* Performs inference on the input\n",
    "\n",
    "Also, notice that there is no dependency on TorchVision in this code. The saved version of your TorchScript model has your <font color=maroon>learning weights</font> and your <font color=maroon>computation graph</font> - nothing else is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and Running Your C++ Inference Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the following CMakeLists.txt file:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cmake_minimum_required(VERSION 3.0 FATAL_ERROR)\n",
    "project(custom_ops)\n",
    "\n",
    "find_package(Torch REQUIRED)\n",
    "\n",
    "add_executable(ts-infer ts-infer.cpp)\n",
    "target_link_libraries(ts-infer \"${TORCH_LIBRARIES}\")\n",
    "set_property(TARGET ts-infer PROPERTY CXX_STANDARD 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# for example:\n",
    "\n",
    "cmake_minimum_required(VERSION 3.0 FATAL_ERROR)    # 指定 cmake 的最小版本\n",
    "project(ts-infer)                                  # 设置项目名称\n",
    "\n",
    "find_package(TORCH REQUIRED)\n",
    "\n",
    "add_executable(ts-infer ts-infer.cpp)                     # 设置编译类型，此处为生成可执行文件。\n",
    "target_link_libraries(ts-infer \"${TORCH_LIBRARIES}\")      # 设置 target 需要链接的库。\n",
    "set_property(TARGET ts-infer PROPERTY CXX_STANDARD 11)    # Sets one property on zero or more objects of a scope.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the program:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cmake -DCMAKE_PREFIX_PATH=<path to your libtorch installation>\n",
    "make"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# for example (for windows):\n",
    "mkdir build\n",
    "cd build\n",
    "\n",
    "cmake -DCMAKE_PREFIX_PATH=D:\\KeepStudy\\0_Coding\\C_Cpp\\myTorch\\libtorch-win-shared-with-deps-debug-1.10.2+cu113\\libtorch ..\n",
    "\n",
    "cmake build .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can run inference in C++, and verify that we get a result:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "$ ./ts-infer r18_scripted.pt\n",
    "Loading model...\n",
    "Model loaded successfully\n",
    " 418\n",
    " 845\n",
    " 111\n",
    " 892\n",
    " 644\n",
    "[ CPULongType{5} ]\n",
    "\n",
    "DONE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <a href=\"https://pytorch.org/\" style=\"text-decoration:none;\">pytorch.org</a> for installation instructions, and more documentation and tutorials.\n",
    "* <a href=\"https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html\" style=\"text-decoration:none;\">Introduction to TorchScript tutorial</a> for a deeper initial exposition of TorchScript\n",
    "* <a href=\"https://pytorch.org/docs/stable/jit.html\" style=\"text-decoration:none;\">Full TorchScript documentation</a> for complete TorchScript language and API reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ptg]",
   "language": "python",
   "name": "conda-env-ptg-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "364px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
