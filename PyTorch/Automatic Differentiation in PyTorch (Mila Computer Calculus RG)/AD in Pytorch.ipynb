{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=center><font color=maroon size=6><b>AD in Pytorch</b></font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>**Author:** Breandan Considine\n",
    "\n",
    "Referenced from <a href=\"https://compcalc.github.io/\" style=\"text-decoration:none;\">Mila Computer Calculus RG</a></font> &emsp;[[Original notebook link]](https://colab.research.google.com/drive/14KPZmXxa21YkFGKsVe37RV-AiT6YMRao#scrollTo=fKa_PFVfKhJU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Picograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Var:\n",
    "    \n",
    "    def __init__(self, val, grad_fn=lambda: []): # grad_fn 返回值是对应函数各变量的偏导数构成的向量\n",
    "        self.v, self.grad_fn = val, grad_fn\n",
    "\n",
    "    def __add__(self, other):\n",
    "        # 两变量相加，对各自的偏导数都是 1。\n",
    "        return Var(self.v + other.v, lambda: [(self, 1.0), (other, 1.0)])\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        # 两变量相乘，对各自的偏导数结果则是另一个变量\n",
    "        # 如：f = x*y\n",
    "        # ∂f/∂x = y\n",
    "        # ∂f/∂y = x\n",
    "        return Var(self.v * other.v, lambda: [(self, other.v), (other, self.v)])\n",
    "\n",
    "    def grad(self, bp=1.0):\n",
    "        grads = [input.grad(bp=val * bp) for (input, val) in self.grad_fn()]\n",
    "        merge = lambda a, b: {k: a.get(k, 0) + b.get(k, 0) for k in set(a) | set(b)}\n",
    "        return reduce(merge, grads, {self: bp})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n",
      "4.0\n",
      "2.0\n"
     ]
    }
   ],
   "source": [
    "x = Var(1.)\n",
    "y = Var(1.)\n",
    "f = x * x * x * y * x + y * y * y\n",
    "grad = f.grad()\n",
    "\n",
    "print(grad[x])\n",
    "print(grad[y])\n",
    "print(f.v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.Var.__add__.<locals>.<lambda>()>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(<__main__.Var at 0x17cde32e580>, 1.0),\n",
       " (<__main__.Var at 0x17cde32e430>, 1.0)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.grad_fn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise #0\n",
    "\n",
    "Implement linear regression using Picograd and compare with PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "torchlr = nn.Linear(3, 3)\n",
    "inputs = [1., 1., 1.]\n",
    "\n",
    "def picolr(x):\n",
    "  # Your code goes here\n",
    "  return []\n",
    "\n",
    "# assert torch.allclose(picolr(inputs), torchlr(torch.tensor([inputs])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "import torch\n",
    "# Certain features in this tutorial are experimental. We will supress the warning.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "from torch._vmap_internals import vmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizing Maps (vmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>A `vmap` takes a function and returns a function which accepts a tensor, and maps that function over the tensor (elementwise by default).</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 4., 4.],\n",
       "        [4., 4., 4.],\n",
       "        [4., 4., 4.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ten = torch.ones(3, 3)\n",
    "fun = lambda x: x + 3\n",
    "vmap(fun)(ten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>`vmap` is composable with most things, including itself.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7., 7., 7.],\n",
       "        [7., 7., 7.],\n",
       "        [7., 7., 7.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vmap(fun)(vmap(fun)(ten))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 4., 4.],\n",
       "        [4., 4., 4.],\n",
       "        [4., 4., 4.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vmap(vmap(fun))(ten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>`vmap` also works on polyadic lambdas:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.7202,  0.1786, -0.7338],\n",
      "        [ 0.1254,  0.7197, -0.1640],\n",
      "        [ 0.2520,  0.1262, -1.7422]])\n",
      "tensor([[ 0.9401, -0.5548,  0.0454],\n",
      "        [ 0.7442,  0.8675, -0.5645],\n",
      "        [-0.3177,  1.2194,  0.0376]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, 3)\n",
    "y = torch.randn(3, 3)\n",
    "\n",
    "print(x)\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6770, -0.0991, -0.0333],\n",
       "        [ 0.0933,  0.6243,  0.0926],\n",
       "        [-0.0801,  0.1539, -0.0655]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vmap(torch.mul)(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6770, -0.0991, -0.0333],\n",
       "        [ 0.0933,  0.6243,  0.0926],\n",
       "        [-0.0801,  0.1539, -0.0655]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x * y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9., 9., 9.],\n",
       "        [9., 9., 9.],\n",
       "        [9., 9., 9.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones(3, 3) * 2\n",
    "y = torch.ones(3, 3) * 3\n",
    "z = torch.ones(3, 3) * 4\n",
    "\n",
    "vmap(lambda x, y, z: x + y + z)(x, y, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>`vmap` can also handle tensor contractions:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5., 5.],\n",
       "        [5., 5.],\n",
       "        [5., 5.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dot              # [  D  ],[  D  ] ->   S\n",
    "vd =  vmap(torch.dot)  # [ N,D ],[ N,D ] -> [ N ]\n",
    "vvd = vmap(vd)         # [N,D,C],[N,D,C] -> [N,D]\n",
    "x, y = torch.ones(3, 2, 5), torch.ones(3, 2, 5)\n",
    "vvd(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "vd =  vmap(torch.dot)  # [ N,D ],[ N,D ] -> [ N ]\n",
    "# vd 的参数是一个由2维 torch.Tensor 构成的数组：([ N,D ], [ N,D ])，\n",
    "# 任何一个2维的 [ N,D ] 都可以看作元素为 [D] 的1维 行 torch.Tensor 堆叠而成的2维 torch.Tensor\n",
    "# 故 vd =  vmap(torch.dot) 相当于对于每一个参数的元素（1维 行 torch.Tensor 的 [D]）\n",
    "# 进行 torch.dot 运算，即 [  D  ],[  D  ] ->   S，结果是一个标量。\n",
    "# 因为任何一个2维的 [ N,D ] 都有 N 行的 [ D ]，故这N个标量又构成一个1维的 torch.Tensor [ N ]\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>Why should you use `vmap`?</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loop completed in 1.5586 seconds\n",
      "vmap completed in 0.0010 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "fun = lambda x: x**2 / x *x **x\n",
    "ten = torch.randn(100000)\n",
    "\n",
    "\n",
    "\n",
    "start = time.perf_counter()\n",
    "\n",
    "for i in range(0, 100000):\n",
    "  ten[i] = fun(ten[i])\n",
    "\n",
    "print(f\"loop completed in {time.perf_counter() - start:0.4f} seconds\")\n",
    "\n",
    "\n",
    "\n",
    "start = time.perf_counter()\n",
    "\n",
    "vmap(fun)(ten)\n",
    "\n",
    "print(f\"vmap completed in {time.perf_counter() - start:0.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise #1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement matrix multiplication using `vmap` and use this to implement the forward pass of an MLP. Compare numerical results with the object-oriented version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_mlp = nn.Sequential(\n",
    "        nn.Linear(3, 3),\n",
    "        nn.Sigmoid(),\n",
    "        nn.Linear(3, 2),\n",
    "        nn.Sigmoid()\n",
    "      )\n",
    "\n",
    "inputs = torch.ones(3)\n",
    "\n",
    "# Your code goes here.\n",
    "\n",
    "# assert torch.allclose(seq_mlp(inputs), vmap_mlp(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>PyTorch generally treats functions as stateful objects. This is the object-oriented way.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grad_fun(f, a, b, grad):\n",
    "    \n",
    "    f.backward()\n",
    "    return a.grad, b.grad\n",
    "\n",
    "x = torch.tensor([1., 3.], requires_grad=True)\n",
    "y = torch.tensor([6., 4.], requires_grad=True)\n",
    "x.grad, y.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>Calling grad_fun mutates the variables...</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([6., 4.]), tensor([1., 3.]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad = torch.tensor([1., 1.])\n",
    "f = sum(x * y)\n",
    "\n",
    "grad_fun(f, x, y, grad)\n",
    "\n",
    "x.grad, y.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(18., grad_fn=<AddBackward0>)\n",
      "tensor([1., 3.], requires_grad=True)\n",
      "tensor([6., 4.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(f)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>What happens if we call the function a second time?</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10900/3999791747.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgrad_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10900/2704867412.py\u001b[0m in \u001b[0;36mgrad_fun\u001b[1;34m(f, a, b, grad)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgrad_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\Anaconda3\\envs\\ptg\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\Anaconda3\\envs\\ptg\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "grad_fun(f, x, y, grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>Gradients in PyTorch are lowered onto `torch.autograd.grad`. This is useful for defining custom gradients.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-1.3858, -2.1631]), tensor([0.4892, 0.3294]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2, requires_grad=True)\n",
    "t = torch.tensor([2., 3.], requires_grad=True)\n",
    "y = torch.randn(2, requires_grad=True)\n",
    "\n",
    "\n",
    "f = sum((x*t - y)**2)**0.5\n",
    "# ∂f/∂x = 1/2 * sum( (x*t-y)**2)**(-1/2) *2 *(x*t-y) *t\n",
    "# ∂f/∂t = 1/2 * sum( (x*t-y)**2)**(-1/2) *2 *(x*t-y) *x\n",
    "\n",
    "\n",
    "torch.autograd.grad(f, inputs=[x, t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.3858, -2.1631], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ∂f/∂x = 1/2 * sum( (x*t-y)**2)**(-1/2) *2 *(x*t-y) *t\n",
    "1/2 * sum( (x*t-y)**2)**(-1/2) *2 *(x*t-y) *t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4892, 0.3294], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ∂f/∂t = 1/2 * sum( (x*t-y)**2)**(-1/2) *2 *(x*t-y) *x\n",
    "1/2 * sum( (x*t-y)**2)**(-1/2) *2 *(x*t-y) *x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10900/22570509.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Programs\\Anaconda3\\envs\\ptg\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[1;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 234\u001b[1;33m     return Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    235\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_outputs_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m         inputs, allow_unused, accumulate_grad=False)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "torch.autograd.grad(f, inputs=[x, t])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jacobians"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>The *Jacobian* $\\mathcal{J}: (\\mathbb{R}^m\\rightarrow\\mathbb{R}^n)\\rightarrow\\mathbb{R}^{n \\times m}$ is a function which takes a vector-valued function and returns a matrix of partials, whose rows are gradients of each of the outputs with respect to each of the inputs.</font>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<font size=4>\n",
    "$$\n",
    "\\mathcal{J}\\circ\\mathbf{f} =\n",
    "            \\begin{bmatrix}\n",
    "                \\dfrac{\\partial \\mathbf{f}}{\\partial x_1} & \\cdots & \\dfrac{\\partial \\mathbf{f}}{\\partial x_m}\n",
    "            \\end{bmatrix}^\\intercal =\n",
    "            \\begin{bmatrix}\n",
    "                \\dfrac{\\partial f_1}{\\partial x_1} & \\cdots & \\dfrac{\\partial f_1}{\\partial x_m}\\\\\n",
    "                \\vdots & \\ddots & \\vdots\\\\\n",
    "                \\dfrac{\\partial f_n}{\\partial x_1} & \\cdots & \\dfrac{\\partial f_n}{\\partial x_m}\n",
    "            \\end{bmatrix} =\n",
    "            \\begin{bmatrix}\n",
    "                \\nabla f_1 \\\\\n",
    "                \\vdots \\\\\n",
    "                \\nabla f_m\n",
    "            \\end{bmatrix}\n",
    "$$</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jacobian(fun, x) -> torch.Tensor:\n",
    "    \n",
    "    x = x.detach().requires_grad_()\n",
    "    y = fun(x)\n",
    "    vjp = lambda v: torch.autograd.grad(y, x, v)[0]\n",
    "\n",
    "    vs = torch.eye(y.numel())\\\n",
    "            .view(y.numel(), *y.shape)\n",
    "    \n",
    "    print(y.numel())\n",
    "    print(torch.eye(y.numel()))\n",
    "    print(vs)\n",
    "    \n",
    "    result = vmap(vjp)(vs)\n",
    "    return result.detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>Adapted from: https://gist.github.com/zou3519/b8c0c534c7509e7a3ed0612d01226fb7</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]])\n",
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[3., 0., 0.],\n",
       "        [0., 3., 0.],\n",
       "        [0., 0., 3.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = lambda x: x ** 3\n",
    "jacobian(f, torch.ones(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9279,  0.0000, -0.0000],\n",
       "        [-0.0000,  0.6971, -0.0000],\n",
       "        [-0.0000,  0.0000, -1.1016]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "y = torch.randn(3)\n",
    "\n",
    "vjp_mul = lambda v: torch.autograd.grad([x * y], [x], grad_outputs=[v])[0]\n",
    "\n",
    "batched_v = torch.eye(3)\n",
    "vmap(vjp_mul)(batched_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here.\n",
    "\n",
    "# assert torch.allclose(forward_mode(f, x), reverse_mode(f, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hessians\n",
    "\n",
    "<font size=3>The *Hessian* $\\mathbf{H}:(\\mathbb{R}^m\\rightarrow\\mathbb{R})\\rightarrow\\mathbb{R}^{m\\times m}$ is a higher-order function which maps scalar functions to a matrix of second-order partial derivatives:</font>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<font size=4>\n",
    "$$\n",
    "\\mathbf{H}\\circ Q = \\begin{bmatrix}{\\dfrac {\\partial ^{2}Q}{\\partial x_{1}^{2}}}\n",
    "                                &{\\dfrac {\\partial ^{2}Q}{\\partial x_{1}\\,\\partial x_{2}}}&\\cdots &{\\dfrac {\\partial ^{2}Q}{\\partial x_{1}\\,\\partial x_{m}}}\\\\[2.2ex]{\\dfrac {\\partial ^{2}Q}{\\partial x_{2}\\,\\partial x_{1}}}&{\\dfrac {\\partial ^{2}Q}{\\partial x_{2}^{2}}}&\\cdots &{\\dfrac {\\partial ^{2}Q}{\\partial x_{2}\\,\\partial x_{m}}}\\\\[2.2ex]\\vdots &\\vdots &\\ddots &\\vdots \\\\[2.2ex]{\\dfrac {\\partial ^{2}Q}{\\partial x_{m}\\,\\partial x_{1}}}&{\\dfrac {\\partial ^{2}Q}{\\partial x_{m}\\,\\partial x_{2}}}&\\cdots &{\\dfrac {\\partial ^{2}Q}{\\partial x_{m}^{2}}}\n",
    "            \\end{bmatrix}\n",
    "$$</font>\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Under [certain conditions](https://en.wikipedia.org/wiki/Symmetry_of_second_derivatives#Theorem_of_Schwarz) the Hessian is symmetric, although these conditions do [not always hold](https://math.stackexchange.com/questions/29536/asymmetric-hessian-matrix)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "锐平添加：<font size=4>$H(Q)^T=(H∘Q)^T=\\mathcal{J}∘∇Q$</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hessian(fun: Callable, x: torch.Tensor) ->  torch.Tensor:\n",
    "    def first_grad(x: torch.Tensor):\n",
    "        y = fun(x)\n",
    "        assert y.dim() == 0  # hessian only defined on scalar-valued function\n",
    "        return torch.autograd.grad(y, x, create_graph=True)[0]\n",
    "\n",
    "    return jacobian(first_grad, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>Adapted from: https://gist.github.com/zou3519/b8c0c534c7509e7a3ed0612d01226fb7</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]])\n",
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[6., 0., 0.],\n",
       "        [0., 6., 0.],\n",
       "        [0., 0., 6.]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = lambda x: (x ** 3).sum()\n",
    "hessian(g, torch.ones(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise #3\n",
    "\n",
    "<br>\n",
    "\n",
    "<font size=3>Show that $(\\mathbf{H} \\circ Q)^\\intercal = \\mathcal{J} \\circ \\nabla Q$.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here.\n",
    "\n",
    "# assert torch.allclose(hessian(g, x), jacobian_gradient(g, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>In `numpy`, it is possible to broadcast two dimensions if their [shapes are equal](https://numpy.org/doc/stable/user/basics.broadcasting.html#general-broadcasting-rules). However keeping track of dimension order and origin can be confusing, i.e. two dimensions may have the same shape but different [semantics](https://en.wikipedia.org/wiki/Covariance_and_contravariance_of_vectors#Use_in_tensor_analysis). [Named tensors](https://namedtensor.github.io/) provide a more user-friendly notation for manipulating tensors.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('batch', 'out')\n"
     ]
    }
   ],
   "source": [
    "state = torch.ones(9, 5, names=('batch', 'D'))\n",
    "trans = torch.randn(5, 5, names=('in', 'out'))\n",
    "next_state = state @ trans\n",
    "print(next_state.names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>In PyTorch, attempting to accumulate named tensors along dimensions with different names will produce a runtime error:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error when attempting to broadcast dims ['X'] and dims ['Z']: dim 'X' and dim 'Z' are at the same position from the right but do not match.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10900/418741992.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'X'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Z'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: Error when attempting to broadcast dims ['X'] and dims ['Z']: dim 'X' and dim 'Z' are at the same position from the right but do not match."
     ]
    }
   ],
   "source": [
    "x = torch.ones(3, names=('X',))\n",
    "y = torch.ones(3, names=('Z',))\n",
    "z = x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>General matrix multiplication is a dot product between tensors.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "matmul: Expected 'B' (index 1 of ['A', 'B', 'C', 'D']) to match 'C' (index 0 of ['C', 'F', 'G']) but they do not match.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10900/1512779827.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'A'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'B'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'C'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'D'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'F'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'G'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: matmul: Expected 'B' (index 1 of ['A', 'B', 'C', 'D']) to match 'C' (index 0 of ['C', 'F', 'G']) but they do not match."
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, 3, 3, 3, names=('A', 'B', 'C', 'D'))\n",
    "y = torch.randn(3, 3, 3, names=('C', 'F', 'G'))\n",
    "torch.matmul(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise #4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement [2D depthwise convolution](https://www.tensorflow.org/api_docs/python/tf/nn/depthwise_conv2d) using named tensors in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ptg]",
   "language": "python",
   "name": "conda-env-ptg-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
