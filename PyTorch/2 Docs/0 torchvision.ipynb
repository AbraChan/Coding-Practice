{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=center><font color=maroon size=6><b>torchvision</b></font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4><b>References:</b></font>\n",
    "* Docs > <a href=\"https://pytorch.org/vision/stable/index.html\" style=\"text-decoration:none;\">torchvision</a>\n",
    "    * Docs > <a href=\"https://pytorch.org/vision/stable/models.html\" style=\"text-decoration:none;\">Models and pre-trained weights</a> (torchvision.models)\n",
    "    * Docs > <a href=\"https://pytorch.org/vision/stable/transforms.html\" style=\"text-decoration:none;\">Transforming and augmenting images</a>\n",
    "    * Docs > <a href=\"\" style=\"text-decoration:none;\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=gray size=4>Docs > </font>\n",
    "# <font color=maroon>**torchvision**</font> <a href=\"https://pytorch.org/vision/stable/index.html\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>This **library** is part of the <a href=\"\" style=\"text-decoration:none;color:maroon;font-size:120%;\">PyTorch</a> project. PyTorch is an open source machine learning framework.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features described in this documentation are classified by release status: (略)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>The <a href=\"https://pytorch.org/vision/stable/index.html#module-torchvision\" style=\"text-decoration:none;font-size:120%\">torchvision</a> package consists of <font color=maroon>popular datasets</font>, <font color=maroon>model architectures</font>, and <font color=maroon>common image transformations</font> for **computer vision**.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <a href=\"https://pytorch.org/vision/stable/transforms.html\" style=\"text-decoration:none;color:maroon;font-size:140%;\">Transforming and augmenting images</a><br><br>\n",
    "    * <a href=\"https://pytorch.org/vision/stable/transforms.html#scriptable-transforms\" style=\"text-decoration:none;font-size:120%;color:black;\">Scriptable transforms</a><br><br>\n",
    "    * <a href=\"https://pytorch.org/vision/stable/transforms.html#compositions-of-transforms\" style=\"text-decoration:none;font-size:120%;color:black;\">Compositions of transforms</a><br><br>\n",
    "    * <a href=\"https://pytorch.org/vision/stable/transforms.html#transforms-on-pil-image-and-torch-tensor\" style=\"text-decoration:none;font-size:120%;color:black;\">Transforms on PIL Image and torch.*Tensor</a><br><br>\n",
    "    * <a href=\"https://pytorch.org/vision/stable/transforms.html#transforms-on-pil-image-only\" style=\"text-decoration:none;font-size:120%;color:black;\">Transforms on PIL Image only</a><br><br>\n",
    "    * <a href=\"https://pytorch.org/vision/stable/transforms.html#transforms-on-torch-tensor-only\" style=\"text-decoration:none;font-size:120%;color:black;\">Transforms on torch.*Tensor only</a><br><br>\n",
    "    * <a href=\"https://pytorch.org/vision/stable/transforms.html#conversion-transforms\" style=\"text-decoration:none;font-size:120%;color:black;\">Conversion Transforms</a><br><br>\n",
    "    * <a href=\"https://pytorch.org/vision/stable/transforms.html#generic-transforms\" style=\"text-decoration:none;font-size:120%;color:black;\">Generic Transforms</a><br><br>\n",
    "    * <a href=\"https://pytorch.org/vision/stable/transforms.html#automatic-augmentation-transforms\" style=\"text-decoration:none;font-size:120%;color:black;\">Automatic Augmentation Transforms</a><br><br>\n",
    "    * <a href=\"https://pytorch.org/vision/stable/transforms.html#functional-transforms\" style=\"text-decoration:none;font-size:120%;color:black;\">Functional Transforms</a><br><br>  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <a href=\"https://pytorch.org/vision/stable/models.html\" style=\"text-decoration:none;color:maroon;font-size:140%;\">Models and pre-trained weights</a><br><br>\n",
    "    * <a href=\"https://pytorch.org/vision/stable/models.html#classification\" style=\"text-decoration:none;font-size:120%;color:black;\">Classification</a><br><br>\n",
    "    * <a href=\"https://pytorch.org/vision/stable/models.html#semantic-segmentation\" style=\"text-decoration:none;font-size:120%;color:black;\">Semantic Segmentation</a><br><br>\n",
    "    * <a href=\"https://pytorch.org/vision/stable/models.html#object-detection-instance-segmentation-and-person-keypoint-detection\" style=\"text-decoration:none;font-size:120%;color:black;\">Object Detection, Instance Segmentation and Person Keypoint Detection</a><br><br>\n",
    "    * <a href=\"https://pytorch.org/vision/stable/models.html#video-classification\" style=\"text-decoration:none;font-size:120%;color:black;\">Video classification</a><br><br>    \n",
    "    * <a href=\"https://pytorch.org/vision/stable/models.html#optical-flow\" style=\"text-decoration:none;font-size:120%;color:black;\">Optical flow</a><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <a href=\"https://pytorch.org/vision/stable/datasets.html\" style=\"text-decoration:none;color:maroon;font-size:140%;\">Datasets</a><br><br>\n",
    "    * <a href=\"https://pytorch.org/vision/stable/datasets.html#built-in-datasets\" style=\"text-decoration:none;font-size:120%;color:black;\">Built-in datasets</a><br><br>\n",
    "    * <a href=\"https://pytorch.org/vision/stable/datasets.html#base-classes-for-custom-datasets\" style=\"text-decoration:none;font-size:120%;color:black;\">Base classes for custom datasets</a><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <a href=\"https://pytorch.org/vision/stable/utils.html\" style=\"text-decoration:none;color:maroon;font-size:140%;\">Utils</a><br><br>\n",
    "    * <a href=\"https://pytorch.org/vision/stable/generated/torchvision.utils.draw_bounding_boxes.html\" style=\"text-decoration:none;font-size:120%;color:black;\">draw_bounding_boxes</a><br><br>\n",
    "    * <a href=\"https://pytorch.org/vision/stable/generated/torchvision.utils.draw_segmentation_masks.html\" style=\"text-decoration:none;font-size:120%;color:black;\">draw_segmentation_masks</a><br><br>\n",
    "    * <a href=\"https://pytorch.org/vision/stable/generated/torchvision.utils.draw_keypoints.html\" style=\"text-decoration:none;font-size:120%;color:black;\">draw_keypoints</a><br><br>    \n",
    "    * <a href=\"https://pytorch.org/vision/stable/generated/torchvision.utils.flow_to_image.html\" style=\"text-decoration:none;font-size:120%;color:black;\">flow_to_image</a><br><br>    \n",
    "    * <a href=\"https://pytorch.org/vision/stable/generated/torchvision.utils.make_grid.html\" style=\"text-decoration:none;font-size:120%;color:black;\">make_grid</a><br><br>    \n",
    "    * <a href=\"https://pytorch.org/vision/stable/generated/torchvision.utils.save_image.html\" style=\"text-decoration:none;font-size:120%;color:black;\">save_image</a><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <a href=\"https://pytorch.org/vision/stable/ops.html\" style=\"text-decoration:none;color:maroon;font-size:140%;\">Operators</a><br><br>\n",
    "    * <a href=\"https://pytorch.org/vision/stable/generated/torchvision.ops.batched_nms.html\" style=\"text-decoration:none;font-size:120%;color:black;\">batched_nms</a><br><br>\n",
    "    * <a href=\"https://pytorch.org/vision/stable/generated/torchvision.ops.box_area.html\" style=\"text-decoration:none;font-size:120%;color:black;\">box_area</a><br><br>\n",
    "    * <a href=\"https://pytorch.org/vision/stable/generated/torchvision.ops.box_convert.html\" style=\"text-decoration:none;font-size:120%;color:black;\">box_convert</a><br><br>    \n",
    "    * <a href=\"https://pytorch.org/vision/stable/generated/torchvision.ops.box_iou.html\" style=\"text-decoration:none;font-size:120%;color:black;\">box_iou</a><br><br>    \n",
    "    * ...... `(其它的 operators 详见原网址)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <a href=\"https://pytorch.org/vision/stable/io.html\" style=\"text-decoration:none;color:maroon;font-size:140%;\">Reading/Writing images and videos</a><br><br>\n",
    "    * <a href=\"https://pytorch.org/vision/stable/io.html#video\" style=\"text-decoration:none;font-size:120%;color:black;\">Video</a><br><br>\n",
    "    * <a href=\"https://pytorch.org/vision/stable/io.html#fine-grained-video-api\" style=\"text-decoration:none;font-size:120%;color:black;\">Fine-grained video API</a><br><br>\n",
    "    * <a href=\"https://pytorch.org/vision/stable/io.html#image\" style=\"text-decoration:none;font-size:120%;color:black;\">Image</a><br><br>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <a href=\"https://pytorch.org/vision/stable/feature_extraction.html\" style=\"text-decoration:none;color:maroon;font-size:140%;\">Feature extraction for model inspection</a><br><br>\n",
    "    * <a href=\"https://pytorch.org/vision/stable/feature_extraction.html#api-reference\" style=\"text-decoration:none;font-size:120%;color:black;\">API Reference</a><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples and training references\n",
    "<br><br>\n",
    "* <a href=\"https://pytorch.org/vision/stable/auto_examples/index.html\" style=\"text-decoration:none;color:maroon;font-size:140%;\">Example gallery</a><br><br><br>\n",
    "* <a href=\"https://pytorch.org/vision/stable/training_references.html\" style=\"text-decoration:none;color:maroon;font-size:140%;\">Training references</a><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<font size=5><font color=gray>torchvision</font>.get_image_backend()</font>\n",
    "<br>\n",
    "<br>\n",
    "<font color=black>Gets the name of the package used to load images</fong>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<font size=5><font color=gray>torchvision</font>.get_video_backend()</font>\n",
    "<br>\n",
    "<br>\n",
    "<font color=black>Returns the currently active video backend used to decode videos.\n",
    "<br>\n",
    "<br>\n",
    "&emsp;&emsp;**Returns:** Name of the video backend. one of {‘pyav’, ‘video_reader’}.\n",
    "\n",
    "&emsp;&emsp;**Return type:** str\n",
    "</font>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<font size=5><font color=gray>torchvision</font>.set_image_backend(backend)</font>\n",
    "<br>\n",
    "<br>\n",
    "<font color=black>Specifies the package used to load images.\n",
    "<br>\n",
    "<br>\n",
    "**Parameters:**<br><br>\n",
    "**backend** (string) – Name of the image backend. one of {‘PIL’, ‘accimage’}. The accimage package uses the Intel IPP library. It is generally faster than PIL, but does not support as many operations.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<font size=5><font color=gray>torchvision</font>.set_video_backend(backend)</font>\n",
    "<br>\n",
    "<br>\n",
    "<font color=black>Specifies the package used to decode videos.\n",
    "<br>\n",
    "<br>\n",
    "**Parameters:**<br><br>\n",
    "**backend** (string) – Name of the video backend. one of {‘pyav’, ‘video_reader’}. The `pyav` package uses the 3rd party PyAv library. It is a Pythonic binding for the FFmpeg libraries. The `video_reader` package includes a native C++ implementation on top of FFMPEG libraries, and a python API of TorchScript custom operator. It generally decodes faster than pyav, but is perhaps less robust.\n",
    "\n",
    "</font>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "<font size=3 color=red><b>NOTE: </b></font>\n",
    "\n",
    "Building with FFMPEG is disabled by default in the latest main. If you want to use the ‘video_reader’ backend, please compile torchvision from source.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <a href=\"https://pytorch.org/docs\" style=\"text-decoration:none;color:maroon;font-size:120%;\">PyTorch</a>\n",
    "\n",
    "\n",
    "* <a href=\"https://pytorch.org/audio\" style=\"text-decoration:none;color:maroon;font-size:120%;\">torchaudio</a>\n",
    "\n",
    "\n",
    "* <a href=\"torchtext\" style=\"text-decoration:none;color:maroon;font-size:120%;\">torchtext</a>\n",
    "\n",
    "\n",
    "* <a href=\"https://pytorch.org/vision\" style=\"text-decoration:none;color:maroon;font-size:120%;\">torchvision</a>\n",
    "\n",
    "\n",
    "* <a href=\"https://pytorch.org/elastic/\" style=\"text-decoration:none;color:maroon;font-size:120%;\">TorchElastic</a>\n",
    "\n",
    "\n",
    "* <a href=\"https://pytorch.org/serve\" style=\"text-decoration:none;color:maroon;font-size:120%;\">TorchServe</a>\n",
    "\n",
    "\n",
    "* <a href=\"http://pytorch.org/xla/\" style=\"text-decoration:none;color:maroon;font-size:120%;\">PyTorch on XLA Devices</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <a href=\"https://pytorch.org/vision/stable/genindex.html\" style=\"text-decoration:none;color:maroon;font-size:120%;\">Index</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=gray size=4>Docs > </font>\n",
    "#  <font color=maroon>**Models and pre-trained weights**</font> <a href=\"https://pytorch.org/vision/stable/models.html\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>The <font color=maroon>**torchvision.models**</font> subpackage contains definitions of models for addressing different tasks, including: \n",
    "* image classification, \n",
    "* pixelwise semantic segmentation, \n",
    "* object detection, \n",
    "* instance segmentation, \n",
    "* person keypoint detection, \n",
    "* video classification, \n",
    "* and optical flow.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<font size=3 color=red><b>NOTE: </b></font>\n",
    "\n",
    "Backward compatibility is guaranteed for loading a serialized **`state_dict`** to the model created using old PyTorch version. On the contrary, loading entire saved models or serialized **`ScriptModules`** (seralized using older versions of PyTorch) may not preserve the historic behaviour. Refer to the following <a href=\"https://pytorch.org/docs/stable/notes/serialization.html#id6\" style=\"text-decoration:none;color:maroon;font-size:120%;\">documentation</a>. \n",
    "`笔记：Notes-16 Serialization semantics.ipynb`\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Classification** <a href=\"https://pytorch.org/vision/stable/models.html#classification\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 论文集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <a href=\"https://arxiv.org/abs/1404.5997\" style=\"text-decoration:none;color:maroon;font-size:120%;\">AlexNet</a>\n",
    "\n",
    "\n",
    "* <a href=\"https://arxiv.org/abs/1409.1556\" style=\"text-decoration:none;color:maroon;font-size:120%;\">VGG</a>\n",
    "\n",
    "\n",
    "* <a href=\"https://arxiv.org/abs/1512.03385\" style=\"text-decoration:none;color:maroon;font-size:120%;\">ResNet</a>\n",
    "\n",
    "\n",
    "* <a href=\"https://arxiv.org/abs/1602.07360\" style=\"text-decoration:none;color:maroon;font-size:120%;\">SqueezeNet</a>\n",
    "\n",
    "\n",
    "* <a href=\"https://arxiv.org/abs/1608.06993\" style=\"text-decoration:none;color:maroon;font-size:120%;\">DenseNet</a>\n",
    "\n",
    "\n",
    "* <a href=\"https://arxiv.org/abs/1512.00567\" style=\"text-decoration:none;color:maroon;font-size:120%;\">Inception</a> v3\n",
    "\n",
    "\n",
    "* <a href=\"https://arxiv.org/abs/1409.4842\" style=\"text-decoration:none;color:maroon;font-size:120%;\">GoogLeNet</a>\n",
    "\n",
    "\n",
    "* <a href=\"https://arxiv.org/abs/1807.11164\" style=\"text-decoration:none;color:maroon;font-size:120%;\">ShuffleNet</a> v2\n",
    "\n",
    "\n",
    "* <a href=\"https://arxiv.org/abs/1801.04381\" style=\"text-decoration:none;color:maroon;font-size:120%;\">MobileNetV2</a>\n",
    "\n",
    "\n",
    "* <a href=\"https://arxiv.org/abs/1905.02244\" style=\"text-decoration:none;color:maroon;font-size:120%;\">MobileNetV3</a>\n",
    "\n",
    "\n",
    "* <a href=\"https://arxiv.org/abs/1611.05431\" style=\"text-decoration:none;color:maroon;font-size:120%;\">ResNeXt</a>\n",
    "\n",
    "\n",
    "* <a href=\"https://pytorch.org/vision/stable/models.html#wide-resnet\" style=\"text-decoration:none;color:maroon;font-size:120%;\">Wide ResNet</a>\n",
    "\n",
    "\n",
    "* <a href=\"https://arxiv.org/abs/1807.11626\" style=\"text-decoration:none;color:maroon;font-size:120%;\">MNASNet</a>\n",
    "\n",
    "\n",
    "* <a href=\"https://arxiv.org/abs/1905.11946\" style=\"text-decoration:none;color:maroon;font-size:120%;\">EfficientNet</a>\n",
    "\n",
    "\n",
    "* <a href=\"https://arxiv.org/abs/2003.13678\" style=\"text-decoration:none;color:maroon;font-size:120%;\">RegNet</a>\n",
    "\n",
    "\n",
    "* <a href=\"https://arxiv.org/abs/2010.11929\" style=\"text-decoration:none;color:maroon;font-size:120%;\">VisionTransformer</a>\n",
    "\n",
    "\n",
    "* <a href=\"https://arxiv.org/abs/2201.03545\" style=\"text-decoration:none;color:maroon;font-size:120%;\">ConvNeXt</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>You can construct a model with random weights by calling its constructor:</font>\n",
    "\n",
    "<font style=\"font-size:130%;line-height: 30px\">\n",
    "    \n",
    "```python\n",
    "import torchvision.models as models\n",
    "    \n",
    "resnet18 = models.resnet18()\n",
    "alexnet = models.alexnet()\n",
    "vgg16 = models.vgg16()\n",
    "squeezenet = models.squeezenet1_0()\n",
    "densenet = models.densenet161()\n",
    "inception = models.inception_v3()\n",
    "googlenet = models.googlenet()\n",
    "shufflenet = models.shufflenet_v2_x1_0()\n",
    "mobilenet_v2 = models.mobilenet_v2()\n",
    "mobilenet_v3_large = models.mobilenet_v3_large()\n",
    "mobilenet_v3_small = models.mobilenet_v3_small()\n",
    "resnext50_32x4d = models.resnext50_32x4d()\n",
    "wide_resnet50_2 = models.wide_resnet50_2()\n",
    "mnasnet = models.mnasnet1_0()\n",
    "    \n",
    "efficientnet_b0 = models.efficientnet_b0()\n",
    "efficientnet_b1 = models.efficientnet_b1()\n",
    "efficientnet_b2 = models.efficientnet_b2()\n",
    "efficientnet_b3 = models.efficientnet_b3()\n",
    "efficientnet_b4 = models.efficientnet_b4()\n",
    "efficientnet_b5 = models.efficientnet_b5()\n",
    "efficientnet_b6 = models.efficientnet_b6()\n",
    "efficientnet_b7 = models.efficientnet_b7()\n",
    "    \n",
    "regnet_y_400mf = models.regnet_y_400mf()\n",
    "regnet_y_800mf = models.regnet_y_800mf()\n",
    "regnet_y_1_6gf = models.regnet_y_1_6gf()\n",
    "regnet_y_3_2gf = models.regnet_y_3_2gf()\n",
    "regnet_y_8gf = models.regnet_y_8gf()\n",
    "regnet_y_16gf = models.regnet_y_16gf()\n",
    "regnet_y_32gf = models.regnet_y_32gf()\n",
    "regnet_y_128gf = models.regnet_y_128gf()\n",
    "regnet_x_400mf = models.regnet_x_400mf()\n",
    "regnet_x_800mf = models.regnet_x_800mf()\n",
    "regnet_x_1_6gf = models.regnet_x_1_6gf()\n",
    "regnet_x_3_2gf = models.regnet_x_3_2gf()\n",
    "regnet_x_8gf = models.regnet_x_8gf()\n",
    "regnet_x_16gf = models.regnet_x_16gf()\n",
    "regnet_x_32gf = models.regnet_x_32gf()\n",
    "    \n",
    "vit_b_16 = models.vit_b_16()\n",
    "vit_b_32 = models.vit_b_32()\n",
    "vit_l_16 = models.vit_l_16()\n",
    "vit_l_32 = models.vit_l_32()\n",
    "    \n",
    "convnext_tiny = models.convnext_tiny()\n",
    "convnext_small = models.convnext_small()\n",
    "convnext_base = models.convnext_base()\n",
    "convnext_large = models.convnext_large()\n",
    "```\n",
    "    \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>We provide pre-trained models, using the PyTorch <a href=\"https://pytorch.org/docs/stable/model_zoo.html#module-torch.utils.model_zoo\" style=\"text-decoration:none;font-size:120%\">torch.utils.model_zoo</a>. These can be constructed by passing ***pretrained=True***:</font>\n",
    "\n",
    "\n",
    "<font style=\"font-size:130%;LINE-HEIGHT: 30px\">\n",
    "\n",
    "```python\n",
    "import torchvision.models as models\n",
    "    \n",
    "resnet18 = models.resnet18(pretrained=True)\n",
    "alexnet = models.alexnet(pretrained=True)\n",
    "squeezenet = models.squeezenet1_0(pretrained=True)\n",
    "vgg16 = models.vgg16(pretrained=True)\n",
    "densenet = models.densenet161(pretrained=True)\n",
    "inception = models.inception_v3(pretrained=True)\n",
    "googlenet = models.googlenet(pretrained=True)\n",
    "shufflenet = models.shufflenet_v2_x1_0(pretrained=True)\n",
    "mobilenet_v2 = models.mobilenet_v2(pretrained=True)\n",
    "mobilenet_v3_large = models.mobilenet_v3_large(pretrained=True)\n",
    "mobilenet_v3_small = models.mobilenet_v3_small(pretrained=True)\n",
    "resnext50_32x4d = models.resnext50_32x4d(pretrained=True)\n",
    "wide_resnet50_2 = models.wide_resnet50_2(pretrained=True)\n",
    "mnasnet = models.mnasnet1_0(pretrained=True)\n",
    "    \n",
    "efficientnet_b0 = models.efficientnet_b0(pretrained=True)\n",
    "efficientnet_b1 = models.efficientnet_b1(pretrained=True)\n",
    "efficientnet_b2 = models.efficientnet_b2(pretrained=True)\n",
    "efficientnet_b3 = models.efficientnet_b3(pretrained=True)\n",
    "efficientnet_b4 = models.efficientnet_b4(pretrained=True)\n",
    "efficientnet_b5 = models.efficientnet_b5(pretrained=True)\n",
    "efficientnet_b6 = models.efficientnet_b6(pretrained=True)\n",
    "efficientnet_b7 = models.efficientnet_b7(pretrained=True)\n",
    "    \n",
    "regnet_y_400mf = models.regnet_y_400mf(pretrained=True)\n",
    "regnet_y_800mf = models.regnet_y_800mf(pretrained=True)\n",
    "regnet_y_1_6gf = models.regnet_y_1_6gf(pretrained=True)\n",
    "regnet_y_3_2gf = models.regnet_y_3_2gf(pretrained=True)\n",
    "regnet_y_8gf = models.regnet_y_8gf(pretrained=True)\n",
    "regnet_y_16gf = models.regnet_y_16gf(pretrained=True)\n",
    "regnet_y_32gf = models.regnet_y_32gf(pretrained=True)\n",
    "regnet_x_400mf = models.regnet_x_400mf(pretrained=True)\n",
    "regnet_x_800mf = models.regnet_x_800mf(pretrained=True)\n",
    "regnet_x_1_6gf = models.regnet_x_1_6gf(pretrained=True)\n",
    "regnet_x_3_2gf = models.regnet_x_3_2gf(pretrained=True)\n",
    "regnet_x_8gf = models.regnet_x_8gf(pretrained=True)\n",
    "regnet_x_16gf = models.regnet_x_16gf(pretrainedTrue)\n",
    "regnet_x_32gf = models.regnet_x_32gf(pretrained=True)\n",
    "    \n",
    "vit_b_16 = models.vit_b_16(pretrained=True)\n",
    "vit_b_32 = models.vit_b_32(pretrained=True)\n",
    "vit_l_16 = models.vit_l_16(pretrained=True)\n",
    "vit_l_32 = models.vit_l_32(pretrained=True)\n",
    "    \n",
    "convnext_tiny = models.convnext_tiny(pretrained=True)\n",
    "convnext_small = models.convnext_small(pretrained=True)\n",
    "convnext_base = models.convnext_base(pretrained=True)\n",
    "convnext_large = models.convnext_large(pretrained=True)\n",
    "```\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>Instancing a pre-trained model will download its weights to a cache directory. This directory can be set using the ***TORCH_HOME*** environment variable. See <a href=\"https://pytorch.org/docs/stable/hub.html#torch.hub.load_state_dict_from_url\" style=\"text-decoration:none;font-size:120%\">torch.hub.load_state_dict_from_url()</a> for details.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>Some models use modules which have different training and evaluation behavior, such as batch normalization. To switch between these modes, use `model.train()` or `model.eval()` as appropriate. See <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.train\" style=\"text-decoration:none;font-size:120%\">train()</a> or <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.eval\" style=\"text-decoration:none;font-size:120%\">eval()</a> for details.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>All pre-trained models expect input images normalized in the same way, i.e. <font color=maroon>mini-batches of 3-channel RGB images of shape (3 x H x W)</font>, where H and W are expected to be <font color=maroon>at least 224</font>. The images have to be loaded in to a <font color=maroon>range of [0, 1] and then normalized using `mean = [0.485, 0.456, 0.406]` and `std = [0.229, 0.224, 0.225]`</font>. You can use the following transform to normalize:</font>\n",
    "\n",
    "\n",
    "<font style=\"font-size:130%;LINE-HEIGHT: 30px\">\n",
    "\n",
    "```python\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "```\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>An example of such normalization can be found in the imagenet example <a href=\"https://github.com/pytorch/examples/blob/42e5b996718797e45c46a25c55b031e6768f8440/imagenet/main.py#L89-L101\" style=\"text-decoration:none;color:maroon;font-size:120%;\">here</a>.</font> `截图如下：`\n",
    "\n",
    "<img src=\"./1 PyTorch documentation/1 Notes/images/Normalize.png\" width=600px align=left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>The process for obtaining the values of mean and std is roughly equivalent to:</font>\n",
    "\n",
    "<font style=\"font-size:130%;LINE-HEIGHT: 30px\">\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from torchvision import datasets, transforms as T\n",
    "\n",
    "transform = T.Compose([T.Resize(256), T.CenterCrop(224), T.ToTensor()])\n",
    "dataset = datasets.ImageNet(\".\", split=\"train\", transform=transform)\n",
    "\n",
    "means = []\n",
    "stds = []\n",
    "for img in subset(dataset):\n",
    "    means.append(torch.mean(img))\n",
    "    stds.append(torch.std(img))\n",
    "\n",
    "mean = torch.mean(torch.tensor(means))\n",
    "std = torch.mean(torch.tensor(stds))\n",
    "```\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>Unfortunately, the concrete subset that was used is lost. For more information see <a href=\"https://github.com/pytorch/vision/issues/1439\" style=\"text-decoration:none;color:maroon;font-size:120%;\">this discussion</a> or <a href=\"https://github.com/pytorch/vision/pull/1965\" style=\"text-decoration:none;color:maroon;font-size:120%;\">these experiments</a>.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>ImageNet 1-crop error rates</font>\n",
    "\n",
    "<img src=\"./1 PyTorch documentation/1 Notes/images/ImageNet 1-crop error rates.jpeg\" width=700px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=magenta>Alexnet</font> <a href=\"https://pytorch.org/vision/stable/models.html#id1\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color=magenta>VGG</font> <a href=\"https://pytorch.org/vision/stable/models.html#id2\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color=magenta>ResNet</font> <a href=\"https://pytorch.org/vision/stable/models.html#id10\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color=magenta>SqueezeNet</font> <a href=\"https://pytorch.org/vision/stable/models.html#id15\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color=magenta>DenseNet</font> <a href=\"https://pytorch.org/vision/stable/models.html#id16\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color=magenta>Inception v3</font> <a href=\"https://pytorch.org/vision/stable/models.html#inception-v3\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color=magenta>GoogLeNet</font> <a href=\"https://pytorch.org/vision/stable/models.html#id20\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color=magenta>ShuffleNet v2</font> <a href=\"https://pytorch.org/vision/stable/models.html#shufflenet-v2\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color=magenta>MobileNet v2</font> <a href=\"https://pytorch.org/vision/stable/models.html#mobilenet-v2\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color=magenta>MobileNet v3</font> <a href=\"https://pytorch.org/vision/stable/models.html#mobilenet-v3\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color=magenta>ResNext</font> <a href=\"https://pytorch.org/vision/stable/models.html#id25\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color=magenta>Wide ResNet</font> <a href=\"https://pytorch.org/vision/stable/models.html#wide-resnet\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color=magenta>MNASNet</font> <a href=\"https://pytorch.org/vision/stable/models.html#id28\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color=magenta>EfficientNet</font> <a href=\"https://pytorch.org/vision/stable/models.html#id32\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color=magenta>RegNet</font> <a href=\"https://pytorch.org/vision/stable/models.html#id40\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color=magenta>VisionTransformer</font> <a href=\"https://pytorch.org/vision/stable/models.html#id55\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color=magenta>ConvNeXt</font> <a href=\"https://pytorch.org/vision/stable/models.html#id59\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Quantized Models <a href=\"https://pytorch.org/vision/stable/models.html#quantized-models\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>The following architectures provide support for INT8 quantized models. You can get a model with random weights by calling its constructor:</font>\n",
    "\n",
    "\n",
    "<font style=\"font-size:130%;LINE-HEIGHT: 30px\">\n",
    "\n",
    "```python\n",
    "import torchvision.models as models\n",
    "    \n",
    "googlenet = models.quantization.googlenet()\n",
    "    \n",
    "inception_v3 = models.quantization.inception_v3()\n",
    "    \n",
    "mobilenet_v2 = models.quantization.mobilenet_v2()\n",
    "mobilenet_v3_large = models.quantization.mobilenet_v3_large()\n",
    "    \n",
    "resnet18 = models.quantization.resnet18()\n",
    "resnet50 = models.quantization.resnet50()\n",
    "resnext101_32x8d = models.quantization.resnext101_32x8d()\n",
    "    \n",
    "shufflenet_v2_x0_5 = models.quantization.shufflenet_v2_x0_5()\n",
    "shufflenet_v2_x1_0 = models.quantization.shufflenet_v2_x1_0()\n",
    "shufflenet_v2_x1_5 = models.quantization.shufflenet_v2_x1_5()\n",
    "shufflenet_v2_x2_0 = models.quantization.shufflenet_v2_x2_0()\n",
    "```\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>Obtaining a pre-trained quantized model can be done with a few lines of code:</font>\n",
    "\n",
    "\n",
    "<font style=\"font-size:130%;LINE-HEIGHT: 30px\">\n",
    "\n",
    "```python\n",
    "import torchvision.models as models\n",
    "    \n",
    "model = models.quantization.mobilenet_v2(pretrained=True, quantize=True)\n",
    "model.eval()\n",
    "# run the model with quantized inputs and weights\n",
    "out = model(torch.rand(1, 3, 224, 224))\n",
    "```\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>We provide pre-trained quantized weights for the following models:</font>\n",
    "\n",
    "<img src=\"./1 PyTorch documentation/1 Notes/images/pre-trained quantized weights.jpeg\" width=600px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Semantic Segmentation** <a href=\"https://pytorch.org/vision/stable/models.html#semantic-segmentation\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 论文集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>The models subpackage contains definitions for the following model architectures for semantic segmentation:</font>\n",
    "\n",
    "* <a href=\"https://arxiv.org/abs/1411.4038\" style=\"text-decoration:none;color:maroon;font-size:120%;\">FCN ResNet50, ResNet101</a>\n",
    "\n",
    "\n",
    "* <a href=\"https://arxiv.org/abs/1706.05587\" style=\"text-decoration:none;color:maroon;font-size:120%;\">DeepLabV3 ResNet50, ResNet101, MobileNetV3-Large</a>\n",
    "\n",
    "\n",
    "* <a href=\"https://arxiv.org/abs/1905.02244\" style=\"text-decoration:none;color:maroon;font-size:120%;\">LR-ASPP MobileNetV3-Large</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>As with image classification models, all pre-trained models expect input images normalized in the same way. The images have to be loaded in to a <font color=maroon>range of [0, 1] and then normalized using `mean = [0.485, 0.456, 0.406]` and `std = [0.229, 0.224, 0.225]`</font>. They have been trained on images resized such that their <font color=maroon>minimum size is 520</font>.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>For details on how to plot the masks of such models, you may refer to <a href=\"https://pytorch.org/vision/stable/auto_examples/plot_visualization_utils.html#semantic-seg-output\" style=\"text-decoration:none;color:maroon;font-size:120%;\">Semantic segmentation models</a>.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset. You can see more information on how the subset has been selected in <font size=4>`references/segmentation/coco_utils.py`</font>. \n",
    "    \n",
    "The classes that the pre-trained model outputs are the following, in order:</font>\n",
    "\n",
    "<font style=\"font-size:130%;LINE-HEIGHT: 30px\">\n",
    "\n",
    "```python\n",
    "['__background__', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus',\n",
    " 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike',\n",
    " 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']\n",
    "```\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>The accuracies of the pre-trained models evaluated on COCO val2017 are as follows:</font> `(详见原网址)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color=magenta>Fully Convolutional Networks</font> <a href=\"https://pytorch.org/vision/stable/models.html#fully-convolutional-networks\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color=magenta>DeepLabV3</font> <a href=\"https://pytorch.org/vision/stable/models.html#deeplabv3\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color=magenta>LR-ASPP</font> <a href=\"https://pytorch.org/vision/stable/models.html#lr-aspp\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Object Detection, Instance Segmentation and Person Keypoint Detection** <a href=\"https://pytorch.org/vision/stable/models.html#object-detection-instance-segmentation-and-person-keypoint-detection\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 论文集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>The models subpackage contains definitions for the following model architectures for detection:</font>\n",
    "\n",
    "* <a href=\"https://arxiv.org/abs/1506.01497\" style=\"text-decoration:none;color:maroon;font-size:120%;\">Faster R-CNN</a>\n",
    "\n",
    "\n",
    "* <a href=\"https://arxiv.org/abs/1904.01355\" style=\"text-decoration:none;color:maroon;font-size:120%;\">FCOS</a>\n",
    "\n",
    "\n",
    "* <a href=\"https://arxiv.org/abs/1703.06870\" style=\"text-decoration:none;color:maroon;font-size:120%;\">Mask R-CNN</a>\n",
    "\n",
    "\n",
    "* <a href=\"https://arxiv.org/abs/1708.02002\" style=\"text-decoration:none;color:maroon;font-size:120%;\">RetinaNet</a>\n",
    "\n",
    "\n",
    "* <a href=\"https://arxiv.org/abs/1512.02325\" style=\"text-decoration:none;color:maroon;font-size:120%;\">SSD</a>\n",
    "\n",
    "\n",
    "* <a href=\"https://arxiv.org/abs/1801.04381\" style=\"text-decoration:none;color:maroon;font-size:120%;\">SSDlite</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>The pre-trained models for detection, instance segmentation and keypoint detection are initialized with the classification models in torchvision.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>The models expect a list of `Tensor[C, H, W]`, in the range `0-1`. The models internally resize the images but the behaviour varies depending on the model. Check the constructor of the models for more information. The output format of such models is illustrated in <a href=\"https://pytorch.org/vision/stable/auto_examples/plot_visualization_utils.html#instance-seg-output\" style=\"text-decoration:none;color:maroon;font-size:120%;\">Instance segmentation models</a>.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>For object detection and instance segmentation, the pre-trained models return the predictions of the following classes:</font>\n",
    "\n",
    "\n",
    "<font style=\"font-size:120%;LINE-HEIGHT: 30px\">\n",
    "\n",
    "```python\n",
    "COCO_INSTANCE_CATEGORY_NAMES = [\n",
    "    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
    "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n",
    "    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
    "    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',\n",
    "    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
    "    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n",
    "    'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n",
    "    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
    "    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',\n",
    "    'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', \n",
    "    'cell phone',     'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', \n",
    "    'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
    "]\n",
    "```\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>Here are the summary of the accuracies for the models trained on the instances set of COCO train2017 and evaluated on COCO val2017.</font> `(详见原网址)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>For person keypoint detection, the pre-trained model return the keypoints in the following order:</font> \n",
    "\n",
    "\n",
    "<font style=\"font-size:110%;LINE-HEIGHT: 30px\">\n",
    "\n",
    "```python\n",
    "COCO_PERSON_KEYPOINT_NAMES = [\n",
    "    'nose',\n",
    "    'left_eye',\n",
    "    'right_eye',\n",
    "    'left_ear',\n",
    "    'right_ear',\n",
    "    'left_shoulder',\n",
    "    'right_shoulder',\n",
    "    'left_elbow',\n",
    "    'right_elbow',\n",
    "    'left_wrist',\n",
    "    'right_wrist',\n",
    "    'left_hip',\n",
    "    'right_hip',\n",
    "    'left_knee',\n",
    "    'right_knee',\n",
    "    'left_ankle',\n",
    "    'right_ankle'\n",
    "]\n",
    "```\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runtime characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementations of the models for object detection, instance segmentation and keypoint detection are efficient.\n",
    "\n",
    "In the following table, we use 8 GPUs to report the results. During training, we use a batch size of 2 per GPU for all models except SSD which uses 4 and SSDlite which uses 24. During testing a batch size of 1 is used.\n",
    "\n",
    "For test time, we report the time for the model evaluation and postprocessing (including mask pasting in image), but not the time for computing the precision-recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(表格详见原网址)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color=magenta>Faster R-CNN</font> <a href=\"https://pytorch.org/vision/stable/models.html#id63\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color=magenta>FCOS</font> <a href=\"https://pytorch.org/vision/stable/models.html#id64\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color=magenta>RetinaNet</font> <a href=\"https://pytorch.org/vision/stable/models.html#id65\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color=magenta>SSD</font> <a href=\"https://pytorch.org/vision/stable/models.html#id66\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color=magenta>SSDlite</font> <a href=\"https://pytorch.org/vision/stable/models.html#id67\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color=magenta>Mask R-CNN</font> <a href=\"https://pytorch.org/vision/stable/models.html#id70\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color=magenta>Keypoint R-CNN</font> <a href=\"https://pytorch.org/vision/stable/models.html#keypoint-r-cnn\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Video classification** <a href=\"https://pytorch.org/vision/stable/models.html#video-classification\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>We provide models for action recognition pre-trained on Kinetics-400. They have all been trained with the scripts provided in <font size=4>`references/video_classification`</font>.</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>All pre-trained models expect input images normalized in the same way, i.e. <font color=maroon>mini-batches of 3-channel RGB videos of shape (3 x T x H x W), where H and W are expected to be 112, and T is a number of video frames in a clip</font>. The images have to be loaded in to a <font color=maroon>range of [0, 1] and then normalized using `mean = [0.43216, 0.394666, 0.37645]` and `std = [0.22803, 0.22145, 0.216989]`.</font></font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "<font size=3 color=red><b>NOTE: </b></font>\n",
    "\n",
    "<font size=3 color=black>The normalization parameters are **different** from the image classification ones, and correspond to the mean and std from Kinetics-400.</font>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<font size=3 color=red><b>NOTE: </b></font>\n",
    "\n",
    "For now, normalization code can be found in `references/video_classification/transforms.py`, see the Normalize function there. **Note that** it differs from standard normalization for images because it assumes the video is 4d.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kinetics 1-crop accuracies for clip length 16 (16x112x112): `(详见原网址)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color=magenta>ResNet 3D</font> <a href=\"https://pytorch.org/vision/stable/models.html#resnet-3d\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color=magenta>ResNet Mixed Convolution</font> <a href=\"https://pytorch.org/vision/stable/models.html#resnet-mixed-convolution\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color=magenta>ResNet (2+1)D</font> <a href=\"https://pytorch.org/vision/stable/models.html#resnet-2-1-d\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Optical flow** <a href=\"https://pytorch.org/vision/stable/models.html#optical-flow\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color=magenta>Raft</font> <a href=\"https://pytorch.org/vision/stable/models.html#raft\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=gray size=4>Docs > </font>\n",
    "# <font color=maroon>**Transforming and augmenting images**</font> <a href=\"https://pytorch.org/vision/stable/transforms.html\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>Transforms are common image transformations available in the <font style=\"font-size:120%;\">`torchvision.transforms`</font> module. They can be chained together using <a href=\"https://pytorch.org/vision/stable/generated/torchvision.transforms.Compose.html#torchvision.transforms.Compose\" style=\"text-decoration:none;font-size:120%\">Compose</a>. Most transform classes have a function equivalent: <a href=\"https://pytorch.org/vision/stable/transforms.html#functional-transforms\" style=\"text-decoration:none;color:maroon;font-size:110%;\">functional transforms</a> give fine-grained control over the transformations. This is useful if you have to build a more complex transformation pipeline (e.g. in the case of segmentation tasks).</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>Most transformations accept both <a href=\"https://pillow.readthedocs.io/\" style=\"text-decoration:none;color:maroon;font-size:120%;\">PIL</a>\n",
    " images and tensor images, although some transformations are <a href=\"https://pytorch.org/vision/stable/transforms.html#transforms-pil-only\" style=\"text-decoration:none;color:maroon;font-size:110%;\">PIL-only</a>\n",
    " and some are <a href=\"https://pytorch.org/vision/stable/transforms.html#transforms-tensor-only\" style=\"text-decoration:none;color:maroon;font-size:110%;\">tensor-only</a>\n",
    ". The <a href=\"https://pytorch.org/vision/stable/transforms.html#conversion-transforms\" style=\"text-decoration:none;color:maroon;font-size:110%;\">Conversion Transforms</a>\n",
    " may be used to convert to and from PIL images.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>The transformations that accept tensor images also accept batches of tensor images. \n",
    "* <font color=maroon>A Tensor Image is a tensor with `(C, H, W)` shape</font>, where C is a number of channels, H and W are image height and width. \n",
    "* <font color=maroon>A batch of Tensor Images is a tensor of `(B, C, H, W)` shape</font>, where B is a number of images in the batch.</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>The expected range of the values of a tensor image is implicitly defined by the tensor dtype. \n",
    "* Tensor images with a float dtype are expected to have values in `[0, 1)`. \n",
    "* Tensor images with an integer dtype are expected to have values in `[0, MAX_DTYPE]` where **MAX_DTYPE** is the largest value that can be represented in that dtype.</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>Randomized transformations will apply the same transformation to all the images of a given batch, but they will produce different transformations across calls. For reproducible transformations across calls, you may use <a href=\"https://pytorch.org/vision/stable/transforms.html#functional-transforms\" style=\"text-decoration:none;color:maroon;font-size:110%;\">functional transforms</a>.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following examples illustrate the use of the available transforms:\n",
    "* <a href=\"https://pytorch.org/vision/stable/auto_examples/plot_transforms.html#sphx-glr-auto-examples-plot-transforms-py\" style=\"text-decoration:none;color:maroon;font-size:120%;\">Illustration of transforms</a>\n",
    "\n",
    "\n",
    "* <a href=\"https://pytorch.org/vision/stable/auto_examples/plot_scripted_tensor_transforms.html#sphx-glr-auto-examples-plot-scripted-tensor-transforms-py\" style=\"text-decoration:none;color:maroon;font-size:120%;\">Tensor transforms and JIT</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "<font size=3 color=red><b>WARNING: </b></font>\n",
    "\n",
    "Since v0.8.0 all random transformations are using torch default random generator to sample random parameters. It is a backward compatibility breaking change and user should set the random state as following:\n",
    "    \n",
    "    # Previous versions\n",
    "    # import random\n",
    "    # random.seed(12)\n",
    "\n",
    "    # Now\n",
    "    import torch\n",
    "    torch.manual_seed(17)\n",
    "\n",
    "Please, keep in mind that the same seed for torch random generator and Python random generator will not produce the same results.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scriptable transforms <a href=\"https://pytorch.org/vision/stable/transforms.html#scriptable-transforms\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>In order to script the transformations, please use `torch.nn.Sequential` instead of `Compose`.</font>\n",
    "\n",
    "<font style=\"font-size:120%;LINE-HEIGHT: 30px\">\n",
    "\n",
    "```python\n",
    "transforms = torch.nn.Sequential(\n",
    "                        transforms.CenterCrop(10),\n",
    "                        transforms.Normalize((0.485, 0.456, 0.406), \n",
    "                                             (0.229, 0.224, 0.225)),\n",
    "                                )\n",
    "scripted_transforms = torch.jit.script(transforms)\n",
    "```\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3 color=maroon>Make sure to use only scriptable transformations, i.e. that work with `torch.Tensor` and does not require `lambda functions` or `PIL.Image`.\n",
    "\n",
    "\n",
    "For any custom transformations to be used with `torch.jit.script`, they should be derived from `torch.nn.Module`.\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <font color=magenta>Compositions of transforms</font> <a href=\"https://pytorch.org/vision/stable/transforms.html#compositions-of-transforms\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <font color=magenta>Transforms on PIL Image and torch.*Tensor</font> <a href=\"https://pytorch.org/vision/stable/transforms.html#transforms-on-pil-image-and-torch-tensor\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <font color=magenta>Transforms on PIL Image only</font> <a href=\"https://pytorch.org/vision/stable/transforms.html#transforms-on-pil-image-only\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <font color=magenta>Transforms on torch.*Tensor only</font> <a href=\"https://pytorch.org/vision/stable/transforms.html#transforms-on-torch-tensor-only\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <font color=magenta>Conversion Transforms</font> <a href=\"https://pytorch.org/vision/stable/transforms.html#conversion-transforms\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <font color=magenta>Generic Transforms</font> <a href=\"https://pytorch.org/vision/stable/transforms.html#generic-transforms\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <font color=magenta>Automatic Augmentation Transforms</font> <a href=\"https://pytorch.org/vision/stable/transforms.html#automatic-augmentation-transforms\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3><a href=\"https://arxiv.org/pdf/1805.09501.pdf\" style=\"text-decoration:none;color:maroon;font-size:120%;\">AutoAugment</a> is a common Data Augmentation technique that can improve the accuracy of Image Classification models. Though the data augmentation policies are directly linked to their trained dataset, empirical studies show that ImageNet policies provide significant improvements when applied to other datasets. \n",
    "    \n",
    "In TorchVision we implemented 3 policies learned on the following datasets: ImageNet, CIFAR10 and SVHN. The new transform can be used standalone or mixed-and-matched with existing transforms:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(其它详见原网址)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <font color=magenta>Functional Transforms</font> <a href=\"https://pytorch.org/vision/stable/transforms.html#functional-transforms\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>**Functional transforms** give you fine-grained control of the transformation pipeline. As opposed to the transformations above, functional transforms <font color=maroon>don’t contain a random number generator for their parameters. That means you have to specify/generate all parameters,</font> but the functional transform will give you reproducible results across calls.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: you can apply a functional transform with the same parameters to multiple images like this:\n",
    "\n",
    "<font style=\"font-size:120%;LINE-HEIGHT: 30px\">\n",
    "\n",
    "```python\n",
    "import torchvision.transforms.functional as TF\n",
    "import random\n",
    "\n",
    "def my_segmentation_transforms(image, segmentation):\n",
    "    if random.random() > 0.5:\n",
    "        angle = random.randint(-30, 30)\n",
    "        image = TF.rotate(image, angle)\n",
    "        segmentation = TF.rotate(segmentation, angle)\n",
    "    # more transforms ...\n",
    "    return image, segmentation\n",
    "```\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: you can use a functional transform to build transform classes with custom behavior:\n",
    "\n",
    "\n",
    "<font style=\"font-size:120%;LINE-HEIGHT: 30px\">\n",
    "\n",
    "```python\n",
    "import torchvision.transforms.functional as TF\n",
    "import random\n",
    "\n",
    "class MyRotationTransform:\n",
    "    \"\"\"Rotate by one of the given angles.\"\"\"\n",
    "\n",
    "    def __init__(self, angles):\n",
    "        self.angles = angles\n",
    "\n",
    "    def __call__(self, x):\n",
    "        angle = random.choice(self.angles)\n",
    "        return TF.rotate(x, angle)\n",
    "\n",
    "rotation_transform = MyRotationTransform(angles=[-30, -15, 0, 15, 30])\n",
    "```\n",
    "\n",
    "</font>\n",
    "\n",
    "<font color=maroon>****</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(其它详见原网址)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <font color=magenta></font> <a href=\"\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<font size=3></font>\n",
    "\n",
    "<a href=\"\" style=\"text-decoration:none;font-size:70%\">[link]</a>\n",
    "<a href=\"\" style=\"text-decoration:none;font-size:120%\"></a>\n",
    "\n",
    "\n",
    "<a href=\"\" style=\"text-decoration:none;color:maroon;font-size:120%;\"></a>\n",
    "\n",
    "&emsp;&emsp;&emsp;&ensp;\n",
    "<a href=\"\" style=\"text-decoration:none;color:maroon;font-size:140%;font-weight:bold;\"></a>\n",
    "\n",
    "\n",
    "<font style=\"font-size:120%;LINE-HEIGHT: 30px\">\n",
    "\n",
    "```python\n",
    "\n",
    "```\n",
    "\n",
    "</font>\n",
    "\n",
    "<font color=maroon>****</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ptg",
   "language": "python",
   "name": "ptg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "264px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
