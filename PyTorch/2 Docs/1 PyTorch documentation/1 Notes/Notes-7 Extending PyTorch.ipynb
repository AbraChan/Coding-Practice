{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=center><font color=maroon size=6><b>Extending PyTorch</b></font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4><b>References:</b></font>\n",
    "* <a href=\"https://pytorch.org/docs/stable/index.html\" style=\"text-decoration:none;\">Docs > PyTorch documentation</a>\n",
    "\n",
    "    * **Notes**\n",
    "        * Docs > 7 <a href=\"https://pytorch.org/docs/stable/notes/extending.html\" style=\"text-decoration:none;\">Extending PyTorch</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>In this note we’ll cover ways of extending `torch.nn`, <a href=\"https://pytorch.org/docs/stable/autograd.html#module-torch.autograd\" style=\"text-decoration:none;\"><font color=maroon>torch.autograd</font></a>, `torch`, and writing `custom C extensions` utilizing our C libraries.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extending <font color=maroon><b>torch.autograd</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding operations to `torch.autograd` requires implementing a new <a href=\"https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function\" style=\"text-decoration:none;\"><font color=maroon size=3>Function</font></a> subclass for each operation. <br>\n",
    "Recall that Functions are what `torch.autograd` uses to encode the operation history and compute gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first part of this doc is focused on <font size=3 color=maroon><b>backward mode AD</b> as it is the most widely used feature.</font><br>\n",
    "A section at the end discusses the extensions for <font size=3 color=maroon><b>forward mode AD</b></font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, implement a custom function if you want to perform computations in your model that are <font color=maroon size=3><b>not differentiable</b></font> or <font color=maroon size=3><b>rely on non-Pytorch libraries</b> (e.g., NumPy), but still wish for your operation to chain with other ops and work with the autograd engine.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some situations, custom functions can also be used to improve performance and memory usage: If you implemented your forward and backward passes using a <a href=\"https://pytorch.org/tutorials/advanced/cpp_extension.html\" style=\"text-decoration:none;\"><font color=maroon>C++ extension</font></a> `Tutorials > Custom C++ and CUDA Extensions`, you can wrap them in `Function` to interface with the autograd engine. If you’d like to reduce the number of buffers saved for the backward pass, custom functions can be used to combine ops together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When not to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you can already write your function in terms of PyTorch’s built-in ops, its backward graph is (most likely) already able to be recorded by autograd. In this case, you do not need to implement the backward function yourself. Consider using a plain old Python function.\n",
    "\n",
    "If you need to maintain state, i.e., trainable parameters, you should (also) use a custom module. See the section below for more information on extending `torch.nn`.\n",
    "\n",
    "If you’d like to alter the gradients during the backward pass or perform a side effect, consider registering a <a href=\"https://pytorch.org/docs/stable/generated/torch.Tensor.register_hook.html#torch.Tensor.register_hook\" style=\"text-decoration:none;\"><font color=maroon></font></a> or <a href=\"https://pytorch.org/docs/stable/notes/modules.html#module-hooks\" style=\"text-decoration:none;\"><font color=maroon>tensor\n",
    "Module</font></a> hook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the following steps: \n",
    "1. Subclass <a href=\"https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function\" style=\"text-decoration:none;\"><font color=maroon><b>Function</b></font></a> and implement the <a href=\"https://pytorch.org/docs/stable/generated/torch.autograd.Function.forward.html#torch.autograd.Function.forward\" style=\"text-decoration:none;\"><font color=maroon>forward()</font></a> and <a href=\"https://pytorch.org/docs/stable/generated/torch.autograd.Function.backward.html#torch.autograd.Function.backward\" style=\"text-decoration:none;\"><font color=maroon>backward()</font></a> methods. \n",
    "2. Call the proper methods on the ***ctx*** argument. \n",
    "3. Declare whether your function supports double backward. \n",
    "4. Validate whether your gradients are correct using gradcheck."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3><b>Step 1:</b> After subclassing Function, you’ll need to define 2 methods:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color=maroon size=3><b>forward()</b></font> is the code that performs the operation. It can take as many arguments as you want, with some of them being optional, if you specify the default values. All kinds of Python objects are accepted here.<br>\n",
    "`Tensor` arguments that track history (i.e., with `requires_grad=True`) will be converted to ones that don’t track history before the call, and their use will be registered in the graph. <br>\n",
    "<font color=maroon>Note that</font> this logic won’t traverse lists/dicts/any other data structures and will only consider tensors that are direct arguments to the call. You can return either a single `Tensor` output, or a `tuple` of tensors if there are multiple outputs. Also, please refer to the docs of `Function` to find descriptions of useful methods that can be called only from `forward()`.\n",
    "\n",
    "<br>\n",
    "\n",
    "* <font color=maroon size=3><b>backward()</b></font> (or <font color=maroon size=3><b>vjp()</b></font>) defines the gradient formula. It will be given as many `Tensor` arguments as there were outputs, with each of them representing gradient w.r.t. that output. <font color=maroon>It is important NEVER to modify these in-place.</font> It should return as many tensors as there were inputs, with each of them containing the gradient w.r.t. its corresponding input. <br>\n",
    "If your inputs didn’t require gradient (`needs_input_grad` is a tuple of booleans indicating whether each input needs gradient computation), or were non-Tensor objects, you can return `None`. Also, if you have optional arguments to `forward()` you can return more gradients than there were inputs, as long as they’re all `None`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3><b>Step 2:</b> It is your responsibility to use the functions in the forward’s ***ctx*** properly in order to ensure that the new `Function` works properly with the `autograd engine`.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <a href=\"https://pytorch.org/docs/stable/generated/torch.autograd.function.FunctionCtx.save_for_backward.html#torch.autograd.function.FunctionCtx.save_for_backward\" style=\"text-decoration:none;\"><font color=maroon size=3>save_for_backward()</font></a> must be used when saving input or output tensors of the forward to be used later in the backward. Anything else, i.e., non-tensors and tensors that are neither input nor output should be stored directly on ***ctx***.\n",
    "\n",
    "\n",
    "* <a href=\"https://pytorch.org/docs/stable/generated/torch.autograd.function.FunctionCtx.mark_dirty.html#torch.autograd.function.FunctionCtx.mark_dirty\" style=\"text-decoration:none;\"><font color=maroon size=3>mark_dirty()</font></a> must be used to mark any input that is modified inplace by the forward function.\n",
    "\n",
    "\n",
    "* <a href=\"https://pytorch.org/docs/stable/generated/torch.autograd.function.FunctionCtx.mark_non_differentiable.html#torch.autograd.function.FunctionCtx.mark_non_differentiable\" style=\"text-decoration:none;\"><font color=maroon size=3>mark_non_differentiable()</font></a> must be used to tell the engine if an output is not differentiable. By default all output tensors that are of differentiable type will be set to require gradient. Tensors of non-differentiable type (i.e., integral types) are never marked as requiring gradients.\n",
    "\n",
    "\n",
    "* <a href=\"https://pytorch.org/docs/stable/generated/torch.autograd.function.FunctionCtx.set_materialize_grads.html#torch.autograd.function.FunctionCtx.set_materialize_grads\" style=\"text-decoration:none;\"><font color=maroon size=3>set_materialize_grads()</font></a> can be used to tell the autograd engine to optimize gradient computations in the cases where the output does not depend on the input by not materializing grad tensors given to backward function. <br>\n",
    "That is, if set to ``False``, ``None`` object in python or “undefined tensor” (tensor ``x`` for which ``x.defined()`` is ``False``) in C++ will not be converted to a tensor filled with zeros prior to calling backward, and so your code will need to handle such objects as if they were tensors filled with zeros. The default value of this setting is `True`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3><b>Step 3:</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your `Function` does not support double backward you should explicitly declare this by decorating backward with the <font color=maroon size=3>once_differentiable()</font>. With this decorator, attempts to perform double backward through your function will produce an error. See our double backward tutorial for more information on double backward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3><b>Step 4:</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is recommended that you use <a href=\"https://pytorch.org/docs/stable/generated/torch.autograd.gradcheck.html#torch.autograd.gradcheck\" style=\"text-decoration:none;\"><font color=maroon>torch.autograd.gradcheck()</font></a> to check whether your backward function correctly computes gradients of the forward by computing the Jacobian matrix using your backward function and comparing the value element-wise with the Jacobian computed numerically using finite-differencing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:magenta;font-size:110%;font-weight:bold\">Example</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you can find code for a `Linear` function from `torch.nn`, with additional comments:\n",
    "```python\n",
    "# Inherit from Function\n",
    "class LinearFunction(Function):\n",
    "\n",
    "    # Note that both forward and backward are @staticmethods\n",
    "    @staticmethod\n",
    "    # bias is an optional argument\n",
    "    def forward(ctx, input, weight, bias=None):\n",
    "        ctx.save_for_backward(input, weight, bias)\n",
    "        output = input.mm(weight.t())\n",
    "        if bias is not None:\n",
    "            output += bias.unsqueeze(0).expand_as(output)\n",
    "        return output\n",
    "\n",
    "    # This function has only a single output, so it gets only one gradient\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # This is a pattern that is very convenient - at the top of backward\n",
    "        # unpack saved_tensors and initialize all gradients w.r.t. inputs to None. \n",
    "        # Thanks to the fact that additional trailing Nones are ignored, \n",
    "        # the return statement is simple even when the function has optional inputs.\n",
    "        input, weight, bias = ctx.saved_tensors\n",
    "        grad_input = grad_weight = grad_bias = None\n",
    "\n",
    "        # These needs_input_grad checks are optional and there only to improve efficiency. \n",
    "        # If you want to make your code simpler, you can skip them. \n",
    "        # Returning gradients for inputs that don't require it is not an error.\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_input = grad_output.mm(weight)\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_weight = grad_output.t().mm(input)\n",
    "        if bias is not None and ctx.needs_input_grad[2]:\n",
    "            grad_bias = grad_output.sum(0)\n",
    "\n",
    "        return grad_input, grad_weight, grad_bias\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to make it easier to use these custom ops, we recommend aliasing their apply method:\n",
    "```python\n",
    "linear = LinearFunction.apply\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we give an additional example of a function that is parametrized by non-Tensor arguments:\n",
    "```python\n",
    "class MulConstant(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, tensor, constant):\n",
    "        # ctx is a context object that can be used to stash information\n",
    "        # for backward computation\n",
    "        ctx.constant = constant\n",
    "        return tensor * constant\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # We return as many input gradients as there were arguments.\n",
    "        # Gradients of non-Tensor arguments to forward must be None.\n",
    "        return grad_output * ctx.constant, None\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here, we optimize the above example by calling `set_materialize_grads(False)`:\n",
    "```python\n",
    "class MulConstant(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, tensor, constant):\n",
    "        ctx.set_materialize_grads(False)\n",
    "        ctx.constant = constant\n",
    "        return tensor * constant\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # Here we must handle None grad_output tensor. In this case we\n",
    "        # can skip unnecessary computations and just return None.\n",
    "        if grad_output is None:\n",
    "            return None, None\n",
    "\n",
    "        # We return as many input gradients as there were arguments.\n",
    "        # Gradients of non-Tensor arguments to forward must be None.\n",
    "        return grad_output * ctx.constant, None\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<font size=3 color=red><b>NOTE: </b></font>\n",
    "\n",
    "Inputs to `backward`, i.e., `grad_output`, can also be tensors that track history. So if `backward` is implemented with differentiable operations, (e.g., invocation of another custom `Function`), higher order derivatives will work. In this case, the tensors saved with `save_for_backward` can also be used in the backward and have gradients flowing back but tensors saved in the `ctx` won’t have gradients flowing back for them. If you need gradients to flow back for a Tensor saved in the `ctx`, you should make it an output of the custom `Function` and save it with `save_for_backward`.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You probably want to check if the backward method you implemented actually computes the derivatives of your function. It is possible by comparing with numerical approximations using small finite differences:\n",
    "```python\n",
    "from torch.autograd import gradcheck\n",
    "\n",
    "# gradcheck takes a tuple of tensors as input, check if your gradient\n",
    "# evaluated with these tensors are close enough to numerical\n",
    "# approximations and returns True if they all verify this condition.\n",
    "input = (torch.randn(20,20,dtype=torch.double,requires_grad=True),\n",
    "         torch.randn(30,20,dtype=torch.double,requires_grad=True))\n",
    "test = gradcheck(linear, input, eps=1e-6, atol=1e-4)\n",
    "print(test)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See `Docs > Automatic differentiation package - torch.autograd > `<a href=\"https://pytorch.org/docs/stable/autograd.html#grad-check\" style=\"text-decoration:none;\"><font color=maroon>Numerical gradient checking</font></a> for more details on finite-difference gradient comparisons. If your function is used in higher order derivatives (differentiating the backward pass) you can use the `gradgradcheck` function from the same package to check higher order derivatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward mode AD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overriding the forward mode AD formula has a very similar API with some different subtleties. You can implement the <a href=\"https://pytorch.org/docs/stable/generated/torch.autograd.Function.jvp.html#torch.autograd.Function.jvp\" style=\"text-decoration:none;\"><font color=maroon size=4><b>jvp()</b></font></a> function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will be given as many Tensor arguments as there were inputs, with each of them representing gradient w.r.t. that input. It should return as many tensors as there were outputs, with each of them containing the gradient w.r.t. its corresponding output. The `jvp()` will be called just after the `forward()` method, before the `apply()` returns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3 color=maroon><b>jvp()</b> has a few subtle differences with the <b>backward()</b> function:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* You can use the ctx to pass any data from the `forward()` to the `jvp()` function. If that state will not be needed for the `backward()`, you can explicitly free it by doing del ctx.foo at the end of the `jvp()` function.\n",
    "\n",
    "\n",
    "* The implementation of `jvp()` must be backward differentiable or explicitly check that none of the given forward mode gradient has **`requires_grad set`**.\n",
    "\n",
    "\n",
    "* The `jvp()` function must match the view/inplace behavior of `forward()`. For example, if the `i` th input is modified inplace, then the `i` th gradient must be updated inplace. Similarly, if the `j` th output is a view of the `k` th input. Then the returned `j` th output gradient must be a view of the given `k` th input gradient.\n",
    "\n",
    "\n",
    "* Because the user cannot specify which gradient needs to be computed, the `jvp()` function should always compute gradients for all the outputs.\n",
    "\n",
    "\n",
    "* The forward mode gradients do respect the flag set by **`set_materialize_grads()`** and you can get None input gradients when this is disabled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extending <font color=maroon><b>torch.nn</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3><font color=maroon>`nn` exports two kinds of interfaces - <b>modules</b> and <b>their functional versions</b>.</font> You can extend it in both ways, \n",
    "* but we recommend using `modules` for all kinds of layers, that hold any parameters or buffers, \n",
    "\n",
    "\n",
    "* and recommend using a `functional` for~m~ parameter-less operations like activation functions, pooling, etc.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a functional version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>Adding a functional version of an operation is already fully covered in the section above.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module\" style=\"text-decoration:none;\">Module</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `nn` heavily utilizes `autograd`, adding a new `Module` requires implementing a `Function` that performs the operation and can compute the gradient. <br>\n",
    "From now on let’s assume that we want to implement a `Linear module` and we have the function implemented as in the listing above. There’s very little code required to add this. Now, there are two functions that need to be implemented:\n",
    "* **`__init__`** (optional) - takes in arguments such as kernel sizes, numbers of features, etc. and initializes parameters and buffers.\n",
    "\n",
    "\n",
    "* **`forward()`** - instantiates a `Function` and uses it to perform the operation. It’s very similar to a functional wrapper shown above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how a `Linear module` can be implemented:\n",
    "```python\n",
    "class Linear(nn.Module):\n",
    "    def __init__(self, input_features, output_features, bias=True):\n",
    "        super(Linear, self).__init__()\n",
    "        self.input_features = input_features\n",
    "        self.output_features = output_features\n",
    "\n",
    "        # nn.Parameter is a special kind of Tensor, that will get automatically registered as \n",
    "        # Module's parameter once it's assigned as an attribute. \n",
    "        # Parameters and buffers need to be registered, or\n",
    "        # they won't appear in .parameters() (doesn't apply to buffers), \n",
    "        # and won't be converted when e.g. .cuda() is called. \n",
    "        # You can use .register_buffer() to register buffers.\n",
    "        #\n",
    "        # nn.Parameters require gradients by default.\n",
    "        self.weight = nn.Parameter(torch.empty(output_features, input_features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.empty(output_features))\n",
    "        else:\n",
    "            # You should always register all possible parameters, but the\n",
    "            # optional ones can be None if you want.\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        # Not a very smart way to initialize weights\n",
    "        nn.init.uniform_(self.weight, -0.1, 0.1)\n",
    "        if self.bias is not None:\n",
    "            nn.init.uniform_(self.bias, -0.1, 0.1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # See the autograd section for explanation of what happens here.\n",
    "        return LinearFunction.apply(input, self.weight, self.bias)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        # (Optional)Set the extra information about this module. \n",
    "        # You can test it by printing an object of this class.\n",
    "        return 'input_features={}, output_features={}, bias={}'.format(self.input_features, \n",
    "                                                                       self.output_features, \n",
    "                                                                       self.bias is not None )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Extending <font color=maroon><b>torch</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create custom types that emulate `Tensor` by defining a custom class with methods that match `Tensor`. But what if you want to be able to pass these types to functions like \n",
    "<a href=\"https://pytorch.org/docs/stable/generated/torch.add.html#torch.add\" style=\"text-decoration:none;\">torch.add()</a> in the top-level `torch` namespace that accept `Tensor` operands?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your custom python type defines a method named ***`__torch_function__`***, PyTorch will invoke your ***`__torch_function__`*** implementation when an instance of your custom class is passed to a function in the `torch` namespace.<br>\n",
    "\n",
    "This makes it possible to define custom implementations for any of the functions in the `torch` namespace which your ***`__torch_function__`*** implementation can call, allowing your users to make use of your custom type with existing PyTorch workflows that they have already written for `Tensor`. <br>\n",
    "\n",
    "This works with “duck” types that are unrelated to `Tensor` as well as user-defined subclasses of `Tensor`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending <font color=maroon>torch</font> with a <font color=maroon><b>Tensor-like</b></font> type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<font size=3 color=red><b>NOTE: </b></font>\n",
    "\n",
    "This functionality is inspired by the NumPy `__array_function__` protocol. See <a href=\"https://docs.scipy.org/doc/numpy/user/basics.dispatch.html#basics-dispatch\" style=\"text-decoration:none;\"><font color=red>the NumPy documentation</font></a> and <a href=\"https://numpy.org/neps/nep-0018-array-function-protocol.html\" style=\"text-decoration:none;\"><font color=red>NEP-0018</font></a> for more details.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make this concrete, let’s begin with a simple example that illustrates the API dispatch mechanism. We’ll create a custom type that represents a 2D scalar tensor, parametrized by the order `N` and value along the diagonal entries, `value`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScalarTensor(object):\n",
    "    \n",
    "    def __init__(self, N, value):\n",
    "        self._N = N\n",
    "        self._value = value\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"DiagonalTensor(N={}, value={})\".format(self._N, self._value)\n",
    "\n",
    "    def tensor(self):\n",
    "        return self._value * torch.eye(self._N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This first iteration of the design isn’t very useful. The main functionality of `ScalarTensor` is to provide a more compact string representation of a scalar tensor than in the base tensor class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DiagonalTensor(N=5, value=2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = ScalarTensor(5, 2)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 0., 0., 0., 0.],\n",
       "        [0., 2., 0., 0., 0.],\n",
       "        [0., 0., 2., 0., 0.],\n",
       "        [0., 0., 0., 2., 0.],\n",
       "        [0., 0., 0., 0., 2.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.tensor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we try to use this object with the `torch` API, we will run into issues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "mean(): argument 'input' (position 1) must be Tensor, not ScalarTensor",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11648/3740236751.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: mean(): argument 'input' (position 1) must be Tensor, not ScalarTensor"
     ]
    }
   ],
   "source": [
    "torch.mean(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding a `__torch_function__` implementation to `ScalarTensor` makes it possible for the above operation to succeed. Let’s re-do our implementation, this time adding a `__torch_function__` implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "HANDLED_FUNCTIONS = {}\n",
    "class ScalarTensor(object):\n",
    "    def __init__(self, N, value):\n",
    "        self._N = N\n",
    "        self._value = value\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"DiagonalTensor(N={}, value={})\".format(self._N, self._value)\n",
    "\n",
    "    def tensor(self):\n",
    "        return self._value * torch.eye(self._N)\n",
    "\n",
    "    @classmethod\n",
    "    def __torch_function__(cls, func, types, args=(), kwargs=None):\n",
    "        if kwargs is None:\n",
    "            kwargs = {}\n",
    "        if func not in HANDLED_FUNCTIONS or not all(\n",
    "            issubclass(t, (torch.Tensor, ScalarTensor)) for t in types):\n",
    "            return NotImplemented\n",
    "        return HANDLED_FUNCTIONS[func](*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `__torch_function__` method takes four arguments: \n",
    "* `func`, a reference to the torch API function that is being overridden, \n",
    "* `types`, the list of types of Tensor-likes that implement `__torch_function__`, \n",
    "* `args`, the tuple of arguments passed to the function, \n",
    "* and `kwargs`, the dict of keyword arguments passed to the function. \n",
    "\n",
    "It uses a global dispatch table named `HANDLED_FUNCTIONS` to store custom implementations. The keys of this dictionary are functions in the `torch` namespace and the values are implementations for `ScalarTensor`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<font size=3 color=red><b>NOTE: </b></font>\n",
    "\n",
    "Using a global dispatch table is not a mandated part of the `__torch_function__` API, it is just a useful design pattern for structuring your override implementations.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class definition isn’t quite enough to make torch.mean do the right thing when we pass it a `ScalarTensor` – we also need to define an implementation for `torch.mean` for `ScalarTensor` operands and add the implementation to the `HANDLED_FUNCTIONS` dispatch table dictionary. <font color=maroon>One way of doing this is to define a <b>decorator</b>:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "def implements(torch_function):\n",
    "    \"\"\"Register a torch function override for ScalarTensor\"\"\"\n",
    "    @functools.wraps(torch_function)\n",
    "    def decorator(func):\n",
    "        HANDLED_FUNCTIONS[torch_function] = func\n",
    "        return func\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which can be applied to the implementation of our override:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@implements(torch.mean)\n",
    "def mean(input):\n",
    "    return float(input._value) / input._N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this change we can now use `torch.mean` with `ScalarTensor`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = ScalarTensor(5, 2)\n",
    "torch.mean(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course `torch.mean` is an example of the simplest kind of function to override since it only takes one operand. We can use the same machinery to override a function that takes more than one operand, any one of which might be a tensor or tensor-like that defines `__torch_function__`, for example for <a href=\"https://pytorch.org/docs/stable/generated/torch.add.html#torch.add\" style=\"text-decoration:none;\">torch.add()</a>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_tensor(data):\n",
    "    if isinstance(data, ScalarTensor):\n",
    "        return data.tensor()\n",
    "    return torch.as_tensor(data)\n",
    "\n",
    "\n",
    "@implements(torch.add)\n",
    "def add(input, other):\n",
    "   try:\n",
    "       if input._N == other._N:\n",
    "           return ScalarTensor(input._N, input._value + other._value)\n",
    "       else:\n",
    "           raise ValueError(\"Shape mismatch!\")\n",
    "   except AttributeError:\n",
    "       return torch.add(ensure_tensor(input), ensure_tensor(other))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version has a fast path for when both operands are `ScalarTensor` instances and also a slower path which degrades to converting the data to tensors when either operand is not a `ScalarTensor`. That makes the override function correctly when either operand is a `ScalarTensor` or a regular `Tensor`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DiagonalTensor(N=2, value=4)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = ScalarTensor(2, 2)\n",
    "torch.add(s, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 1.],\n",
       "        [1., 3.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor([[1, 1,], [1, 1]])\n",
    "torch.add(s, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that our implementation of `add` does not take `alpha` or `out` as keyword arguments like <a href=\"https://pytorch.org/docs/stable/generated/torch.add.html#torch.add\" style=\"text-decoration:none;\">torch.add()</a> does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "add() got an unexpected keyword argument 'alpha'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11648/1728538881.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11648/3440402949.py\u001b[0m in \u001b[0;36m__torch_function__\u001b[1;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[0;32m     18\u001b[0m             issubclass(t, (torch.Tensor, ScalarTensor)) for t in types):\n\u001b[0;32m     19\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mHANDLED_FUNCTIONS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: add() got an unexpected keyword argument 'alpha'"
     ]
    }
   ],
   "source": [
    "torch.add(s, s, alpha=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For speed and flexibility the `__torch_function__` dispatch mechanism does not check that the signature of an override function matches the signature of the function being overrided in the `torch` API. For some applications ignoring optional arguments would be fine but to ensure full compatibility with `Tensor`, user implementations of torch API functions should take care to exactly emulate the API of the function that is being overrided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions in the torch API that do not have explicit overrides will return `NotImplemented` from `__torch_function__`. If all operands with `__torch_function__` defined on them return `NotImplemented`, PyTorch will raise a `TypeError`. This means that most of the time operations that do not have explicit overrides for a type will raise a `TypeError` when an instance of such a type is passed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "no implementation found for 'torch.mul' on types that implement __torch_function__: [000001B58B30C990]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11648/3830781741.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: no implementation found for 'torch.mul' on types that implement __torch_function__: [000001B58B30C990]"
     ]
    }
   ],
   "source": [
    "torch.mul(s, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原文档中的：\n",
    "```python\n",
    "TypeError: no implementation found for 'torch.mul' on types that implement __torch_function__: [ScalarTensor]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>In practice this means that if you would like to implement your overrides using a `__torch_function__` implementation along these lines, you will need to explicitly implement the full `torch` API or the entire subset of the API that you care about for your use case. <font color=maroon>This may be a tall order as the full `torch` API is quite extensive.</font></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3 color=maroon>Another option is to not return `NotImplemented` for operations that are not handled but to instead pass a `Tensor` to the original `torch` function when no override is available.</font> For example, if we change our implementation of `__torch_function__` for `ScalarTensor` to the one below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "@classmethod\n",
    "def __torch_function__(cls, func, types, args=(), kwargs=None):\n",
    "    if kwargs is None:\n",
    "        kwargs = {}\n",
    "    if func not in HANDLED_FUNCTIONS or not all(\n",
    "            issubclass(t, (torch.Tensor, ScalarTensor)) for t in types):\n",
    "        args = [a.tensor() if hasattr(a, 'tensor') else a for a in args]\n",
    "        return func(*args, **kwargs)\n",
    "    return HANDLED_FUNCTIONS[func](*args, **kwargs)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "HANDLED_FUNCTIONS = {}\n",
    "class ScalarTensor(object):\n",
    "    def __init__(self, N, value):\n",
    "        self._N = N\n",
    "        self._value = value\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"DiagonalTensor(N={}, value={})\".format(self._N, self._value)\n",
    "\n",
    "    def tensor(self):\n",
    "        return self._value * torch.eye(self._N)\n",
    "\n",
    "    @classmethod\n",
    "    def __torch_function__(cls, func, types, args=(), kwargs=None):\n",
    "        if kwargs is None:\n",
    "            kwargs = {}\n",
    "        if func not in HANDLED_FUNCTIONS or not all(\n",
    "                issubclass(t, (torch.Tensor, ScalarTensor))\n",
    "                for t in types\n",
    "            ):\n",
    "            args = [a.tensor() if hasattr(a, 'tensor') else a for a in args]\n",
    "            return func(*args, **kwargs)\n",
    "        return HANDLED_FUNCTIONS[func](*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then <a href=\"https://pytorch.org/docs/stable/generated/torch.mul.html#torch.mul\" style=\"text-decoration:none;\">torch.mul()</a> will work correctly, although the return type will always be a `Tensor` rather than a `ScalarTensor`, even if both operands are `ScalarTensor` instances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 0.],\n",
       "        [0., 4.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = ScalarTensor(2, 2)\n",
    "torch.mul(s, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>Also see the `MetadataTensor` example below for another variation on this pattern but instead always returns a `MetadataTensor` to propagate metadata through operations in the `torch` API.\n",
    "\n",
    "\n",
    "The `__torch_function__` protocol is designed for full coverage of the API, partial coverage may lead to undesirable results, in particular, certain functions raising a `TypeError`. This is especially true for subclasses, where all three of ***torch.add***, ***torch.Tensor.__add__*** and ***torch.Tensor.add*** must be covered, even if they return exactly the same result. Failing to do this may also lead to infinite recursion. <br>\n",
    "If one requires the implementation of a function from `torch.Tensor` subclasses, they must use `super().__torch_function__` inside their implementation.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subclassing <font color=maroon><b>torch.Tensor</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As of version 1.7.0, **`methods`**` on torch.Tensor` and **`functions`**` in public torch.*` namespaces applied on `torch.Tensor` subclasses will return subclass instances instead of `torch.Tensor` instances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubTensor(torch.Tensor):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SubTensor'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(torch.add(SubTensor([0]), SubTensor([1]))).__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SubTensor'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(torch.add(SubTensor([0]), torch.tensor([1]))).__name__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If multiple subclasses exist, the lowest one in the hierarchy will be chosen by default. If there is no unique way to determine such a case, then a `TypeError` is raised:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubTensor2(SubTensor):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SubTensor2'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(torch.add(SubTensor2([0]), SubTensor([1]))).__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SubTensor2'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(torch.add(SubTensor2([0]), torch.tensor([1]))).__name__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OtherSubTensor(torch.Tensor):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "no implementation found for 'torch.add' on types that implement __torch_function__: [000001B58C138D50, 000001B58C0763F0]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11648/2503873747.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSubTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOtherSubTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: no implementation found for 'torch.add' on types that implement __torch_function__: [000001B58C138D50, 000001B58C0763F0]"
     ]
    }
   ],
   "source": [
    "torch.add(SubTensor([0]), OtherSubTensor([1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原文档中的：\n",
    "```python\n",
    "TypeError: no implementation found for 'torch.add' on types that implement __torch_function__: [SubTensor, OtherSubTensor]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>If one wishes to have a global override for all tensor methods, one can use `__torch_function__`. Here is an example that logs all function/method calls:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class LoggingTensor(torch.Tensor):\n",
    "    @classmethod\n",
    "    def __torch_function__(cls, func, types, args=(), kwargs=None):\n",
    "        # NOTE: Logging calls Tensor.__repr__, so we can't log __repr__ without infinite recursion\n",
    "        if func is not torch.Tensor.__repr__:\n",
    "            logging.info(f\"func: {func.__name__}, args: {args!r}, kwargs: {kwargs!r}\")\n",
    "        if kwargs is None:\n",
    "            kwargs = {}\n",
    "        return super().__torch_function__(func, types, args, kwargs)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>However, if one instead wishes to override a method on the Tensor subclass, there one can do so either by directly overriding the method (by defining it for a subclass), or by using `__torch_function__` and matching with `func`.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One should be careful within `__torch_function__` for subclasses to always call `super().__torch_function__(func, ...)` instead of func directly, as was the case before version 1.7.0. Failing to do this may cause func to recurse back into `__torch_function__` and therefore cause infinite recursion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending <font color=maroon>torch</font> with a <font color=maroon><b>Tensor wrapper</b></font> type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(暂略)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operations on multiple types that define <font color=maroon>__torch_function__</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(暂略)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Coverage of Overrides for the PyTorch API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(暂略)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing custom C++ extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See this <a href=\"https://pytorch.org/tutorials/advanced/cpp_extension.html\" style=\"text-decoration:none;\"><font color=maroon>PyTorch tutorial</font></a> `Tutorials > Custom C++ and CUDA Extensions` for a detailed explanation and examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentations are available at <a href=\"https://pytorch.org/docs/stable/cpp_extension.html\" style=\"text-decoration:none;\"><font color=maroon>torch.utils.cpp_extension</font></a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing custom C extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example available at <a href=\"https://github.com/pytorch/extension-ffi\" style=\"text-decoration:none;\"><font color=maroon>this GitHub repository</font></a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ptg]",
   "language": "python",
   "name": "conda-env-ptg-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "269px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
