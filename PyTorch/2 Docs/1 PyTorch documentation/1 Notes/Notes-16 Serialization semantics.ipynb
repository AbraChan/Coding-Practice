{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=center><font color=maroon size=6><b>Serialization semantics</b></font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4><b>References:</b></font>\n",
    "* <a href=\"https://pytorch.org/docs/stable/index.html\" style=\"text-decoration:none;\">Docs > PyTorch documentation</a>\n",
    "\n",
    "    * **Notes**\n",
    "        * Docs > 16 <a href=\"https://pytorch.org/docs/stable/notes/serialization.html\" style=\"text-decoration:none;\">Serialization semantics</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>This note describes how you can save and load PyTorch tensors and module states in Python, and how to serialize Python modules so they can be loaded in C++.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and loading tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "<a href=\"https://pytorch.org/docs/master/generated/torch.save.html#torch.save\" style=\"text-decoration:none;font-size:120%\">torch.save()</a> and \n",
    "<a href=\"https://pytorch.org/docs/master/generated/torch.load.html#torch.load\" style=\"text-decoration:none;font-size:120%\">torch.load()</a> let you easily save and load tensors:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor([1., 2.])\n",
    "torch.save(t, './model_weights/tensor.pt')\n",
    "torch.load('./model_weights/tensor.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3 color=maroon>By convention, PyTorch files are typically written with a **`.pt`** or **`.pth`** extension.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>`torch.save()` and `torch.load()` <font color=maroon>use Python’s pickle by default</font>, so you can also save multiple tensors as part of Python objects like tuples, lists, and dicts:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': tensor([1., 2.]), 'b': tensor([3., 4.])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {'a': torch.tensor([1., 2.]), 'b': torch.tensor([3., 4.])}\n",
    "torch.save(d, './model_weights/tensor_dict.pt')\n",
    "torch.load('./model_weights/tensor_dict.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>Custom data structures that include PyTorch tensors can also be saved if the data structure is pickle-able.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and loading tensors preserves views"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>Saving tensors preserves their view relationships:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 4, 6, 8])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers = torch.arange(1, 10)\n",
    "evens = numbers[1::2]\n",
    "evens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 2, 3, 4, 5, 6, 7, 8, 9]), tensor([2, 4, 6, 8]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save([numbers, evens], './model_weights/tensors.pt')\n",
    "loaded_numbers, loaded_evens = torch.load('./model_weights/tensors.pt')\n",
    "l\n",
    "oaded_numbers, loaded_evens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4,  8, 12, 16])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_evens *= 2    # 这将会改变 loaded_numbers\n",
    "loaded_evens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  4,  3,  8,  5, 12,  7, 16,  9])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>Behind the scenes, these tensors share the same “storage.” See <a href=\"https://pytorch.org/docs/master/tensor_view.html\" style=\"text-decoration:none;font-size:120%\">Tensor Views</a> for more on views and storage.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>When PyTorch saves tensors it saves their **storage objects** and **tensor metadata** separately. <font color=maroon>This is an implementation detail that may change in the future, but it typically saves space and lets PyTorch easily reconstruct the view relationships between the loaded tensors.</font> In the above snippet, for example, only a single storage is written to `tensors.pt`.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>In some cases, however, <font color=maroon>saving the current **storage objects** may be unnecessary and create prohibitively large files</font>. In the following snippet a storage much larger than the saved tensor is written to a file:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "999"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large = torch.arange(1, 1000)\n",
    "small = large[0:5]\n",
    "torch.save(small, './model_weights/small.pt')\n",
    "\n",
    "loaded_small = torch.load('./model_weights/small.pt')\n",
    "loaded_small.storage().size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>Instead of saving only the five values in the small tensor to `small.pt`, the 999 values in the storage it shares with *large* were saved and loaded.\n",
    "\n",
    "When saving tensors with fewer elements than their storage objects, <font color=maroon>the size of the saved file can be reduced by first **cloning** the tensors</font>. **Cloning a tensor** produces a new tensor with a new storage object containing only the values in the tensor:\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large = torch.arange(1, 1000)\n",
    "small = large[0:5]\n",
    "torch.save(small.clone(), './model_weights/small.pt')   # saves a clone of small\n",
    "\n",
    "loaded_small = torch.load('./model_weights/small.pt')\n",
    "loaded_small.storage().size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3 color=maroon>Since the cloned tensors are independent of each other, however, they have none of the view relationships the original tensors did. If both file size and view relationships are important when saving tensors smaller than their storage objects, then care must be taken to construct new tensors that minimize the size of their storage objects but still have the desired view relationships before saving.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and loading torch.nn.Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See also: Tutorial: <a href=\"https://pytorch.org/tutorials/beginner/saving_loading_models.html\" style=\"text-decoration:none;color:maroon;font-size:120%;\">Saving and loading modules</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>In PyTorch, a module’s state is frequently serialized using a ‘state dict.’ A module’s state dict contains all of its `parameters` and `persistent buffers`:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('weight',\n",
       "  Parameter containing:\n",
       "  tensor([1., 1., 1.], requires_grad=True)),\n",
       " ('bias',\n",
       "  Parameter containing:\n",
       "  tensor([0., 0., 0.], requires_grad=True))]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn = torch.nn.BatchNorm1d(3, track_running_stats=True)\n",
    "list(bn.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('running_mean', tensor([0., 0., 0.])),\n",
       " ('running_var', tensor([1., 1., 1.])),\n",
       " ('num_batches_tracked', tensor(0))]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(bn.named_buffers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight', tensor([1., 1., 1.])),\n",
       "             ('bias', tensor([0., 0., 0.])),\n",
       "             ('running_mean', tensor([0., 0., 0.])),\n",
       "             ('running_var', tensor([1., 1., 1.])),\n",
       "             ('num_batches_tracked', tensor(0))])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>Instead of saving a module directly, for compatibility reasons it is recommended to instead save only its state dict. Python modules even have a function, <a href=\"https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.load_state_dict\" style=\"text-decoration:none;font-size:120%\">load_state_dict()</a>, to restore their states from a state dict:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(bn.state_dict(), './model_weights/bn.pt')\n",
    "bn_state_dict = torch.load('./model_weights/bn.pt')\n",
    "new_bn = torch.nn.BatchNorm1d(3, track_running_stats=True)\n",
    "new_bn.load_state_dict(bn_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>Note that the state dict is first loaded from its file with `torch.load()` and the state then restored with `load_state_dict()`.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3 color=maroon>Even custom modules and modules containing other modules have state dicts and can use this pattern:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('l0.weight',\n",
       "              tensor([[ 0.4406,  0.0225,  0.2327,  0.2363],\n",
       "                      [-0.0538, -0.0514, -0.4410, -0.4944]])),\n",
       "             ('l0.bias', tensor([ 0.1049, -0.0507])),\n",
       "             ('l1.weight', tensor([[ 0.2251, -0.6717]])),\n",
       "             ('l1.bias', tensor([-0.3721]))])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyModule(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModule, self).__init__()\n",
    "        self.l0 = torch.nn.Linear(4, 2)\n",
    "        self.l1 = torch.nn.Linear(2, 1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        out0 = self.l0(input)\n",
    "        out0_relu = torch.nn.functional.relu(out0)\n",
    "        return self.l1(out0_relu)\n",
    "\n",
    "\n",
    "m = MyModule()\n",
    "m.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(m.state_dict(), './model_weights/mymodule.pt')\n",
    "m_state_dict = torch.load('./model_weights/mymodule.pt')\n",
    "new_m = MyModule()\n",
    "new_m.load_state_dict(m_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=maroon>Serializing torch.nn.Modules and loading them in C++</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See also: <a href=\"https://pytorch.org/tutorials/advanced/cpp_export.html\" style=\"text-decoration:none;color:maroon;font-size:120%;\">Tutorial: Loading a TorchScript Model in C++</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>ScriptModules can be serialized as a TorchScript program and loaded using <a href=\"https://pytorch.org/docs/master/generated/torch.jit.load.html#torch.jit.load\" style=\"text-decoration:none;font-size:120%\">torch.jit.load()</a>. This serialization encodes all the modules’ methods, submodules, parameters, and attributes, and it allows the serialized program to be loaded in C++ (i.e. without Python).</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3 color=maroon>The distinction between `torch.jit.save()` and `torch.save()` may not be immediately clear.</font> \n",
    "\n",
    "* <a href=\"https://pytorch.org/docs/master/generated/torch.save.html#torch.save\" style=\"text-decoration:none;font-size:140%\">torch.save()</a> saves Python objects with pickle. This is especially useful for prototyping, researching, and training. \n",
    "\n",
    "\n",
    "* <a href=\"https://pytorch.org/docs/master/generated/torch.jit.save.html#torch.jit.save\" style=\"text-decoration:none;font-size:140%\">torch.jit.save()</a>, on the other hand, serializes ScriptModules to a format that can be loaded in Python or C++. This is useful when saving and loading C++ modules or for running modules trained in Python with C++, a common practice when deploying PyTorch models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color=maroon>To **script**, **serialize** and **load** a module in Python:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RecursiveScriptModule(\n",
       "  original_name=MyModule\n",
       "  (l0): RecursiveScriptModule(original_name=Linear)\n",
       "  (l1): RecursiveScriptModule(original_name=Linear)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scripted_module = torch.jit.script(MyModule())   # script\n",
    "torch.jit.save(scripted_module, './model_weights/mymodule.pt')   # serialize\n",
    "torch.jit.load('./model_weights/mymodule.pt')                    # load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3><font color=maroon>**Traced modules** can also be saved with `torch.jit.save()`, with the caveat that only the traced code path is serialized.</font> The following example demonstrates this:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A module with control flow\n",
    "class ControlFlowModule(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(ControlFlowModule, self).__init__()\n",
    "        self.l0 = torch.nn.Linear(4, 2)\n",
    "        self.l1 = torch.nn.Linear(2, 1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        if input.dim() > 1:\n",
    "            return torch.tensor(0)\n",
    "\n",
    "        out0 = self.l0(input)\n",
    "        out0_relu = torch.nn.functional.relu(out0)\n",
    "        return self.l1(out0_relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1782],\n",
       "        [0.4974]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traced_module = torch.jit.trace(ControlFlowModule(), torch.randn(4))\n",
    "torch.jit.save(traced_module, './model_weights/controlflowmodule_traced.pt')\n",
    "loaded = torch.jit.load('./model_weights/controlflowmodule_traced.pt')\n",
    "loaded(torch.randn(2, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scripted_module = torch.jit.script(ControlFlowModule(), torch.randn(4))\n",
    "torch.jit.save(scripted_module, './model_weights/controlflowmodule_scripted.pt')\n",
    "loaded = torch.jit.load('./model_weights/controlflowmodule_scripted.pt')\n",
    "loaded(torch.randn(2, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3 color=maroon><br>\n",
    "* The above module has an if statement that is **not triggered by the traced inputs**, and so is not part of the **traced module** and not serialized with it. \n",
    "\n",
    "    \n",
    "* The **scripted module**, however, contains the if statement and is serialized with it. </font>\n",
    "\n",
    "See the <a href=\"https://pytorch.org/docs/stable/jit.html\" style=\"text-decoration:none;color:maroon;font-size:120%;\">TorchScript documentation</a> for more on scripting and tracing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>Finally, to load the module in C++:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5>\n",
    "    \n",
    "```c++\n",
    "torch::jit::script::Module module;\n",
    "```\n",
    "```c++\n",
    "module = torch::jit::load('controlflowmodule_scripted.pt');\n",
    "```\n",
    "    \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the <a href=\"https://pytorch.org/cppdocs/\" style=\"text-decoration:none;color:maroon;font-size:120%;\">PyTorch C++ API documentation</a> for details about how to use PyTorch modules in C++."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and loading ScriptModules across PyTorch versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3><font color=maroon>The PyTorch Team recommends saving and loading modules with the same version of PyTorch.</font> Older versions of PyTorch may not support newer modules, and newer versions may have removed or modified older behavior. These changes are explicitly described in PyTorch’s release notes, and modules relying on functionality that has changed may need to be updated to continue working properly. </font>\n",
    "\n",
    "\n",
    "<font color=maroon size=3>In limited cases, detailed below, PyTorch will preserve the historic behavior of serialized ScriptModules so they do not require an update.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.div performing integer division"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "详见 <a href=\"https://pytorch.org/docs/master/notes/serialization.html#torch-div-performing-integer-division\" style=\"text-decoration:none;color:maroon;font-size:120%;\">这里</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.full always inferring a float dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "详见 <a href=\"https://pytorch.org/docs/master/notes/serialization.html#torch-full-always-inferring-a-float-dtype\" style=\"text-decoration:none;color:maroon;font-size:120%;\">这里</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ptg]",
   "language": "python",
   "name": "conda-env-ptg-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "226px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
