{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=center><font color=maroon size=8><b>Autograd</b></font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4><b>References:</b></font>\n",
    "* `Tutorials >` Deep Learning with PyTorch: A 60 Minute Blitz > <a href=\"https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html\" style=\"text-decoration:none;\">A Gentle Introduction to torch.autograd</a>\n",
    "* `Tutorials > Learn the Basics >` <a href=\"https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html\" style=\"text-decoration:none;\">Automatic Differentiation with torch.autograd</a> (Automatic Differentiation)\n",
    "* \n",
    "* <a href=\"https://pytorch.org/docs/stable/index.html\" style=\"text-decoration:none;\">Docs > PyTorch documentation</a>\n",
    "    * \n",
    "    * **Developer Notes**\n",
    "        * `Docs > 2` <a href=\"https://pytorch.org/docs/stable/notes/autograd.html\" style=\"text-decoration:none;\">Autograd mechanics</a>\n",
    "        * `Docs > 10` <a href=\"https://pytorch.org/docs/stable/notes/gradcheck.html\" style=\"text-decoration:none;\">Gradcheck mechanics</a>\n",
    "    * \n",
    "    * `Docs >` <a href=\"https://pytorch.org/docs/stable/autograd.html\" style=\"text-decoration:none;\">Automatic differentiation package - torch.autograd</a>\n",
    "    * `Docs >` Automatic differentiation package - torch.autograd > <a href=\"https://pytorch.org/docs/stable/generated/torch.autograd.grad.html\" style=\"text-decoration:none;\">torch.autograd.grad</a>\n",
    "    * `Docs >` Automatic differentiation package - torch.autograd > <a href=\"https://pytorch.org/docs/stable/generated/torch.autograd.backward.html\" style=\"text-decoration:none;\">torch.autograd.backward</a>\n",
    "    * \n",
    "    * `Tutorials >` <a href=\"https://pytorch.org/tutorials/intermediate/forward_ad_usage.html\" style=\"text-decoration:none;\">Forward-mode Automatic Differentiation (Beta)</a> （forward-mode AD tutorial）\n",
    "    * `Docs >` <a href=\"https://pytorch.org/cppdocs/notes/inference_mode.html\" style=\"text-decoration:none;\">Inference Mode</a>\n",
    "    * Docs > <a href=\"\" style=\"text-decoration:none;\"></a>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=gray size=3>Tutorials > Deep Learning with PyTorch: A 60 Minute Blitz > </font>\n",
    "\n",
    "## <font style=\"font-size:120%;color:maroon;font-weight:bold\">A Gentle Introduction to torch.autograd</font> <a href=\"https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html\" style=\"text-decoration:none;\"><font size=2>[link]</font></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "详见:\n",
    "* 上面蓝色 [link] 的链接，\n",
    "* 或者见自己笔记：`D:\\KeepStudy\\0_Coding\\Pytorch\\1 Tutorials\\4 Learning PyTorch\\4-1 Deep Learning with PyTorch - A 60 Minute Blitz .ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=gray size=3>Tutorials > Learn the Basics</font>\n",
    "\n",
    "## <font style=\"font-size:120%;color:maroon;font-weight:bold\">Automatic Differentiation with torch.autograd</font> <a href=\"https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html\" style=\"text-decoration:none;\"><font size=2>[link]</font></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training neural networks, the most frequently used algorithm is ***back propagation***. In this algorithm, parameters (model weights) are adjusted according to the **gradient** of the loss function with respect to the given parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute those gradients, PyTorch has a built-in differentiation engine called <font color=blue size=3>**torch.autograd**</font>. <font color=maroon>It supports automatic computation of gradient for any computational graph.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the simplest one-layer neural network, with input `x`, parameters `w` and `b`, and some loss function. It can be defined in PyTorch in the following manner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.ones(5)      # input tensor\n",
    "y = torch.zeros(3)     # expected output\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "\n",
    "z = torch.matmul(x, w)+b\n",
    "\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors, Functions and Computational graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code defines the following <font size=4 color=blue>**computational graph**:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../1 Tutorials/images/simple computational graph.png\" width=600px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this network, `w` and `b` are parameters, which we need to optimize. Thus, we need to be able to compute the gradients of loss function with respect to those variables. <font color=maroon>In order to do that, we set the ***requires_grad*** property of those tensors.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<font size=3 color=red><b>NOTE: </b></font>\n",
    "\n",
    "You can set the value of **`requires_grad`** when creating a tensor, or later by using **`x.requires_grad_(True)`** method.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3><font color=maroon>A function that we apply to tensors to construct computational graph is in fact an object of class **`Function`**.</font> This object knows how to compute the function in the ***forward*** direction, and also how to compute its derivative during the ***backward propagation*** step. <br>\n",
    "<font color=maroon>A reference to the backward propagation function is stored in ***`grad_fn`*** property of a tensor.</font> You can find more information of `Function` <a href=\"https://pytorch.org/docs/stable/autograd.html#function\" style=\"text-decoration:none;\">in the documentation</a>.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient function for z = <AddBackward0 object at 0x000002158DC27970>\n",
      "Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x000002158DC27610>\n"
     ]
    }
   ],
   "source": [
    "print(f\"Gradient function for z = {z.grad_fn}\")\n",
    "print(f\"Gradient function for loss = {loss.grad_fn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To optimize weights of parameters in the neural network, we need to compute the derivatives of our loss function with respect to parameters, namely, we need <font size=4>$\\frac{\\partial loss}{\\partial w}$</font> and <font size=4>$\\frac{\\partial loss}{\\partial b}$</font>  <font color=maroon>under some fixed values of `x` and `y`.</font> \n",
    "\n",
    "To compute those derivatives, we call `loss.backward()`, and then retrieve the values from `w.grad` and `b.grad`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1149, 0.0778, 0.0487],\n",
      "        [0.1149, 0.0778, 0.0487],\n",
      "        [0.1149, 0.0778, 0.0487],\n",
      "        [0.1149, 0.0778, 0.0487],\n",
      "        [0.1149, 0.0778, 0.0487]])\n",
      "tensor([0.1149, 0.0778, 0.0487])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<font size=3 ><font color=red><b>NOTE: </b></font>\n",
    "\n",
    "* We can only obtain the `grad` properties for the leaf nodes of the computational graph, which have `requires_grad` property set to `True`. For all other nodes in our graph, gradients will not be available.\n",
    "    \n",
    "    <br>\n",
    "    \n",
    "* We can only perform gradient calculations using `backward` once on a given graph, for performance reasons. If we need to do several `backward` calls on the same graph, we need to pass `retain_graph=True` to the `backward` call.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disabling Gradient Tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=maroon>By default, all tensors with `requires_grad=True` are tracking their computational history and support gradient computation. </font>\n",
    "    \n",
    "However, there are some cases when we do not need to do that, for example, when we have trained the model and just want to apply it to some input data, i.e. we only want to do forward computations through the network. \n",
    "\n",
    "<br>\n",
    "\n",
    "<font size=3 color=maroon>**①** We can stop tracking computations by surrounding our computation code with `torch.no_grad()` block:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3 color=maroon>**②** Another way to achieve the same result is to use the `detach()` method on the tensor:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)\n",
    "\n",
    "\n",
    "z_det = z.detach()\n",
    "print(z_det.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3><font color=maroon>There are reasons you might want to disable gradient tracking:</font>\n",
    "* To mark some parameters in your neural network as **frozen parameters**. This is a very common scenario for <a href=\"https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html\" style=\"text-decoration:none;font-size:120%\">finetuning a pretrained network</a>.\n",
    "    \n",
    "    \n",
    "* To **speed up computations** when you are only doing forward pass, because computations on tensors that do not track gradients would be more efficient.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More on Computational Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conceptually, autograd keeps a record of data (tensors) & all executed operations (along with the resulting new tensors) in a <font size=3 color=blue><b>directed acyclic graph `(`DAG`)`</b></font> consisting of \n",
    "<a href=\"https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function\" style=\"text-decoration:none;\"><font size=3>Function</font></a> objects.\n",
    "\n",
    "<font size=3>In this DAG, <font color=blue><b>leaves</b></font> are the `input tensors`, <font color=blue><b>roots</b></font> are the `output tensors`. By tracing this graph from roots to leaves, you can automatically compute the gradients using the chain rule.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a <font size=3 color=maroon>**forward pass**</font>, autograd does two things simultaneously:\n",
    "\n",
    "* run the `requested operation` to compute a resulting tensor, and\n",
    "* maintain the operation’s `gradient function` in the DAG.\n",
    "\n",
    "\n",
    "The <font size=3 color=maroon>**backward pass**</font> kicks off when `.backward()` is called on the DAG root. `autograd` then:\n",
    "\n",
    "* computes the gradients from each `.grad_fn`,\n",
    "* accumulates them in the respective tensor’s `.grad` attribute, and\n",
    "* using the chain rule, propagates all the way to the leaf tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<font size=3 color=red><b>NOTE: </b></font><br><br>\n",
    "<font size=3>\n",
    "**DAGs are dynamic in PyTorch** An important thing to note is that the graph is recreated from scratch; after each `.backward()` call, autograd starts populating a new graph. This is exactly what allows you to use control flow statements in your model; you can change the shape, size and operations at every iteration if needed.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional Reading: Tensor Gradients and Jacobian Products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=maroon>In many cases, we have a ***`scalar`***` loss function`, and we need to compute the gradient with respect to some parameters. However, there are cases when the output function is an arbitrary tensor. In this case, PyTorch allows you to compute so-called ***Jacobian product***, and **not the actual gradient**.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a vector function <font size=4>$\\vec{y}=f(\\vec{x})$</font>, where <font size=4>$\\vec{x}=\\langle x_1,\\dots,x_n\\rangle$</font> and <font size=4>$\\vec{y}=\\langle y_1,\\dots,y_m\\rangle$</font>, a gradient of <font size=4>$\\vec{y}$</font> with respect to <font size=4>$\\vec{x}$</font> is given by **Jacobian matrix**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "$$\n",
    "J = \\left(\\frac{∂\\mathcal{y}}{∂x_1} \\cdots \\frac{∂\\mathcal{y}}{∂x_n} \\right)\n",
    "  = \n",
    "\\left(\n",
    "\\begin{matrix}\n",
    "\\frac{∂y_1}{∂x_1}  & \\cdots & \\frac{∂y_1}{∂x_n}      \\\\\n",
    "\\vdots             & \\ddots & \\vdots \\\\\n",
    "\\frac{∂y_m}{∂x_1}  & \\cdots & \\frac{∂y_m}{∂x_n}      \\\\\n",
    "\\end{matrix}\n",
    "\\right)\n",
    "$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3 color=maroon>Instead of computing the Jacobian matrix itself, PyTorch allows you to compute ***Jacobian Product*** <font size=4>$ \\ \\ \\ v^T\\cdot J$</font> for a given input vector <font size=4>$v=(v_1 \\dots v_m)$</font>. This is achieved by calling **`backward`** with <font size=4>$v$</font> as an argument. </font>\n",
    "    \n",
    "<font size=3>The size of <font size=4>$v$</font> should be the same as the size of the original tensor, with respect to which we want to compute the product:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 1.]], requires_grad=True)\n",
      "tensor([[4., 1., 1., 1., 1.],\n",
      "        [1., 4., 1., 1., 1.],\n",
      "        [1., 1., 4., 1., 1.],\n",
      "        [1., 1., 1., 4., 1.],\n",
      "        [1., 1., 1., 1., 4.]], grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "inp = torch.eye(5, requires_grad=True)   # x\n",
    "out = (inp+1).pow(2)                     # y = (x+1)^2\n",
    "\n",
    "print(inp)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First call\n",
      "tensor([[4., 2., 2., 2., 2.],\n",
      "        [2., 4., 2., 2., 2.],\n",
      "        [2., 2., 4., 2., 2.],\n",
      "        [2., 2., 2., 4., 2.],\n",
      "        [2., 2., 2., 2., 4.]])\n",
      "\n",
      "Second call\n",
      "tensor([[8., 4., 4., 4., 4.],\n",
      "        [4., 8., 4., 4., 4.],\n",
      "        [4., 4., 8., 4., 4.],\n",
      "        [4., 4., 4., 8., 4.],\n",
      "        [4., 4., 4., 4., 8.]])\n",
      "\n",
      "Third call\n",
      "tensor([[12.,  6.,  6.,  6.,  6.],\n",
      "        [ 6., 12.,  6.,  6.,  6.],\n",
      "        [ 6.,  6., 12.,  6.,  6.],\n",
      "        [ 6.,  6.,  6., 12.,  6.],\n",
      "        [ 6.,  6.,  6.,  6., 12.]])\n",
      "\n",
      "Call after zeroing gradients\n",
      "tensor([[4., 2., 2., 2., 2.],\n",
      "        [2., 4., 2., 2., 2.],\n",
      "        [2., 2., 4., 2., 2.],\n",
      "        [2., 2., 2., 4., 2.],\n",
      "        [2., 2., 2., 2., 4.]])\n"
     ]
    }
   ],
   "source": [
    "# ∂y/∂x = 2*(x+1)\n",
    "out.backward(torch.ones_like(inp), retain_graph=True)\n",
    "print(f\"First call\\n{inp.grad}\")\n",
    "\n",
    "\n",
    "# ∂^2{y}/∂^2{x} = \n",
    "out.backward(torch.ones_like(inp), retain_graph=True)\n",
    "print(f\"\\nSecond call\\n{inp.grad}\")\n",
    "\n",
    "\n",
    "out.backward(torch.ones_like(inp), retain_graph=True)\n",
    "print(f\"\\nThird call\\n{inp.grad}\")\n",
    "\n",
    "\n",
    "inp.grad.zero_()\n",
    "out.backward(torch.ones_like(inp), retain_graph=True)\n",
    "print(f\"\\nCall after zeroing gradients\\n{inp.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3 color=maroon>**Notice that**</font> when we call `backward` for the second time with the same argument, the value of the gradient is different. <font size=3 color=maroon>This happens because when doing `backward` propagation, PyTorch **accumulates the gradients**, i.e. the value of computed gradients is added to the `grad` property of all leaf nodes of computational graph. If you want to compute the proper gradients, you need to **zero out the** `grad` property before. In real-life training an ***optimizer*** helps us to do this.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<font size=3 color=red><b>NOTE: </b></font>\n",
    "\n",
    "Previously we were calling `backward()` function without parameters. This is essentially equivalent to calling `backward(torch.tensor(1.0))`, which is a useful way to compute the gradients in case of a scalar-valued function, such as loss during neural network training.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <a href=\"https://pytorch.org/docs/stable/notes/autograd.html\" style=\"text-decoration:none;font-size:120%;color:maroon\">Autograd Mechanics</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=gray size=3>Docs > Notes9 </font>\n",
    "\n",
    "# <font style=\"font-size:120%;color:maroon;font-weight:bold\">Gradcheck mechanics</font> <a href=\"https://pytorch.org/docs/stable/notes/gradcheck.html\" style=\"text-decoration:none;\"><font size=2>[link]</font></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This note presents an overview of how the <a href=\"https://pytorch.org/docs/stable/generated/torch.autograd.gradcheck.html\" style=\"text-decoration:none;\"><font size=4>gradcheck()</font></a> and <a href=\"https://pytorch.org/docs/stable/generated/torch.autograd.gradgradcheck.html\" style=\"text-decoration:none;\"><font size=4>gradgradcheck()</font></a> functions work.\n",
    "\n",
    "It will cover both **`forward and backward mode AD`** for both real and complex-valued functions as well as higher-order derivatives. This note also covers both the default behavior of gradcheck as well as the case where `fast_mode=True` argument is passed (referred to as fast gradcheck below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:maroon;font-size:110%\">Notations and background information</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout this note, we will use the following convention:\n",
    "* <font size=3>$x,y,a,b,v,u,ur$</font> and <font size=3>$ui$</font> are `real-valued vectors` and <font size=3>$z$</font> is a `complex-valued vector` that can be rewritten in terms of two real-valued vectors as <font size=3>$z=a+ib$</font>.\n",
    "\n",
    "\n",
    "* <font size=3>$N$</font> and <font size=3>$M$</font> are two integers that we will use for the dimension of the input and output space respectively.\n",
    "\n",
    "\n",
    "* <font size=3>$f: \\ R^N → R^M$</font> is our basic real-to-real function such that <font size=3>$y=f(x)$</font>.\n",
    "\n",
    "\n",
    "* <font size=3>$g: \\ C^N → R^M$</font> is our basic complex-to-real function such that <font size=3>$y=g(x)$</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=magenta size=4>For the simple <b>real-to-real case</b></font>, we write as <font size=4 color=maroon><b>$J_f$</b></font> the Jacobian matrix associated with <font size=3><b>$f$</b></font> of size <font size=3 color=maroon><b>$M*N$</b></font>. This matrix contains all the partial derivatives such that the entry at position <font size=3><b>$(i,j)$</b></font> contains <font size=5><b>$\\frac{\\partial y_i}{\\partial x_j}$</b></font>. \n",
    "\n",
    "* **Backward mode AD** is then computing, for a given vector <font size=4 color=maroon><b>$v$</b></font> of size <font size=3 color=maroon><b>$M$</b></font>, the quantity <font size=4 color=maroon><b>$v^T J_f$</b></font>. \n",
    "\n",
    "\n",
    "* **Forward mode AD** on the other hand is computing, for a given vector <font size=4 color=maroon><b>$u$</b></font> of size <font size=3 color=maroon><b>$N$</b></font>, the quantity <font size=4 color=maroon><b>$J_f u$</b></font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For functions that contain `complex values`, the story is a lot more complex. We only provide the gist here and the full description can be found at `Docs > Autograd mechanics > `<a href=\"https://pytorch.org/docs/stable/notes/autograd.html#complex-autograd-doc\" style=\"text-decoration:none;\"><font color=maroon>Autograd for Complex Numbers</font></a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "<font size=3><b>$上面是 \\ real \\ case  \\ 的情况，下面是 \\ complex \\ case  \\ 的情况$</b></font>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:blue;font-size:110%;font-weight:bold;\">Wirtinger calculus</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The constraints to `satisfy complex differentiability` (<font size=3 color=blue>Cauchy-Riemann equations</font>) are <font color=red>too restrictive</font> for `all real-valued loss functions`, so we instead opted to use <font size=3 color=blue><b>Wirtinger calculus</b></font>. \n",
    "\n",
    "In a basic setting of `Wirtinger calculus`, the chain rule requires access to both the **`Wirtinger derivative`** (called <font size=3><b>$W$</b></font> below) and the **`Conjugate Wirtinger derivative`** (called <font size=3><b>$CW$</b></font> below). \n",
    "\n",
    "<br>\n",
    "\n",
    "<font color=maroon size=3>Both <b>$W$</b> and <b>$CW$</b> <b>need to be propagated</b> because in general, despite their name, <b>one is not the complex conjugate of the other</b>.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=maroon size=4>To avoid having to propagate both values:</font>\n",
    "\n",
    "* <font color=maroon size=3><b>For backward mode AD</b>, we always work under the assumption that</font> `the function whose derivative is being calculated is either a real-valued function or is part of a bigger real-valued function`. This assumption <font color=maroon><b>means that</b></font> all the intermediary gradients we compute during the backward pass are also associated with real-valued functions. <br><br>In practice, this assumption is not restrictive when doing optimization as such problem require real-valued objectives (as there is no natural ordering of the complex numbers).\n",
    "<br>\n",
    "<br>\n",
    "Under this assumption, using <font size=3><b>$W$</b></font> and <font size=3><b>$CW$</b></font> definitions, we can show that <font size=3><b>$W = CW^*$</b></font> (we use <font size=3><b>$*$</b></font> to denote complex conjugation here) and so only one of the two values actually need to be “backwarded through the graph” as the other one can easily be recovered.\n",
    "<br>\n",
    "<br>\n",
    "<font color=maroon>To simplify internal computations, <b>PyTorch</b> uses <font size=3><b>$2∗CW$</b></font> as the value it backwards and returns when the user asks for gradients.</font>\n",
    "<br>\n",
    "<font size=3>Similarly to the real case, when the output is actually in $R^M$, backward mode AD does not compute $2∗CW$ <font color=maroon>but only $v^T (2 * CW)$ for a given vector $v∈R^M$.</font></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font size=3 color=maroon><b>For forward mode AD</b>, we use a similar logic,</font> `in this case, assuming that the function is part of a larger function whose input is in`***`R`***. \n",
    "<br>\n",
    "<br>\n",
    "Under this assumption, we can make a similar claim that every intermediary result corresponds to a function whose input is in <font size=3>$R$</font> and in this case, using <font size=3>$W$</font> and <font size=3>$CW$</font> definitions, we can show that <font size=3>$W=CW$</font> for the intermediary functions. \n",
    "<br>\n",
    "<br>\n",
    "To make sure the forward and backward mode compute the same quantities in the elementary case of a one dimensional function, the forward mode also computes <font size=3>$2∗CW$</font>. \n",
    "<br>\n",
    "<font size=3>Similarly to the real case, when the input is actually in $R^N$, forward mode AD does not compute $2∗CW$ <font color=maroon>but only $(2∗CW)u$ for a given vector $u∈R^N$.</font></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:maroon;font-size:110%\"><b>Default</b> backward mode gradcheck behavior</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red>Real-to-real</font> functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test a function <font size=4>$f:R^N \\to R^M, x\\to y$</font>, we reconstruct the full Jacobian matrix <font size=4>$J_f$</font> of size <font size=3>$M×N$</font> in two ways: <font size=4 color=maroon><b>analytically</b> and <b>numerically</b>. </font>\n",
    "\n",
    "* The ***`analytical version`***` uses our `**`backward mode AD`** \n",
    "* while the ***`numerical version`***` uses `**`finite difference`**. \n",
    "\n",
    "<font color=maroon size=3>The two reconstructed <b>Jacobian matrices</b> are then compared <i>elementwise</i> for equality.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font style=\"font-size:110%\">Default real input `numerical` evaluation</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we consider the elementary case of a one-dimensional function (N=M=1), then we can use the basic finite difference formula from <a href=\"https://en.wikipedia.org/wiki/Finite_difference\" style=\"text-decoration:none;\">the wikipedia article</a>. We use the <font size=3 color=maroon><b>“central difference”</b> for better numerical properties</font>:\n",
    "\n",
    "<br>\n",
    "\n",
    "<font size=4>$$\\frac{∂y}{∂x}≈\\frac{f(x+eps)-f(x-eps)}{2*eps}$$</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This formula easily generalizes for multiple outputs <font size=4>$(M \\gt 1)$</font> by having <font size=5>$\\frac{\\partial y}{\\partial x}$</font> be <font size=4 color=maroon>a <b>column vector</b> of size $M \\times 1$ like $f(x + eps)$</font>. In that case, the above formula can be re-used as-is and approximates the full Jacobian matrix with only two evaluations of the user function (namely <font size=4>$f(x+eps) \\ and \\ f(x−eps)$</font>).\n",
    "\n",
    "\n",
    "* It is more computationally expensive to handle the case with multiple inputs <font size=4>$(N \\gt 1)$</font>. In this scenario, we loop over all the inputs one after the other and apply the <font size=4>$eps$</font> perturbation for each element of <font size=4>$x$</font> one after the other. This allows us to reconstruct the <font size=4 color=maroon>$J_f$ matrix <b>column by column</b>.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font style=\"font-size:110%\">Default real input `analytical` evaluation</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the analytical evaluation, we use the fact, as described above, that backward mode AD computes <font size=4 color=maroon>$v^TJ_f$</font>. \n",
    "\n",
    "* For functions with a single output, we simply use <font size=3>$v=1$</font> to `recover the full Jacobian matrix` with a single backward pass.\n",
    "\n",
    "\n",
    "* For functions with more than one output, we `resort to a for-loop` which iterates over the outputs where <font color=maroon size=4>each $v$ is a <b>one-hot vector</b></font> corresponding to each output one after the other. This allows to <font size=4 color=maroon>reconstruct the $J_f$ matrix <b>row by row</b>.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red>Complex-to-real</font> functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test a function <font size=4 color=maroon>$g: \\mathcal{C}^N \\to \\mathcal{R}^M, z \\to y$</font> with <font size=4>$z=a+ib$</font>, we reconstruct the (complex-valued) matrix that contains <font size=4>$2∗CW$</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font style=\"font-size:110%\">Default complex input `numerical` evaluation</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the elementary case where <font size=3>$N=M=1$</font> first. We know from (chapter 3 of) this research paper that:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color=maroon>$$CW:=\\frac{∂y}{∂z^*}=\\frac{1}{2}(\\frac{∂y}{∂a}+i\\frac{∂y}{∂b})$$</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=maroon size=3><b>Note that</b></font> <font size=5>$\\frac{\\partial y}{\\partial a}$</font> and <font size=5>$\\frac{\\partial y}{\\partial b}$</font>, in the above equation, are <font size=4>$\\mathcal{R} \\to \\mathcal{R}$</font> derivatives. To evaluate these numerically, we use the method described above for the real-to-real case. <font size=3 color=maroon>This allows us to compute the $CW$ matrix and then multiply it by $2$.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the code, as of time of writing, computes this value in a slightly convoluted way:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Code from https://github.com/pytorch/pytorch/blob/58eb23378f2a376565a66ac32c93a316c45b6131/torch/autograd/gradcheck.py#L99-L105\n",
    "# Notation changes in this code block:\n",
    "# s here is y above\n",
    "# x, y here are a, b above\n",
    "\n",
    "ds_dx = compute_gradient(eps)\n",
    "ds_dy = compute_gradient(eps * 1j)\n",
    "# conjugate wirtinger derivative\n",
    "conj_w_d = 0.5 * (ds_dx + ds_dy * 1j)\n",
    "# wirtinger derivative\n",
    "w_d = 0.5 * (ds_dx - ds_dy * 1j)\n",
    "d[d_idx] = grad_out.conjugate() * conj_w_d + grad_out * w_d.conj()\n",
    "\n",
    "# Since grad_out is always 1, and W and CW are complex conjugate of each other, the last line ends up computing exactly \n",
    "# `conj_w_d + w_d.conj() = conj_w_d + conj_w_d = 2 * conj_w_d`.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font style=\"font-size:110%\">Default complex input `analytical` evaluation</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since backward mode AD computes exactly twice the <font size=4>$CW$</font> derivative already, we simply use the same trick as for the real-to-real case here and reconstruct the matrix row by row when there are multiple real outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:maroon;font-size:110%\">Functions with <b>complex outputs</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the user-provided function does not follow the assumption from the autograd that the function we compute backward AD for is real-valued. This means that using autograd directly on this function is not well defined. \n",
    "\n",
    "\n",
    "To solve this, we will replace the test of the function <font size=4>$h: \\mathcal{P}^N \\to \\mathcal{C}^M$</font> (where <font size=4>$\\mathcal{P}$</font> can be either <font size=4>$\\mathcal{R}$</font> or <font size=4>$\\mathcal{C}$</font>), with two functions: <font size=4>$hr$</font> and <font size=4>$hi$</font> such that:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color=maroon>$$hr(q):=real(f(q))$$</font>\n",
    "<br>\n",
    "<font size=5 color=maroon>$$hi(q):=imag(f(q))$$</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where <font size=4>$q \\in \\mathcal{P}$</font>. We then do a basic gradcheck for both <font size=4>$hr$</font> and <font size=4>$hi$</font> using either the real-to-real or complex-to-real case described above, depending on <font size=4>$\\mathcal{P}$</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=maroon size=3><b>Note that,</b></font> the code, as of time of writing, does not create these functions explicitly but perform the chain rule with the <font size=4 color=maroon>$real$</font> or <font size=4 color=maroon>$imag$</font> functions manually by passing the <font size=3 color=maroon>$\\text{grad_out}$</font> arguments to the different functions. \n",
    "\n",
    "* When <font size=3>$\\text{grad_out} = 1$</font>, then we are considering <font size=3>$hr$</font>. \n",
    "\n",
    "\n",
    "* When <font size=3>$\\text{grad_out} = 1j$</font>, then we are considering <font size=3>$hi$</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:maroon;font-size:110%\"><b>Fast</b> backward mode gradcheck</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the above formulation of gradcheck is great, both, to ensure `correctness and debuggability`, it is **very slow** `because it reconstructs the full Jacobian matrices`. \n",
    "\n",
    "This section presents a way to perform gradcheck in a faster way without affecting its correctness. The debuggability can be recovered by adding special logic when we detect an error. In that case, we can run the default version that reconstructs the full matrix to give full details to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The high level strategy here is to find a scalar quantity that can be computed efficiently by both the numerical and analytical methods and that represents the full matrix computed by the slow gradcheck well enough to ensure that it will catch any discrepancy in the Jacobians."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:red;font-size:110%\"><b>Fast gradcheck</b> `for real-to-real functions`</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scalar quantity that we want to compute here is <font size=4>$v^T J_f u$</font> for a given random vector <font size=4>$v \\in \\mathcal{R}^M$</font> and a random unit norm vector <font size=4>$u \\in \\mathcal{R}^N$</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `For the `**`numerical`**` evaluation`, we can efficiently compute <br><br><font size=4>$$J_fu≈\\frac{f(x+u∗eps)−f(x−u∗eps)}{2∗eps}$$</font>\n",
    "<br>\n",
    "We then perform the dot product between this vector and <font size=4>$v$</font> to get the scalar value of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `For the `**`analytical`**` version`, we can use backward mode AD to compute <font size=4>$v^T J_f$</font> directly. We then perform the dot product with <font size=4>$u$</font> to get the expected value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:red;font-size:110%\"><b>Fast gradcheck</b> `for complex-to-real  functions`</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the real-to-real case, we want to `perform a `**`reduction`**` of the full matrix`. But the <font size=3>$2∗CW$</font> matrix is complex-valued and so in this case, we will compare to complex scalars.\n",
    "\n",
    "\n",
    "Due to some constraints on what we can compute efficiently in the numerical case and to keep the number of numerical evaluations to a minimum, we compute the following (albeit surprising) scalar value:\n",
    "\n",
    "<font size=5 color=maroon>$$s:=2∗v^T (real(CW)ur+i∗imag(CW)ui)$$</font>\n",
    "\n",
    "where <font size=3>$v \\in \\mathcal{R}^M$</font>, <font size=3>$ur \\in \\mathcal{R}^N$</font> and <font size=3>$ui \\in \\mathcal{R}^N$</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font style=\"font-size:110%\">Fast complex input `numerical` evaluation</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first consider how to compute <font size=4>$s$</font> with a numerical method. To do so, keeping in mind that we’re considering <font size=3>$g: \\mathcal{C}^N \\to \\mathcal{R}^M, z \\to y$</font> with <font size=3>$z=a+ib$</font>, and that <font size=3 color=red>$CW = \\frac{1}{2} * (\\frac{\\partial y}{\\partial a} + i \\frac{\\partial y}{\\partial b})$</font>, we rewrite it as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color=maroon>\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "s &=2∗v^T (real(CW)ur+i∗imag(CW)ui) \\\\\n",
    "  &=2∗v^T (\\frac{1}{2}∗\\frac{∂y}{∂a}ur+i∗\\frac{1}{2}∗\\frac{∂y}{∂b}ui) \\\\\n",
    "  &=v^T(\\frac{∂y}{∂a}ur+i∗\\frac{∂y}{∂b}ui) \\\\\n",
    "  &=v^T((\\frac{∂y}{∂a}ur)+i∗(\\frac{∂y}{∂b}ui))\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this formula, we can see that <font size=4>$\\frac{\\partial y}{\\partial a} ur$</font> and <font size=4>$\\frac{\\partial y}{\\partial b} ui$</font> can be evaluated the same way as the fast version for the real-to-real case. Once these real-valued quantities have been computed, we can reconstruct the complex vector on the right side and do a dot product with the real-valued <font size=4>$v$</font> vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br<"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font style=\"font-size:110%\">Fast complex input `analytical ` evaluation</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the analytical case, things are simpler and we rewrite the formula as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color=maroon>\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "s &=2∗v^T (real(CW)ur+i∗imag(CW)ui) \\\\\n",
    "  &=v^T real(2∗CW)ur+i∗v^T imag(2∗CW)ui) \\\\\n",
    "  &=real(v^T (2∗CW))ur+i∗imag(v^T (2∗CW))ui\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can thus use the fact that the backward mode AD provides us with an efficient way to compute <font size=3>$v^T (2 * CW)$</font> and then perform a dot product of the real part with <font size=4>$ur$</font> and the imaginary part with <font size=4>$ui$</font> before reconstructing the final complex scalar <font size=4>$s$</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font style=\"color:red;font-size:110%\">Why not use a complex ***u***</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you might be wondering why we did not select a complex <font size=4>$u$</font> and just performed the reduction <font size=4>$2 * v^T CW u'$</font>. To dive into this, in this paragraph, we will use the complex version of <font size=4>$u$</font> noted <font size=4>$u' = ur' + i ui'$</font>. Using such complex <font size=4>$u'$</font>, the problem is that when doing the numerical evaluation, we would need to compute:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "2∗CWu′ &= (\\frac{∂y}{∂a}+i \\frac{∂y}{∂b})(ur′+iui′) \\\\\n",
    "       &= \\frac{∂y}{∂a}ur′ +i\\frac{∂y}{∂a}ui′ +i\\frac{∂y}{∂b}ur′ − \\frac{∂y}{∂b}ui′\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which would require four evaluations of real-to-real finite difference (twice as much compared to the approached proposed above). Since this approach does not have more degrees of freedom (same number of real valued variables) and we try to get the fastest possible evaluation here, we use the other formulation above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fast gradcheck for functions with complex outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like in the slow case, we consider two real-valued functions and use the appropriate rule from above for each function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradgradcheck implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch also provide a utility to verify second order gradients. The goal here is to make sure that the backward implementation is also properly differentiable and computes the right thing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This feature is implemented by considering the function <font size=4 color=maroon>$F: x, v \\to v^T J_f$</font> and use the gradcheck defined above on this function. <font size=3 color=maroon><b>Note that</b></font> <font size=4>$v$</font> in this case is just a random vector with the same type as <font size=4>$f(x)$</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fast version of gradgradcheck is implemented by using the fast version of gradcheck on that same function <font size=4>$F$</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=gray size=3>Docs > Notes2 </font>\n",
    "\n",
    "# <font style=\"font-size:120%;color:maroon;font-weight:bold\">Autograd mechanics</font> <a href=\"https://pytorch.org/docs/stable/notes/autograd.html\" style=\"text-decoration:none;\"><font size=2>[link]</font></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3 color=maroon>This note will present an overview of how autograd works and records the operations. It’s not strictly necessary to understand all this, but we recommend getting familiar with it, as it will help you write more efficient, cleaner programs, and can aid you in debugging.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How autograd encodes the history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3 color=blue><b>Autograd</b></font>` is reverse automatic differentiation system.` Conceptually, autograd records a graph recording all of the operations that created the data as you execute operations, giving you a directed acyclic graph whose ***leaves*** are the input tensors and ***roots*** are the output tensors. By tracing this graph from roots to leaves, you can automatically compute the gradients using the chain rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Internally, autograd represents this graph as a graph of `Function` objects (really expressions), which can be `apply()` ed to compute the result of evaluating the graph. When computing the forwards pass, autograd simultaneously performs the requested computations and builds up a graph representing the function that computes the gradient (the `.grad_fn` attribute of each **`torch.Tensor`** is an entry point into this graph). <font color=maroon size=3>When the forwards pass is completed, we evaluate this graph in the backwards pass to compute the gradients.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important thing to note is that <font size=3 color=maroon><b>the graph is recreated from scratch at every iteration</b>, and this is exactly what allows for using arbitrary Python control flow statements, that can change the overall shape and size of the graph at every iteration. You don’t have to encode all possible paths before you launch the training - what you run is what you differentiate.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saved tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some operations need intermediary results to be saved during the forward pass in order to execute the backward pass. For example, the function <font size=3>$x↦x^2$</font> saves the input xx to compute the gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When defining a custom Python <a href=\"https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function\" style=\"text-decoration:none;\"><b>Function</b></a>, you can use `save_for_backward()` to save tensors during the forward pass and `saved_tensors` to retrieve them during the backward pass. See <a href=\"https://pytorch.org/docs/stable/notes/extending.html\" style=\"text-decoration:none;\"><b>Extending PyTorch</b></a> for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For operations that PyTorch defines (e.g. <a href=\"https://pytorch.org/docs/stable/generated/torch.pow.html#torch.pow\" style=\"text-decoration:none;\"><b>torch.pow()</b></a>), tensors are automatically saved as needed. You can explore (for educational or debugging purposes) which tensors are saved by a certain `grad_fn` by looking for its attributes starting with the prefix `_saved`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5, requires_grad=True)\n",
    "y = x.pow(2)\n",
    "\n",
    "print(x.equal(y.grad_fn._saved_self))  # True\n",
    "print(x is y.grad_fn._saved_self)      # True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous code, **`y.grad_fn._saved_self`** refers to the same Tensor object as x. But that may not always be the case. For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5, requires_grad=True)\n",
    "y = x.exp()\n",
    "\n",
    "print(y.equal(y.grad_fn._saved_result))  # True\n",
    "print(y is y.grad_fn._saved_result)      # False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the hood, to prevent reference cycles, PyTorch has packed the tensor upon saving and unpacked it into a different tensor for reading. Here, the tensor you get from accessing `y.grad_fn._saved_result` is a different tensor object than x (but they still share the same storage)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whether a tensor will be packed into a different tensor object depends on whether it is an output of its own ***grad_fn***, which is an implementation detail subject to change and that users should not rely on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can control how PyTorch does packing / unpacking with (section below) <a href=\"https://pytorch.org/docs/stable/notes/autograd.html#saved-tensors-hooks-doc\" style=\"text-decoration:none;\"><font size=3>Hooks for saved tensors</font></a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradients for non-differentiable functions \n",
    "\n",
    "<a href=\"https://pytorch.org/docs/stable/notes/autograd.html#gradients-for-non-differentiable-functions\" style=\"text-decoration:none\">暂略 [link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locally disabling gradient computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several mechanisms available from Python to locally disable gradient computation:\n",
    "\n",
    "To disable gradients across entire blocks of code, there are `context managers like `<font size=4 color=maroon><b>no-grad mode</b></font>` and `<font size=4 color=maroon><b>inference mode</b></font>. For more fine-grained exclusion of subgraphs from gradient computation, there is setting the **`requires_grad`**` field` of a tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, in addition to discussing the mechanisms above, we also describe <font size=4 color=maroon><b>evaluation mode</b></font> (`nn.Module.eval()`), <font size=4 color=maroon><b>a method that</b></font> is not actually used to disable gradient computation but, because of its name, is often mixed up with the three."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting `requires_grad`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>***requires_grad*** is a flag, defaulting to **false** unless wrapped in a `nn.Parameter`, that allows for fine-grained exclusion of subgraphs from gradient computation. It takes effect in both the forward and backward passes:\n",
    "\n",
    "* During the forward pass, an operation is only recorded in the backward graph if at least one of its input tensors require grad. \n",
    "\n",
    "    \n",
    "* During the backward pass (`.backward()`), only leaf tensors with ***requires_grad=True*** will have gradients accumulated into their `.grad` fields.\n",
    "</font>\n",
    "\n",
    "\n",
    "\n",
    "<font size=3><font color=maroon><b>It is important to note that</b></font> even though every tensor has this flag, setting it only makes sense for <font color=maroon>leaf tensors</font> (tensors that do not have a ***grad_fn***, e.g., a **nn.Module**’s parameters). <font color=maroon>Non-leaf tensors</font> (tensors that do have ***grad_fn***) are tensors that have a backward graph associated with them. Thus their gradients will be needed as an intermediary result to compute the gradient for a leaf tensor that requires grad. <font color=maroon> From this definition, it is clear that all non-leaf tensors will automatically have **`require_grad=True`**.</font>\n",
    "\n",
    "\n",
    "<font color=maroon>Setting **requires_grad** should be the main way you control which parts of the model are part of the gradient computation,</font> for example, if you need to freeze parts of your pretrained model during model fine-tuning.\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>To freeze parts of your model, simply apply `.requires_grad_(False)` to the parameters that you don’t want updated. And as described above, since computations that use these parameters as inputs would not be recorded in the forward pass, they won’t have their `.grad` fields updated in the backward pass because they won’t be part of the backward graph in the first place, as desired.    </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>Because this is such a common pattern, `requires_grad` can also be set at the module level with `nn.Module.requires_grad_()`. When applied to a module, `.requires_grad_()` takes effect on all of the module’s parameters (which have `requires_grad=True` by default).</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grad Modes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from setting ***requires_grad*** there are also three possible modes enableable from Python that can affect how computations in PyTorch are processed by autograd internally: \n",
    "* default mode (grad mode), \n",
    "* no-grad mode, \n",
    "* and inference mode, \n",
    "\n",
    "all of which can be togglable via <font color=maroon><b>context managers</b></font> and <font color=maroon><b>decorators</b></font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font style=\"color:blue;font-size:120%;font-weight:bold\">Default Mode (Grad Mode)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The “default mode” is actually the mode we are implicitly in when no other modes like no-grad and inference mode are enabled. To be contrasted with “no-grad mode” the default mode is also sometimes called “grad mode”.\n",
    "\n",
    "<font color=maroon><b>The most important thing</b></font> to know about the default mode is that <font size=3 color=maroon>it is the only mode in which **`requires_grad`** takes effect.</font> **`requires_grad`** is always overridden to be **`False`** in both the two other modes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font style=\"color:blue;font-size:120%;font-weight:bold\">No-grad Mode</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computations in no-grad mode <font color=maroon>behave as if none of the inputs require grad.</font> In other words, computations in no-grad mode are <font color=maroon>never recorded in the backward graph even if there are inputs that have ***require_grad=True***.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enable no-grad mode when you need to perform operations that should not be recorded by autograd, but you’d still like to use the outputs of these computations in grad mode later. This context manager makes it convenient to disable gradients for a block of code or function without having to temporarily set tensors to have *requires_grad=False*, and then back to *True*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, no-grad mode might be useful when writing an optimizer: when performing the training update you’d like to update parameters in-place without the update being recorded by autograd. You also intend to use the updated parameters for computations in grad mode in the next forward pass.\n",
    "\n",
    "The implementations in <a href=\"https://pytorch.org/docs/stable/nn.init.html#nn-init-doc\" style=\"text-decoration:none;\">torch.nn.init</a> also rely on no-grad mode when initializing the parameters as to avoid autograd tracking when updating the intialized parameters in-place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font style=\"color:blue;font-size:120%;font-weight:bold\">Inference Mode</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference mode is the extreme version of no-grad mode. Just like in no-grad mode, computations in inference mode are not recorded in the backward graph, but enabling inference mode will allow PyTorch to speed up your model even more. This better runtime comes with a drawback: tensors created in inference mode will not be able to be used in computations to be recorded by autograd after exiting inference mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enable inference mode when you are performing computations that don’t need to be recorded in the backward graph, AND you don’t plan on using the tensors created in inference mode in any computation that is to be recorded by autograd later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=maroon size=3><b>It is recommended that</b> you try out inference mode in the parts of your code that do not require autograd tracking (e.g., <b>data processing</b> and <b>model evaluation</b>).</font> If it works out of the box for your use case it’s a free performance win. If you run into errors after enabling inference mode, check that you are not using tensors created in inference mode in computations that are recorded by autograd after exiting inference mode. If you cannot avoid such use in your case, you can always switch back to no-grad mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For details on inference mode please see <a href=\"https://pytorch.org/cppdocs/notes/inference_mode.html\" style=\"text-decoration:none;\"><font size=4>Inference Mode</font></a>.\n",
    "\n",
    "For implementation details of inference mode see <a href=\"https://github.com/pytorch/rfcs/pull/17\" style=\"text-decoration:none;\"><font size=4>RFC-0011-InferenceMode</font></a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font style=\"color:blue;font-size:120%;font-weight:bold\">Evaluation Mode (`nn.Module.eval()`)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation mode is not actually a mechanism to locally disable gradient computation. It is included here anyway because it is sometimes confused to be such a mechanism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functionally, **`module.eval()`** (or equivalently **`module.train()`**) are completely orthogonal to no-grad mode and inference mode. How `model.eval()` affects your model depends entirely on the specific modules used in your model and whether they define any training-mode specific behavior.\n",
    "\n",
    "<font size=3>You are responsible for calling `model.eval()` and `model.train()` if your model relies on modules such as <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#torch.nn.Dropout\" style=\"text-decoration:none;\"><font size=4>torch.nn.Dropout</font></a> and <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html#torch.nn.BatchNorm2d\" style=\"text-decoration:none;\"><font size=4>torch.nn.BatchNorm2d</font></a> that may behave differently depending on training mode, for example, to avoid updating your BatchNorm running statistics on validation data.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=maroon><b>It is recommended that</b> you always use `model.train()` when training and `model.eval()` when evaluating your model (validation/testing)</font> even if you aren’t sure your model has training-mode specific behavior, because a module you are using might be updated to behave differently in training and eval modes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-place operations with autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supporting in-place operations in autograd is a hard matter, and we discourage their use in most cases. Autograd’s aggressive buffer freeing and reuse makes it very efficient and there are very few occasions when in-place operations actually lower memory usage by any significant amount. Unless you’re operating under heavy memory pressure, you might never need to use them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two main reasons that limit the applicability of in-place operations:\n",
    "\n",
    "* In-place operations can potentially overwrite values required to compute gradients.\n",
    "\n",
    "* Every in-place operation actually requires the implementation to rewrite the computational graph. Out-of-place versions simply allocate new objects and keep references to the old graph, while in-place operations, require changing the creator of all inputs to the `Function` representing this operation. This can be tricky, especially if there are many Tensors that reference the same storage (e.g. created by indexing or transposing), and in-place functions will actually raise an error if the storage of modified inputs is referenced by any other `Tensor`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-place correctness checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=maroon>Every tensor keeps a version counter, that is incremented every time it is marked dirty in any operation.</font> When a Function saves any tensors for backward, a version counter of their containing Tensor is saved as well. Once you access `self.saved_tensors` it is checked, and if it is greater than the saved value an error is raised. This ensures that if you’re using in-place functions and not seeing any errors, you can be sure that the computed gradients are correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multithreaded Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The autograd engine is responsible for running all the backward operations necessary to compute the backward pass. This section will describe all the details that can help you make the best use of it in a `multithreaded environment`.(this is relevant only for PyTorch 1.6+ as the behavior in previous version was different).\n",
    "\n",
    "User could train their model with `multithreading code` (e.g. `Hogwild training`), and does not block on the concurrent backward computations, example code could be:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Define a train function to be used in different threads\n",
    "def train_fn():\n",
    "    x = torch.ones(5, 5, requires_grad=True)\n",
    "    # forward\n",
    "    y = (x + 3) * (x + 4) * 0.5\n",
    "    # backward\n",
    "    y.sum().backward()\n",
    "    # potential optimizer update\n",
    "\n",
    "\n",
    "# User write their own threading code to drive the train_fn\n",
    "threads = []\n",
    "for _ in range(10):\n",
    "    p = threading.Thread(target=train_fn, args=())\n",
    "    p.start()\n",
    "    threads.append(p)\n",
    "\n",
    "for p in threads:\n",
    "    p.join()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>Note that some behaviors that user should be aware of:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concurrency on CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you run `backward()` or `grad()` via python or C++ API in multiple threads on CPU, you are expecting to see extra concurrency instead of serializing all the backward calls in a specific order during execution (behavior before PyTorch 1.6)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-determinism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are calling `backward()` on multiple thread concurrently but with shared inputs (i.e. Hogwild CPU training). Since parameters are automatically shared across threads, gradient accumulation might become non-deterministic on backward calls across threads, because two backward calls might access and try to accumulate the same `.grad` attribute. This is technically not safe, and it might result in racing condition and the result might be invalid to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this is expected pattern if you are using the multithreading approach to drive the whole training process but using shared parameters, user who use multithreading should have the threading model in mind and should expect this to happen. User could use the functional API <a href=\"https://pytorch.org/docs/stable/generated/torch.autograd.grad.html#torch.autograd.grad\" style=\"text-decoration:none;\"><b>torch.autograd.grad()</b></a> to calculate the gradients instead of `backward()` to avoid non-determinism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph retaining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If part of the autograd graph is shared between threads, i.e. run first part of forward single thread, then run second part in multiple threads, then the first part of graph is shared. In this case different threads execute `grad()` or `backward()` on the same graph might have issue of destroying the graph on the fly of one thread, and the other thread will crash in this case. Autograd will error out to the user similar to what call `backward()` twice without `retain_graph=True`, and let the user know they should use `retain_graph=True`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thread Safety on Autograd Node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Autograd allows the caller thread to drive its backward execution for potential parallelism, it’s important that we ensure thread safety on CPU with parallel backwards that share part/whole of the GraphTask.\n",
    "\n",
    "Custom Python `autograd.function` is automatically thread safe because of GIL. for built-in C++ Autograd Nodes(e.g. AccumulateGrad, CopySlices) and custom `autograd::Function`, the Autograd Engine uses thread mutex locking to protect thread safety on autograd Nodes that might have state write/read."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No thread safety on C++ hooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autograd relies on the user to write thread safe C++ hooks. If you want the hook to be correctly applied in multithreading environment, you will need to write proper thread locking code to ensure the hooks are thread safe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd for Complex Numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The short version:\n",
    "\n",
    "* When you use PyTorch to differentiate any function <font size=3><b>$f(z)$</b></font> with complex domain and/or codomain, the gradients are computed under the assumption that the function is a part of a larger real-valued loss function <font size=3><b>$g(input)=L$</b></font>. The gradient computed is <font size=5><b>$\\frac{\\partial L}{\\partial z^*}$</b></font> (note the conjugation of z), the negative of which is precisely the direction of steepest descent used in Gradient Descent algorithm. Thus, all the existing optimizers work out of the box with complex parameters.\n",
    "\n",
    "\n",
    "* This convention matches TensorFlow’s convention for complex differentiation, but is different from JAX (which computes <font size=5><b>$\\frac{\\partial L}{\\partial z}$</b></font>.\n",
    "\n",
    "\n",
    "* If you have a real-to-real function which internally uses complex operations, the convention here doesn’t matter: you will always get the same result that you would have gotten if it had been implemented with only real operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are curious about the mathematical details, or want to know how to define complex derivatives in PyTorch, read on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are complex derivatives?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mathematical definition of complex-differentiability takes the limit definition of a derivative and generalizes it to operate on complex numbers. Consider a function <font size=4><b>$f: ℂ → ℂ$</b></font>:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5>$$f(z=x+yj)=u(x,y)+v(x,y)j$$</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where <font size=4><b>$u$</b></font> and <font size=4><b>$v$</b></font> are two variable real valued functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the derivative definition, we can write:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5>$$f′(z)=\\lim_{h→0,h∈C}\\frac{f(z+h)−f(z)}{h}$$</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order for this limit to exist, not only must <font size=4><b>$u$</b></font> and <font size=4><b>$v$</b></font> must be real differentiable, but ff must also satisfy the <a href=\"https://en.wikipedia.org/wiki/Cauchy%E2%80%93Riemann_equations\" style=\"text-decoration:none;\"><font size=3>Cauchy-Riemann equations</font></a>. In other words: the limit computed for real and imaginary steps (<font size=4><b>$h$</b></font>) must be equal. This is a more restrictive condition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=royalblue>The complex differentiable functions are commonly known as **holomorphic functions**.</font> They are well behaved, have all the nice properties that you’ve seen from real differentiable functions, but are practically of no use in the optimization world. For optimization problems, only real valued objective functions are used in the research community since complex numbers are not part of any ordered field and so having complex valued loss does not make much sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=maroon>It also turns out that no interesting real-valued objective fulfill the Cauchy-Riemann equations. So the theory with homomorphic function cannot be used for optimization and most people therefore use the **Wirtinger calculus**.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wirtinger Calculus comes in picture …"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=maroon>So, we have this great theory of `complex differentiability` and `holomorphic functions`, and we can’t use any of it at all, because many of the commonly used functions are not holomorphic. \n",
    "    \n",
    "What’s a poor mathematician to do? Well, Wirtinger observed that even if <font size=4><b>$f(z)$</b></font> isn’t holomorphic, one could rewrite it as a two variable function <font size=4><b>$f(z, z*)$</b></font> which is always holomorphic.</font>\n",
    "\n",
    "This is because real and imaginary of the components of <font size=4><b>$z$</b></font> can be expressed in terms of <font size=4><b>$z$</b></font> and <font size=4><b>$z^*$</b></font> as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>$$Re(z)=\\frac{z+z^*}{2}$$</font>\n",
    "\n",
    "\n",
    "<font size=4>$$Im(z)=\\frac{z-z^*}{2j}$$</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=maroon size=3>Wirtinger calculus suggests to study <font size=4><b>$f(z, z^*)$</b></font> instead, which is guaranteed to be holomorphic if <font size=4><b>$f$</b></font> was real differentiable (another way to think of it is as a change of coordinate system, from <font size=4><b>$f(x, y)$</b></font> to <font size=4><b>$f(z, z^*)$</b></font>.)</font> This function has partial derivatives<font size=5><b>$\\frac{\\partial }{\\partial z}$</b></font> and <font size=5><b>$\\frac{\\partial}{\\partial z^{*}}$</b></font>. We can use the **chain rule** to establish a relationship between these partial derivatives and the partial derivatives w.r.t., the real and imaginary components of <font size=4><b>$z$</b></font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\frac{∂}{∂x} &= \\frac{∂z}{∂x}*\\frac{∂}{∂z} + \\frac{∂z^*}{∂x}*\\frac{∂}{∂z^*} \\\\\n",
    "             &= \\frac{∂}{∂z} + \\frac{∂}{∂z^*}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "</font>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\frac{∂}{∂y} &= \\frac{∂z}{∂y}*\\frac{∂}{∂z} + \\frac{∂z^*}{∂y}*\\frac{∂}{∂z^*} \\\\\n",
    "             &= 1j*(\\frac{∂}{∂z} - \\frac{∂}{∂z^*})\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above equations, we get:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>$$\\frac{∂}{∂z}=1/2*(\\frac{∂}{∂x}-1j*\\frac{∂}{∂y})$$</font>\n",
    "\n",
    "\n",
    "<font size=4>$$\\frac{∂}{∂z^*}=1/2*(\\frac{∂}{∂x}+1j*\\frac{∂}{∂y})$$</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is the classic definition of Wirtinger calculus that you would find on <a href=\"https://en.wikipedia.org/wiki/Wirtinger_derivatives\" style=\"text-decoration:none;\"><b>Wikipedia</b></a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of beautiful consequences of this change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For one, the `Cauchy-Riemann equations` translate into simply saying that <font size=4>$\\frac{\\partial f}{\\partial z^*} = 0$</font> (that is to say, the function <font size=4>$f$</font> can be written entirely in terms of <font size=4>$z$</font>, without making reference to <font size=4>$z^*$</font>).\n",
    "\n",
    "\n",
    "* Another important (and somewhat counterintuitive) result, as we’ll see later, is that when we do optimization on a real-valued loss, the step we should take while making variable update is given by <font size=4>$\\frac{\\partial Loss}{\\partial z^*}$</font> (not <font size=4>$\\frac{\\partial Loss}{\\partial z}$</font>)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more reading, check out: <a href=\"https://arxiv.org/pdf/0906.4835.pdf\" style=\"text-decoration:none;\"><b>https://arxiv.org/pdf/0906.4835.pdf</b></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How is Wirtinger Calculus useful in optimization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3 color=maroon>Researchers in audio and other fields, more commonly, use `gradient descent` to optimize real valued loss functions with complex variables.</font> Typically, these people treat the real and imaginary values as separate channels that can be updated. For a step size <font size=4>$α/2$</font> and loss <font size=4>$L$</font>, we can write the following equations in <font size=4>$ℝ^2$</font>:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>$$x_{n+1}=x_n-\\alpha/2*\\frac{∂L}{∂x}$$</font>\n",
    "\n",
    "<font size=4>$$y_{n+1}=y_n-\\alpha/2*\\frac{∂L}{∂y}$$</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do these equations translate into complex space <font size=4>$ℂ$</font>?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "z_{n+1}&=x_n-(\\alpha/2)*\\frac{∂L}{∂x}+1j*(y_n-(\\alpha/2)*\\frac{∂L}{∂y}) \\\\\n",
    "       &=z_n-\\alpha * 1/2 * (\\frac{∂L}{∂x}+j\\frac{∂L}{∂y}) \\\\\n",
    "       &=z_n-\\alpha * \\frac{∂L}{∂z^*}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "    \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>Something very interesting has happened: <font color=maroon>Wirtinger calculus tells us that we can simplify the complex variable update formula above to only refer to the conjugate Wirtinger derivative <font size=5>$\\frac{\\partial L}{\\partial z^*} $</font>, giving us exactly the step we take in optimization.</font></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3 color=maroon>Because the conjugate Wirtinger derivative gives us exactly the correct step for a real valued loss function, PyTorch gives you this derivative when you differentiate a function with a real valued loss.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:red;font-size:110%;\">How does PyTorch compute the <b>conjugate Wirtinger derivative</b>?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, our derivative formulas take in grad_output as an input, representing the incoming Vector-Jacobian product that we’ve already computed, aka, <font size=5>$\\frac{\\partial L}{\\partial s^*}$</font>, where <font size=4>$L$</font> is the loss of the entire computation (producing a real loss) and <font size=4>$s$</font> is the output of our function. The goal here is to compute <font size=5>$\\frac{\\partial L}{\\partial z^*}$</font>, where <font size=4>$z$</font> is the input of the function. \n",
    " \n",
    "\n",
    "\n",
    "It turns out that in the case of real loss, we can get away with only calculating <font size=5>$\\frac{\\partial L}{\\partial z^*}$</font>, even though the chain rule implies that we also need to have access to <font size=5>$\\frac{\\partial L}{\\partial z^*}$</font>. If you want to skip this derivation, look at the last equation in this section and then skip to the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s continue working with <font size=4>$f: ℂ → ℂ$</font> defined as <font size=4>$f(z) = f(x+yj) = u(x, y) + v(x, y)j$</font>. As discussed above, autograd’s gradient convention is centered around optimization for real valued loss functions, so let’s assume <font size=4>$f$</font> is a part of larger real valued loss function <font size=4>$g$</font>. Using chain rule, we can write:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>$$\\frac{∂L}{∂z^*}=\\frac{∂L}{∂u}*\\frac{∂u}{∂z^*}+\\frac{∂L}{∂v}*\\frac{∂v}{∂z^*} \\ \\ \\ \\ \\ \\ \\ \\ \\ (1)$$</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now using Wirtinger derivative definition, we can write:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>$$\\frac{∂L}{∂s}=1/2*(\\frac{∂L}{∂u}-\\frac{∂L}{∂v}j)$$</font>\n",
    "\n",
    "<font size=4>$$\\frac{∂L}{∂s^*}=1/2*(\\frac{∂L}{∂u}+\\frac{∂L}{∂v}j)$$</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be noted here that since uu and vv are real functions, and LL is real by our assumption that ff is a part of a real valued function, we have:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>$$(\\frac{∂L}{∂s})^*=\\frac{∂L}{∂s^*} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (2)$$</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i.e., <font size=4>$\\frac{\\partial L}{\\partial s} $</font> equals to <font size=4 color=blue>$grad\\_output^*$</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solving the above equations for <font size=4>$\\frac{\\partial L}{\\partial u}$</font> and <font size=4>$\\frac{\\partial L}{\\partial v}$</font>, we get:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>$$\\frac{∂L}{∂u}=\\frac{∂L}{∂s}+\\frac{∂L}{∂s^*}$$</font>\n",
    "<font size=4>$$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (3)$$</font>\n",
    "<font size=4>$$\\frac{∂L}{∂v}=-1j*(\\frac{∂L}{∂s}-\\frac{∂L}{∂s^*})$$</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Substituting <font size=3 color=red>(3)</font> in <font size=3 color=red>(1)</font>, we get:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\frac{∂L}{∂z^*}&=(\\frac{∂L}{∂s} + \\frac{∂L}{∂s^*})*\\frac{∂u}{∂z^*}-1j*(\\frac{∂L}{∂s}-\\frac{∂L}{∂s^*})*\\frac{∂v}{∂z^*} \\\\ \\\\\n",
    "         &=\\frac{∂L}{∂s}*(\\frac{∂u}{∂z^*}+\\frac{∂v}{∂z^*}j)+\\frac{∂L}{∂s^*}*(\\frac{∂u}{∂z^*}-\\frac{∂v}{∂z^*}j)  \\\\ \\\\\n",
    "         &=\\frac{∂L}{∂s^*}*\\frac{∂(u+vj)}{∂z^*}+\\frac{∂L}{∂s}*\\frac{∂(u+vj)^*}{∂z^*} \\\\ \\\\\n",
    "         &=\\frac{∂L}{∂s}*\\frac{∂s}{∂z^*}+\\frac{∂L}{∂s^*}*\\frac{∂s^*}{∂z^*}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using <font size=3 color=red>(2)</font>, we get:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>$$\\frac{∂L}{∂z^*}=(\\frac{∂L}{∂s^*})^**\\frac{∂s}{∂z^*}+\\frac{∂L}{∂s^*}*(\\frac{∂s}{∂z})^*$$</font>\n",
    "<div class=\"alert alert-block alert-danger\" style=\"font-size:150%\">\n",
    "$ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ =(𝑔𝑟𝑎𝑑\\_𝑜𝑢𝑡𝑝𝑢𝑡)^**\\frac{∂s}{∂z^*}+ 𝑔𝑟𝑎𝑑\\_𝑜𝑢𝑡𝑝𝑢𝑡 *(\\frac{∂s}{∂z})^*\\ \\ \\ \\ (4)$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red size=5>This last equation is the important one for writing your own gradients, as it decomposes our derivative formula into a simpler one that is easy to compute by hand.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font style=\"color:red;font-size:110%\">How can I write my own derivative formula for a complex function?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above boxed equation gives us the general formula for all derivatives on complex functions. However, we still need to compute <font size=5>$\\frac{\\partial s}{\\partial z}$</font> and <font size=5>$\\frac{\\partial s}{\\partial z^*}$</font>. There are two ways you could do this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The first way is to just use the definition of Wirtinger derivatives directly and calculate <font size=5>$\\frac{\\partial s}{\\partial z}$</font> and <font size=5>$\\frac{\\partial s}{\\partial z^*}$</font> by using <font size=5>$\\frac{\\partial s}{\\partial x}$</font> and <font size=5>$\\frac{\\partial s}{\\partial y}$</font> (which you can compute in the normal way).\n",
    "\n",
    "\n",
    "* The second way is to use the change of variables trick and rewrite <font size=4>$f(z)$</font> as a two variable function <font size=4>$f(z, z^*)$</font>, and compute the conjugate Wirtinger derivatives by treating <font size=4>$z$</font> and <font size=4>$z^*$</font> as independent variables. This is often easier; for example, if the function in question is holomorphic, only <font size=4>$z$</font> will be used (and <font size=5>$\\frac{\\partial s}{\\partial z^*}$</font> will be zero)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s consider the function <font size=4>$f(z=x+yj)=c∗z=c∗(x+yj)$</font> as an example, where <font size=4>$c \\in ℝ$</font>.\n",
    "\n",
    "Using the first way to compute the Wirtinger derivatives, we have:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\frac{∂s}{∂z}\n",
    "    &=1/2*(\\frac{∂s}{∂x}-\\frac{∂s}{∂y}j) \\\\\n",
    "    &=1/2*(c-(c*1j)*1j) \\\\\n",
    "    &=c\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\frac{∂s}{∂z^*}\n",
    "    &=1/2*(\\frac{∂s}{∂x}+\\frac{∂s}{∂y}j) \\\\\n",
    "    &=1/2*(c+(c*1j)*1j) \\\\\n",
    "    &=0\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using <font size=3 color=red>(4)</font>, and *`grad_output = 1.0`* (which is the default grad output value used when `backward()` is called on a scalar output in PyTorch), we get:\n",
    "\n",
    "<font size=4>$$\\frac{∂L}{∂z^*}=1*0+1*c=c$$</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the second way to compute Wirtinger derivatives, we directly get:\n",
    "\n",
    "<br>\n",
    "\n",
    "<font size=4>\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\frac{∂s}{∂z}=\\frac{∂(c*z)}{∂z}=c\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "</font>\n",
    "\n",
    "<br>\n",
    "\n",
    "<font size=4>\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\frac{∂s}{∂z^*}=\\frac{∂(c*z)}{∂z^*}=0\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And using <font size=3 color=red>(4)</font> again, we get <font size=4>$\\frac{\\partial L}{\\partial z^*} = c$</font>. As you can see, <font color=red size=5>the second way involves lesser calculations, and comes in more handy for faster calculations.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about cross-domain functions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some functions map from complex inputs to real outputs, or vice versa. These functions form a special case of <font size=3 color=red>(4)</font>, which we can derive using the chain rule:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For <font size=4>$f:ℂ → ℝ$</font>, we get:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\"><font size=5>$$\\frac{∂L}{∂z^*}=2*𝑔𝑟𝑎𝑑\\_𝑜𝑢𝑡𝑝𝑢𝑡*\\frac{∂s}{∂z^*}$$</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For <font size=4>$f: ℝ → ℂ$</font>, we get:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\"><font size=5>$$\\frac{∂L}{∂z^*}=2*Re(𝑔𝑟𝑎𝑑\\_𝑜𝑢𝑡𝑝𝑢𝑡^**\\frac{∂s}{∂z^*})$$</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hooks for saved tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can control <a href=\"https://pytorch.org/docs/stable/notes/autograd.html#saved-tensors-doc\" style=\"text-decoration:none;\">how saved tensors are packed / unpacked</a> by defining a pair of `pack_hook` / `unpack_hook hooks`. \n",
    "\n",
    "* The `pack_hook` function should take a tensor as its single argument but can return any python object (e.g. another tensor, a tuple, or even a string containing a filename). \n",
    "\n",
    "\n",
    "* The `unpack_hook` function takes as its single argument the output of pack_hook and should return a tensor to be used in the backward pass. The tensor returned by unpack_hook only needs to have the same content as the tensor passed as input to pack_hook. In particular, any autograd-related metadata can be ignored as they will be overwritten during unpacking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of such pair is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class SelfDeletingTempFile():\n",
    "    def __init__(self):\n",
    "        self.name = os.path.join(tmp_dir, str(uuid.uuid4()))\n",
    "\n",
    "    def __del__(self):\n",
    "        os.remove(self.name)\n",
    "\n",
    "        \n",
    "def pack_hook(tensor):\n",
    "    temp_file = SelfDeletingTempFile()\n",
    "    torch.save(tensor, temp_file.name)\n",
    "    return temp_file\n",
    "\n",
    "def unpack_hook(temp_file):\n",
    "    return torch.load(temp_file.name)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=maroon>Notice that</font> the `unpack_hook` should not delete the temporary file because it might be called multiple times: the temporary file should be alive for as long as the returned ***`SelfDeletingTempFile`*** object is alive. In the above example, we prevent leaking the temporary file by closing it when it is no longer needed (on deletion of the ***`SelfDeletingTempFile`*** object)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<font size=3 color=red><b>NOTE: </b></font>\n",
    "\n",
    "We guarantee that `pack_hook` will only be called once but `unpack_hook` can be called as many times as the backward pass requires it and we expect it to return the same data each time.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "<font size=3 color=red><b>WARNNING: </b></font>\n",
    "\n",
    "Performing inplace operations on the input of any of the functions is forbidden as they may lead to unexpected side-effects. PyTorch will throw an error if the input to a pack hook is modified inplace but does not catch the case where the input to an unpack hook is modified inplace.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registering hooks for a saved tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can register a pair of hooks on a saved tensor by calling the `register_hooks()` method on a `SavedTensor` object. Those objects are exposed as attributes of a `grad_fn` and start with the `_raw_saved_` prefix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "x = torch.randn(5, requires_grad=True)\n",
    "y = x.pow(2)\n",
    "y.grad_fn._raw_saved_self.register_hooks(pack_hook, unpack_hook)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The `pack_hoo`k method is called as soon as the pair is registered. \n",
    "\n",
    "* The `unpack_hook` method is called each time the saved tensor needs to be accessed, either by means of `y.grad_fn._saved_self` or during the backward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "<font size=3 color=red><b>WARNNING: </b></font>\n",
    "\n",
    "If you maintain a reference to a `SavedTensor` after the saved tensors have been released (i.e. after backward has been called), calling its `register_hooks()` is forbidden. PyTorch will throw an error most of the time but it may fail to do so in some cases and undefined behavior may arise.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registering default hooks for saved tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can use the context-manager <a href=\"https://pytorch.org/docs/stable/autograd.html#torch.autograd.graph.saved_tensors_hooks\" style=\"text-decoration:none;\"><b>saved_tensors_hooks</b></a> to register a pair of hooks which will be applied to ***all*** saved tensors that are created in that context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Only save on disk tensors that have size >= 1000\n",
    "SAVE_ON_DISK_THRESHOLD = 1000\n",
    "\n",
    "def pack_hook(x):\n",
    "    if x.numel() < SAVE_ON_DISK_THRESHOLD:\n",
    "        return x\n",
    "    temp_file = SelfDeletingTempFile()\n",
    "    torch.save(tensor, temp_file.name)\n",
    "    return temp_file\n",
    "\n",
    "def unpack_hook(tensor_or_sctf):\n",
    "    if isinstance(tensor_or_sctf, torch.Tensor):\n",
    "        return tensor_or_sctf\n",
    "    return torch.load(tensor_or_sctf.name)\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def forward(self, x):\n",
    "        with torch.autograd.graph.saved_tensors_hooks(pack_hook, unpack_hook):\n",
    "          # ... compute output\n",
    "          output = x\n",
    "        return output\n",
    "\n",
    "model = Model()\n",
    "net = nn.DataParallel(model)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hooks defined with this context manager are <font size=3 color=maroon><b>thread-local</b></font>. Hence, the following code will not produce the desired effects because the hooks <font size=3 color=maroon>do not go through DataParallel</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Example what NOT to do\n",
    "\n",
    "net = nn.DataParallel(model)\n",
    "with torch.autograd.graph.saved_tensors_hooks(pack_hook, unpack_hook):\n",
    "    output = net(input)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=maroon><b>Note that</b></font> using those hooks disables all the optimization in place to reduce Tensor object creation. For example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "with torch.autograd.graph.saved_tensors_hooks(lambda x: x, lambda x: x):\n",
    "    x = torch.randn(5, requires_grad=True)\n",
    "    y = x * x\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without the hooks, `x`, `y.grad_fn._saved_self` and `y.grad_fn._saved_other` all refer to the same tensor object. With the hooks, PyTorch will pack and unpack x into two new tensor objects that share the same storage with the original x (no copy performed)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Hooks execution \n",
    "\n",
    "<a href=\"https://pytorch.org/docs/stable/notes/autograd.html#backward-hooks-execution\" style=\"text-decoration:none\">暂略 [link]</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=gray size=3>Docs > Automatic differentiation package - torch.autograd ></font>\n",
    "\n",
    "# <font style=\"font-size:120%;color:maroon\"> torch.autograd.backward</font><a href=\"https://pytorch.org/docs/stable/generated/torch.autograd.backward.html\" style=\"text-decoration:none;\"><font size=2>[link]</font></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<font size=4><a href=\"https://pytorch.org/docs/stable/generated/torch.autograd.backward.html\" style=\"text-decoration:none;\">torch.autograd<b>.backward</b></a>(<font color=gray size=3><i>tensors, grad_tensors=None, retain_graph=None, create_graph=False, grad_variables=None, inputs=None</i></font>)</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color=red>Computes the sum of gradients of <b><i>given tensors</i></b> with respect to <b><i>graph leaves</b></i>.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>The graph is differentiated using the ***chain rule***. If any of **`tensors`** are non-scalar (i.e. their data has more than one element) and require gradient, then the ***Jacobian-vector product*** would be computed, in this case the function additionally requires specifying **`grad_tensors`**. It should be a sequence of matching length, that contains the “vector” in the *Jacobian-vector product*, usually the gradient of the differentiated function w.r.t. corresponding tensors (**`None`** is an acceptable value for all tensors that don’t need gradient tensors).</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>This function accumulates gradients in the leaves - you might need to zero **`.grad`** attributes or set them to **`None`** before calling it. See `Docs > Automatic differentiation package - torch.autograd > `<a href=\"https://pytorch.org/docs/stable/autograd.html#default-grad-layouts\" style=\"text-decoration:none;\"><font color=maroon>Default gradient layouts</font></a> for details on the memory layout of accumulated gradients.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters** `(详见 `<a href=\"https://pytorch.org/docs/stable/generated/torch.autograd.backward.html\" style=\"text-decoration:none;\"><font size=2>[link]</font></a>` 链接)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<font size=3 color=red><b>NOTE: </b></font>\n",
    "\n",
    "Using this method with `create_graph=True` will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using `autograd.grad` when creating the graph to avoid this. If you have to use this function, make sure to reset the `.grad` fields of your parameters to `None` after use to break the cycle and avoid the leak.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<font size=3 color=red><b>NOTE: </b></font>\n",
    "\n",
    "If you run any forward ops, create `grad_tensors`, and/or call `backward` in a user-specified CUDA stream context, see <a href=\"https://pytorch.org/docs/stable/notes/cuda.html#bwd-cuda-stream-semantics\" style=\"text-decoration:none;\"><font color=maroon>Stream semantics of backward passes</font></a>.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<font size=3 color=red><b>NOTE: </b></font>\n",
    "\n",
    "When `inputs` are provided and a given input is not a ***leaf***, the current implementation will call its `grad_fn` (even though it is not strictly needed to get this gradients). It is an implementation detail on which the user should not rely. See <a href=\"https://github.com/pytorch/pytorch/pull/60521#issuecomment-867061780\" style=\"text-decoration:none;\"><font color=maroon>https://github.com/pytorch/pytorch/pull/60521#issuecomment-867061780</font></a> for more details.</font></a>.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=gray size=3>Docs > Automatic differentiation package - torch.autograd > </font>\n",
    "\n",
    "# <font style=\"font-size:120%;color:maroon\">torch.autograd.grad</font><a href=\"https://pytorch.org/docs/stable/generated/torch.autograd.grad.html\" style=\"text-decoration:none;\"><font size=2>[link]</font></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<font size=4><a href=\"https://pytorch.org/docs/stable/generated/torch.autograd.grad.html#torch.autograd.grad\" style=\"text-decoration:none;\">torch.autograd<b>.grad</b></a>(<font color=gray size=3><i>outputs, inputs, grad_outputs=None, retain_graph=None, create_graph=False, only_inputs=True, allow_unused=False, is_grads_batched=False</i></font>)</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color=red>Computes and returns the sum of gradients of <b><i>outputs</i></b> with respect to the <b><i>inputs</i></b>.</font>\n",
    "\n",
    "<font size=3>`grad_outputs` should be a sequence of length matching `output` containing the “vector” in ***vector-Jacobian product***, usually the pre-computed gradients w.r.t. each of the outputs. If an output doesn’t `require_grad`, then the gradient can be `None`).</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters** `(详见 `<a href=\"https://pytorch.org/docs/stable/generated/torch.autograd.grad.html\" style=\"text-decoration:none;\"><font size=2>[link]</font></a>` 链接)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<font size=3 color=red><b>NOTE: </b></font>\n",
    "\n",
    "If you run any forward ops, create `grad_outputs`, and/or call `grad` in a user-specified CUDA stream context, see <a href=\"https://pytorch.org/docs/stable/notes/cuda.html#bwd-cuda-stream-semantics\" style=\"text-decoration:none;\"><font color=maroon>Stream semantics of backward passes</font></a>.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<font size=3 color=red><b>NOTE: </b></font>\n",
    "\n",
    "`only_inputs` argument is deprecated and is ignored now (defaults to `True`). To accumulate gradient for other parts of the graph, please use <a href=\"https://pytorch.org/docs/stable/generated/torch.autograd.backward.html\" style=\"text-decoration:none;\"><font color=maroon>torch.autograd.backward</font></a>.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=gray size=3>Docs > </font>\n",
    "\n",
    "# <font style=\"font-size:120%;color:maroon\">Automatic differentiation package - <b>torch.autograd</b></font> <a href=\"https://pytorch.org/docs/stable/autograd.html\" style=\"text-decoration:none;\"><font size=2>[link]</font></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color=maroon><b>torch.autograd</b></font> provides classes and functions implementing automatic differentiation of arbitrary scalar valued functions. It requires minimal changes to the existing code - you only need to declare `Tensor` s for which gradients should be computed with the `requires_grad=True` keyword. As of now, we only support autograd for ***floating point Tensor types*** ( half, float, double and bfloat16) and ***complex Tensor types*** (cfloat, cdouble)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <a href=\"https://pytorch.org/docs/stable/generated/torch.autograd.backward.html#torch.autograd.backward\" style=\"text-decoration:none;\"><font color=maroon size=4><b>backward</b> (torch.autograd.backward)</font></a><br>\n",
    "Computes the sum of gradients of given tensors `with respect to `**`graph leaves`**. (另见本 notebook 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <a href=\"https://pytorch.org/docs/stable/generated/torch.autograd.grad.html#torch.autograd.grad\" style=\"text-decoration:none;\"><font color=maroon size=4><b>grad</b> (torch.autograd.grad)</font></a><br>\n",
    "Computes and returns the sum of gradients of outputs `with respect to `**`the inputs`**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:red;font-size:110%\">Forward-mode Automatic Differentiation</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WARNNING:** (略)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please see the <a href=\"https://pytorch.org/tutorials/intermediate/forward_ad_usage.html\" style=\"text-decoration:none;\"><font size=4>forward-mode AD tutorial</font></a> for detailed steps on how to use this API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <a href=\"https://pytorch.org/docs/stable/generated/torch.autograd.forward_ad.dual_level.html#torch.autograd.forward_ad.dual_level\" style=\"text-decoration:none;\"><font color=maroon size=4>forward_ad.dual_level</font></a><br>\n",
    "Context-manager that enables forward AD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <a href=\"https://pytorch.org/docs/stable/generated/torch.autograd.forward_ad.make_dual.html#torch.autograd.forward_ad.make_dual\" style=\"text-decoration:none;\"><font color=maroon size=4>forward_ad.make_dual</font></a><br>\n",
    "Associates a tensor value with a forward gradient, the tangent, to create a “dual tensor”, which is used to compute forward AD gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <a href=\"https://pytorch.org/docs/stable/generated/torch.autograd.forward_ad.unpack_dual.html#torch.autograd.forward_ad.unpack_dual\" style=\"text-decoration:none;\"><font color=maroon size=4>forward_ad.unpack_dual</font></a><br>\n",
    "Unpacks a “dual tensor” to get both its Tensor value and its forward AD gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functional higher level API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WARNNING:** (略)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section contains the higher level API for the autograd that `builds on the basic API above` and allows you to compute ***jacobians***, ***hessians***, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This API works with **`user-provided functions`**` that take only Tensors as input and return only Tensors.` <font color=maroon>If your function takes other arguments that are not Tensors or Tensors that don’t have `requires_grad` set, you can use a lambda to capture them.</font> \n",
    "\n",
    "For example, for a function `f` that takes three inputs, a Tensor for which we want the jacobian, another tensor that should be considered constant and a boolean flag as `f(input, constant, flag=flag)` you can use it as `functional.jacobian(lambda x: f(x, constant, flag=flag), input)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <a href=\"https://pytorch.org/docs/stable/generated/torch.autograd.functional.jacobian.html#torch.autograd.functional.jacobian\" style=\"text-decoration:none;\"><font color=maroon size=4>functional.jacobian</font></a><br>\n",
    "Function that computes the Jacobian of a given function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <a href=\"https://pytorch.org/docs/stable/generated/torch.autograd.functional.hessian.html#torch.autograd.functional.hessian\" style=\"text-decoration:none;\"><font color=maroon size=4>functional.hessian</font></a><br>\n",
    "Function that computes the Hessian of a given scalar function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* (其它略)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:red;font-size:110%\">Locally disabling gradient computation</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See Docs > Autograd mechanics > <a href=\"https://pytorch.org/docs/stable/notes/autograd.html#locally-disable-grad-doc\" style=\"text-decoration:none;\">Locally disabling gradient computation</a> for more information on the differences between `no-grad` and `inference mode` as well as `other related mechanisms` that may be confused with the two.\n",
    "\n",
    "Also see <a href=\"https://pytorch.org/docs/stable/torch.html#torch-rst-local-disable-grad\" style=\"text-decoration:none\">Locally disabling gradient computation</a> for a list of functions that can be used to locally disable gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <a href=\"https://pytorch.org/docs/stable/generated/torch.autograd.no_grad.html#torch.autograd.no_grad\" style=\"text-decoration:none;\"><font color=maroon size=4>no_grad</font></a><br>\n",
    "Context-manager that disabled gradient calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <a href=\"https://pytorch.org/docs/stable/generated/torch.autograd.enable_grad.html#torch.autograd.enable_grad\" style=\"text-decoration:none;\"><font color=maroon size=4>enable_grad</font></a><br>\n",
    "Context-manager that enables gradient calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <a href=\"https://pytorch.org/docs/stable/generated/torch.autograd.set_grad_enabled.html#torch.autograd.set_grad_enabled\" style=\"text-decoration:none;\"><font color=maroon size=4>set_grad_enabled</font></a><br>\n",
    "Context-manager that sets gradient calculation to on or off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <a href=\"https://pytorch.org/docs/stable/generated/torch.autograd.inference_mode.html#torch.autograd.inference_mode\" style=\"text-decoration:none;\"><font color=maroon size=4>inference_mode</font></a><br>\n",
    "Context-manager that enables or disables inference mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default gradient layouts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a non-sparse param receives a non-sparse gradient during <a href=\"https://pytorch.org/docs/stable/generated/torch.autograd.backward.html#torch.autograd.backward\" style=\"text-decoration:none;\">torch.autograd.backward()</a> or <a href=\"https://pytorch.org/docs/stable/generated/torch.Tensor.backward.html#torch.Tensor.backward\" style=\"text-decoration:none;\">torch.Tensor.backward()</a> `param.grad` is accumulated as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font size=3><b>If `param.grad` is initially `None`:</b></font>\n",
    "    1. If `param`’s memory is non-overlapping and dense, `.grad` is created with strides matching `param` (thus matching `param`’s layout).\n",
    "<br>\n",
    "<br>\n",
    "    2. Otherwise, `.grad` is created with rowmajor-contiguous strides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font size=3><b>If `param` already has a non-sparse `.grad` attribute:</b></font>\n",
    "    3. If `create_graph=False`, `backward()` accumulates into `.grad` in-place, which preserves its strides.\n",
    "<br>\n",
    "<br>\n",
    "    4. If `create_graph=True`, `backward()` replaces `.grad` with a new tensor `.grad + new grad`, which attempts (but does not guarantee) matching the preexisting `.grad’s` strides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=maroon size=3>The default behavior (letting `.grads` be `None` before the first `backward()`, such that their layout is created according to 1 or 2, and retained over time according to 3 or 4) is recommended for best performance. Calls to `model.zero_grad()` or `optimizer.zero_grad()` will not affect `.grad` layouts.\n",
    "\n",
    "In fact, resetting all `.grads` to `None` before each accumulation phase, e.g.:</font>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for iterations...\n",
    "    ...\n",
    "    for param in model.parameters():\n",
    "        param.grad = None\n",
    "    loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=maroon size=3>such that they’re recreated according to 1 or 2 every time, is a valid alternative to `model.zero_grad()` or `optimizer.zero_grad()` that may improve performance for some networks.</font>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual gradient layouts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=maroon size=3>If you need manual control over `.grad`’s strides, assign `param.grad = a zeroed tensor with desired strides` before the first `backward()`, and never reset it to `None`. <b>3</b> guarantees your layout is preserved as long as `create_graph=False`. <b>4</b> indicates your layout is likely preserved even if `create_graph=True`.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-place operations on Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supporting in-place operations in autograd is a hard matter, and we discourage their use in most cases. Autograd’s aggressive buffer freeing and reuse makes it very efficient and there are very few occasions when in-place operations actually lower memory usage by any significant amount. Unless you’re operating under heavy memory pressure, you might never need to use them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-place correctness checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All `Tensor`s keep track of in-place operations applied to them, and if the implementation detects that a tensor was saved for backward in one of the functions, but it was modified in-place afterwards, an error will be raised once backward pass is started. This ensures that if you’re using in-place functions and not seeing any errors, you can be sure that the computed gradients are correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable (deprecated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "<font size=3 color=red><b>WARNNING: </b></font>\n",
    "\n",
    "The Variable API has been deprecated: Variables are no longer necessary to use autograd with tensors. Autograd automatically supports Tensors with `requires_grad` set to `True`. \n",
    "<br>\n",
    "<br>\n",
    "Below please find a quick guide on what has changed:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <font style=\"color:red;font-weight:bold\">Tensor autograd functions</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color=maroon size=4>torch.Tensor.grad</font><br>\n",
    "This attribute is `None` by default and becomes a Tensor the first time a call to <a href=\"https://pytorch.org/docs/stable/generated/torch.autograd.backward.html#torch.autograd.backward\" style=\"text-decoration:none;\"><font color=maroon>backward()</font></a> computes gradients for `self`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color=maroon size=4>torch.Tensor.requires_grad</font><br>\n",
    "Is `True` if gradients need to be computed for this Tensor, `False` otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color=maroon size=4>torch.Tensor.is_leaf</font><br>\n",
    "<font size=3>All Tensors that have `requires_grad` which is `False` will be **leaf Tensors** by convention.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color=maroon size=4>torch.Tensor.backward</font>([gradient, …])<br>\n",
    "Computes the gradient of current tensor w.r.t."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color=maroon size=4>torch.Tensor.detach</font><br>\n",
    "Returns a new Tensor, detached from the current graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color=maroon size=4>torch.Tensor.detach_</font><br>\n",
    "Detaches the Tensor from the graph that created it, making it a leaf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color=maroon size=4>torch.Tensor.register_hook</font>(hook)<br>\n",
    "Registers a backward hook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color=maroon size=4>torch.Tensor.retain_grad</font>()<br>\n",
    "Enables this Tensor to have their <a href=\"https://pytorch.org/docs/stable/generated/torch.autograd.grad.html#torch.autograd.grad\" style=\"text-decoration:none;\"><font color=maroon>grad</font></a> populated during <a href=\"https://pytorch.org/docs/stable/generated/torch.autograd.backward.html#torch.autograd.backward\" style=\"text-decoration:none;\"><font color=maroon>backward()</font></a>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <font style=\"color:red;font-weight:bold\">Function</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<font size=4 color=black>`CLASS` torch.autograd.Function(*args, **kwargs)</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=magenta>Base class to create custom *`autograd.Function`*</font>\n",
    "\n",
    "To create a custom *`autograd.Function`*, subclass this class and implement the <a href=\"https://pytorch.org/docs/stable/generated/torch.autograd.Function.forward.html#torch.autograd.Function.forward\" style=\"text-decoration:none;\"><font color=maroon>forward()</font></a> and <a href=\"https://pytorch.org/docs/stable/generated/torch.autograd.backward.html#torch.autograd.backward\" style=\"text-decoration:none;\"><font color=maroon>backward()</font></a> static methods. Then, to use your custom op in the forward pass, call the class method apply. Do not call `forward()` directly.\n",
    "\n",
    "To ensure correctness and best performance, make sure you are calling the correct methods on `ctx` and validating your backward function using <a href=\"https://pytorch.org/docs/stable/generated/torch.autograd.gradcheck.html#torch.autograd.gradcheck\" style=\"text-decoration:none;\"><font color=maroon>torch.autograd.gradcheck()</font></a>.\n",
    "\n",
    "See Docs > Extending PyTorch > <a href=\"https://pytorch.org/docs/stable/notes/extending.html#extending-autograd\" style=\"text-decoration:none;\"><font color=maroon>Extending torch.autograd</font></a> for more details on how to use this class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class Exp(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, i):\n",
    "        result = i.exp()\n",
    "        ctx.save_for_backward(result)\n",
    "        return result\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        result, = ctx.saved_tensors\n",
    "        return grad_output * result\n",
    "    \n",
    "# Use it by calling the apply method:\n",
    "output = Exp.apply(input)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <a href=\"https://pytorch.org/docs/stable/generated/torch.autograd.Function.forward.html#torch.autograd.Function.forward\" style=\"text-decoration:none;\"><font color=maroon size=4>Function.forward</font></a><br>\n",
    "Performs the operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <a href=\"https://pytorch.org/docs/stable/generated/torch.autograd.Function.backward.html#torch.autograd.Function.backward\" style=\"text-decoration:none;\"><font color=maroon size=4>Function.backward</font></a><br>\n",
    "Defines a formula for differentiating the operation with backward mode automatic differentiation (alias to the `vjp` function)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <a href=\"https://pytorch.org/docs/stable/generated/torch.autograd.Function.jvp.html#torch.autograd.Function.jvp\" style=\"text-decoration:none;\"><font color=maroon size=4>Function.jvp</font></a><br>\n",
    "Defines a formula for differentiating the operation with forward mode automatic differentiation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context method mixins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When creating a new <a href=\"https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function\" style=\"text-decoration:none;\">Function</a>, the following methods are available to *`ctx`*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <a href=\"https://pytorch.org/docs/stable/generated/torch.autograd.function.FunctionCtx.mark_dirty.html#torch.autograd.function.FunctionCtx.mark_dirty\" style=\"text-decoration:none;\"><font color=maroon size=4>function.FunctionCtx.mark_dirty</font></a><br>\n",
    "Marks given tensors as modified in an in-place operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <a href=\"https://pytorch.org/docs/stable/generated/torch.autograd.function.FunctionCtx.mark_non_differentiable.html#torch.autograd.function.FunctionCtx.mark_non_differentiable\" style=\"text-decoration:none;\"><font color=maroon size=4>function.FunctionCtx.mark_non_differentiable</font></a><br>\n",
    "Marks outputs as non-differentiable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <a href=\"https://pytorch.org/docs/stable/generated/torch.autograd.function.FunctionCtx.save_for_backward.html#torch.autograd.function.FunctionCtx.save_for_backward\" style=\"text-decoration:none;\"><font color=maroon size=4>function.FunctionCtx.save_for_backward</font></a><br>\n",
    "Saves given tensors for a future call to <a href=\"https://pytorch.org/docs/stable/generated/torch.autograd.Function.backward.html#torch.autograd.Function.backward\" style=\"text-decoration:none;\">backward()</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <a href=\"https://pytorch.org/docs/stable/generated/torch.autograd.function.FunctionCtx.set_materialize_grads.html#torch.autograd.function.FunctionCtx.set_materialize_grads\" style=\"text-decoration:none;\"><font color=maroon size=4>function.FunctionCtx.set_materialize_grads</font></a><br>\n",
    "Sets whether to materialize output grad tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical gradient checking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <a href=\"https://pytorch.org/docs/stable/generated/torch.autograd.gradcheck.html#torch.autograd.gradcheck\" style=\"text-decoration:none;\"><font color=maroon size=4>gradcheck</font></a><br>\n",
    "Check gradients computed via small finite differences against analytical gradients w.r.t."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <a href=\"https://pytorch.org/docs/stable/generated/torch.autograd.gradgradcheck.html#torch.autograd.gradgradcheck\" style=\"text-decoration:none;\"><font color=maroon size=4>gradgradcheck</font></a><br>\n",
    "Check gradients of gradients computed via small finite differences against analytical gradients w.r.t."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autograd includes a profiler that lets you inspect the cost of different operators inside your model - both on the CPU and GPU. There are two modes implemented at the moment:\n",
    "* **CPU-only** using `profile`. \n",
    "* and **nvprof** based (registers both CPU and GPU activity) using `emit_nvtx`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `torch.autograd.profiler`**.profile()**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<font size=4 color=black>`CLASS` <a href=\"https://pytorch.org/docs/stable/autograd.html#torch.autograd.profiler.profile\" style=\"text-decoration:none;\">torch.autograd.profiler<b>.profile</b></a>(<font color=gray size=3>enabled=True, *, use_cuda=False, record_shapes=False, with_flops=False, profile_memory=False, with_stack=False, with_modules=False, use_kineto=False, use_cpu=True</font>)</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Context manager that manages autograd profiler state and holds a summary of results. Under the hood it just records events of functions being executed in C++ and exposes those events to Python. You can wrap any code into it and it will only report runtime of PyTorch functions. \n",
    "\n",
    "Note: profiler is thread local and is automatically propagated into the async tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters：** (详见蓝色字体链接)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                              aten::pow        32.32%       6.578ms        43.74%       8.903ms      44.515us           200  \n",
      "                                              aten::mul        14.31%       2.913ms        22.91%       4.664ms      23.320us           200  \n",
      "                                            aten::copy_         9.25%       1.883ms         9.25%       1.883ms       9.415us           200  \n",
      "                                           PowBackward0         6.62%       1.348ms        44.17%       8.991ms      89.910us           100  \n",
      "                                             aten::add_         6.46%       1.314ms         6.46%       1.314ms      13.273us            99  \n",
      "                                               aten::to         6.26%       1.274ms        10.21%       2.078ms       6.927us           300  \n",
      "                                        aten::ones_like         5.10%       1.038ms        13.30%       2.707ms      27.070us           100  \n",
      "                                    aten::empty_strided         4.35%     885.000us         4.52%     921.000us       4.605us           200  \n",
      "                 struct torch::autograd::AccumulateGrad         3.11%     634.000us        11.51%       2.342ms      23.420us           100  \n",
      "                                       aten::empty_like         2.70%     550.000us         6.62%       1.348ms      13.480us           100  \n",
      "                                      aten::result_type         2.61%     532.000us         2.61%     532.000us       2.660us           200  \n",
      "                                                 detach         1.62%     330.000us         1.62%     330.000us     330.000us             1  \n",
      "                                            aten::fill_         1.58%     321.000us         1.58%     321.000us       3.210us           100  \n",
      "                                         aten::_to_copy         1.47%     300.000us         5.71%       1.163ms      11.630us           100  \n",
      "autograd::engine::evaluate_function: struct torch::a...         1.33%     270.000us        12.54%       2.552ms      25.520us           100  \n",
      "      autograd::engine::evaluate_function: PowBackward0         0.89%     181.000us        45.06%       9.172ms      91.720us           100  \n",
      "                                           aten::detach         0.02%       4.000us         1.64%     334.000us     334.000us             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 20.355ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.randn((1, 1), requires_grad=True)\n",
    "with torch.autograd.profiler.profile() as prof:\n",
    "    for _ in range(100):  # any normal python code, really!\n",
    "        y = x ** 2\n",
    "        y.backward()\n",
    "# NOTE: some columns were removed for brevity\n",
    "print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <a href=\"https://pytorch.org/docs/stable/generated/torch.autograd.profiler.profile.export_chrome_trace.html#torch.autograd.profiler.profile.export_chrome_trace\" style=\"text-decoration:none;\"><font color=maroon size=4>profiler.profile.export_chrome_trace</font></a><br>\n",
    "Exports an EventList as a Chrome tracing tools file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <a href=\"https://pytorch.org/docs/stable/generated/torch.autograd.profiler.profile.key_averages.html#torch.autograd.profiler.profile.key_averages\" style=\"text-decoration:none;\"><font color=maroon size=4>profiler.profile.key_averages</font></a><br>\n",
    "Averages all function events over their keys."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <a href=\"https://pytorch.org/docs/stable/generated/torch.autograd.profiler.profile.self_cpu_time_total.html#torch.autograd.profiler.profile.self_cpu_time_total\" style=\"text-decoration:none;\"><font color=maroon size=4>profiler.profile.self_cpu_time_total</font></a><br>\n",
    "Returns total time spent on CPU obtained as a sum of all self times across all the events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <a href=\"https://pytorch.org/docs/stable/generated/torch.autograd.profiler.profile.total_average.html#torch.autograd.profiler.profile.total_average\" style=\"text-decoration:none;\"><font color=maroon size=4>profiler.profile.total_average</font></a><br>\n",
    "Averages all events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `torch.autograd.profiler`**.emit_nvtx()**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<font size=4 color=black>`CLASS` <a href=\"https://pytorch.org/docs/stable/autograd.html#torch.autograd.profiler.emit_nvtx\" style=\"text-decoration:none;\">torch.autograd.profiler<b>.emit_nvtx</b></a>(<font color=gray size=3>enabled=True, record_shapes=False</font>)</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Context manager that makes every autograd operation emit an NVTX range.\n",
    "\n",
    "It is useful when running the program under ` nvprof`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=gray>锐平：nvprof (nvidia profile?) 和 NVIDIA Visual Profiler 是用来分析CUDA程序性能的工具。</font>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "nvprof --profile-from-start off -o trace_name.prof -- <regular command here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, there’s no way to force nvprof to flush the data it collected to disk, so for CUDA profiling one has to use this context manager to annotate nvprof traces and wait for the process to exit before inspecting them. Then, either `NVIDIA Visual Profiler (nvvp)` can be used to visualize the timeline, or <a href=\"https://pytorch.org/docs/stable/generated/torch.autograd.profiler.load_nvprof.html#torch.autograd.profiler.load_nvprof\" style=\"text-decoration:none;\"><font color=maroon>torch.autograd.profiler.load_nvprof()</font></a> can load the results for inspection e.g. in Python REPL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters：** (详见蓝色字体链接)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "with torch.cuda.profiler.profile():\n",
    "    model(x)      # Warmup CUDA memory allocator and profiler\n",
    "    with torch.autograd.profiler.emit_nvtx():\n",
    "        model(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font style=\"font-size:120%\">Forward-backward correlation</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When viewing a profile created using `emit_nvtx` in the Nvidia Visual Profiler, correlating each backward-pass op with the corresponding forward-pass op can be difficult. To ease this task, `emit_nvtx` appends sequence number information to the ranges it generates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the forward pass, each function range is decorated with `seq=<N>`. `seq` is a running counter, incremented each time a new backward Function object is created and stashed for backward. Thus, the `seq=<N>` annotation associated with each forward function range tells you that if a backward Function object is created by this forward function, the backward object will receive sequence number N. During the backward pass, the top-level range wrapping each C++ backward Function’s `apply()` call is decorated with `stashed seq=<M>`. `M` is the sequence number that the backward object was created with. By comparing `stashed seq` numbers in backward with `seq` numbers in forward, you can track down which forward op created each backward Function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any functions executed during the backward pass are also decorated with `seq=<N>`. During default backward (with `create_graph=False`) this information is irrelevant, and in fact, `N` may simply be 0 for all such functions. Only the top-level ranges associated with backward Function objects’ `apply()` methods are useful, as a way to correlate these Function objects with the earlier forward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font style=\"font-size:120%\">Double-backward</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If, on the other hand, a backward pass with `create_graph=True` is underway (in other words, if you are setting up for a ***double-backward***), each function’s execution during backward is given a nonzero, useful `seq=<N>`. Those functions may themselves create Function objects to be executed later during double-backward, just as the original functions in the forward pass did. <font color=maroon>The relationship between *backward* and *double-backward* is conceptually the same as the relationship between forward and backward:</font> The functions still emit current-sequence-number-tagged ranges, the Function objects they create still stash those sequence numbers, and during the eventual double-backward, the Function objects’ `apply()` ranges are still tagged with `stashed seq` numbers, which can be compared to seq numbers from the backward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <a href=\"https://pytorch.org/docs/stable/generated/torch.autograd.profiler.load_nvprof.html#torch.autograd.profiler.load_nvprof\" style=\"text-decoration:none;\"><font color=maroon size=4>profiler.load_nvprof</font></a><br>\n",
    "Opens an nvprof trace file and parses autograd annotations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<font size=4 color=black>`CLASS` <a href=\"https://pytorch.org/docs/stable/autograd.html#anomaly-detection\" style=\"text-decoration:none;\">torch.autograd<b>.detect_anomaly</b></a></font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Context-manager that enable anomaly detection for the autograd engine.\n",
    "\n",
    "This does two things:\n",
    "\n",
    "* Running the forward pass with detection enabled will allow the backward pass to print the traceback of the forward operation that created the failing backward function.\n",
    "\n",
    "\n",
    "* Any backward computation that generate “nan” value will raise an error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "<font size=3 color=red><b>WARNNING: </b></font>\n",
    "\n",
    "<font color=black>This mode should be enabled `only for debugging` as the different tests will slow down your program execution.</font>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可以试运行下面的程序，会报错\n",
    "import torch\n",
    "from torch import autograd\n",
    "\n",
    "class MyFunc(autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, inp):\n",
    "        return inp.clone()\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, gO):\n",
    "        # Error during the backward pass\n",
    "        raise RuntimeError(\"Some error in backward\")\n",
    "        return gO.clone()\n",
    "    \n",
    "\n",
    "def run_fn(a):\n",
    "    out = MyFunc.apply(a)\n",
    "    return out.sum()\n",
    "inp = torch.rand(10, 10, requires_grad=True)\n",
    "out = run_fn(inp)\n",
    "out.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(48.1482, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可以试运行下面的程序，会报错\n",
    "with autograd.detect_anomaly():\n",
    "    inp = torch.rand(10, 10, requires_grad=True)\n",
    "    out = run_fn(inp)\n",
    "    out.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<font size=4 color=black>`CLASS` <a href=\"https://pytorch.org/docs/stable/autograd.html#anomaly-detection\" style=\"text-decoration:none;\">torch.autograd<b>.set_detect_anomaly</b></a>(<font color=gray size=3>mode</font>)</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Context-manager that sets the anomaly detection for the autograd engine on or off.\n",
    "\n",
    "`set_detect_anomaly` will enable or disable the autograd anomaly detection based on its argument mode. It can be used as a context-manager or as a function.\n",
    "\n",
    "See `detect_anomaly` above for details of the anomaly detection behaviour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters：** \n",
    "\n",
    "* **mode** (bool) – Flag whether to enable anomaly detection (`True`), or disable (`False`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saved tensors default hooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some operations need intermediary results to be saved during the forward pass in order to execute the backward pass. You can define how these saved tensors should be packed / unpacked using hooks. A common application is to trade compute for memory by saving those intermediary results to disk or to CPU instead of leaving them on the GPU. This is especially useful if you notice your model fits on GPU during evaluation, but not training. Also see `Docs > Autograd mechanics > `<a href=\"https://pytorch.org/docs/stable/notes/autograd.html#saved-tensors-hooks-doc\" style=\"text-decoration:none;\">Hooks for saved tensors</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<font size=4 color=black>`CLASS` <a href=\"https://pytorch.org/docs/stable/autograd.html#saved-tensors-default-hooks\" style=\"text-decoration:none;\">torch.autograd.graph<b>.saved_tensors_hooks</b></a>(<font color=gray size=3>pack_hook, unpack_hook </font>)</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Context-manager** that sets a pair of `pack / unpack hooks` for saved tensors.\n",
    "\n",
    "Use this context-manager to define how intermediary results of an operation should be packed before saving, and unpacked on retrieval.\n",
    "\n",
    "In that context, the `pack_hook` function will be called everytime an operation saves a tensor for backward (<font color=maroon>this includes intermediary results saved using `save_for_backward()` but also those recorded by a PyTorch-defined operation</font>). <font color=maroon size=4>The output of `pack_hook` is then stored in the <b>computation graph</b> instead of the original tensor.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `unpack_hook` is called when the saved tensor needs to be accessed, namely when executing <a href=\"https://pytorch.org/docs/stable/generated/torch.Tensor.backward.html#torch.Tensor.backward\" style=\"text-decoration:none;\"><font color=maroon>torch.Tensor.backward()</font></a> or <a href=\"https://pytorch.org/docs/stable/generated/torch.autograd.grad.html#torch.autograd.grad\" style=\"text-decoration:none;\"><font color=maroon>torch.autograd.grad()</font></a>. It takes as argument the packed object returned by `pack_hook` and should return a tensor which has the same content as the original tensor (passed as input to the corresponding `pack_hook`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hooks should have the following signatures:\n",
    "\n",
    "    pack_hook(tensor: Tensor) -> Any\n",
    "\n",
    "    unpack_hook(Any) -> Tensor\n",
    "\n",
    "where the return value of `pack_hook` is a valid input to `unpack_hook`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, you want `unpack_hook(pack_hook(t))` to be equal to t in terms of value, size, dtype and device."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packing tensor([1., 1., 1., 1., 1.], requires_grad=True)\n",
      "Packing tensor([2., 2., 2., 2., 2.], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def pack_hook(x):\n",
    "    print(\"Packing\", x)\n",
    "    return x\n",
    "\n",
    "def unpack_hook(x):\n",
    "    print(\"Unpacking\", x)\n",
    "    return x\n",
    "\n",
    "a = torch.ones(5, requires_grad=True)\n",
    "b = torch.ones(5, requires_grad=True) * 2\n",
    "\n",
    "with torch.autograd.graph.saved_tensors_hooks(pack_hook, unpack_hook):\n",
    "    y = a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unpacking tensor([1., 1., 1., 1., 1.], requires_grad=True)\n",
      "Unpacking tensor([2., 2., 2., 2., 2.], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y.sum().backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "<font size=3 color=red><b>WARNNING: </b></font>\n",
    "\n",
    "<font color=black>Performing an inplace operation on the input to either hooks may lead to undefined behavior.</font>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "<font size=3 color=red><b>WARNNING: </b></font>\n",
    "\n",
    "<font color=black>Only one pair of hooks is allowed at a time. When recursively nesting this context-manager, only the inner-most pair of hooks will be applied.</font>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<font size=4 color=black>`CLASS` <a href=\"https://pytorch.org/docs/stable/autograd.html#saved-tensors-default-hooks\" style=\"text-decoration:none;\">torch.autograd.graph<b>.save_on_cpu</b></a>(<font color=gray size=3>pin_memory=False</font>)</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Context-manager under which tensors saved by the forward pass will be stored on cpu, then retrieved for backward.\n",
    "\n",
    "When performing operations within this context manager, intermediary results saved in the graph during the forward pass will be moved to CPU, then copied back to the original device when needed for the backward pass. If the graph was already on CPU, no tensor copy is performed.\n",
    "\n",
    "<font color=maroon size=4>Use this context-manager to trade compute for GPU memory usage (e.g. when your model doesn’t fit in GPU memory during training).</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters**\n",
    "* **pin_memory** (bool) – If `True` tensors will be saved to CPU pinned memory during packing and copied to GPU asynchronously during unpacking. Defaults to `False`. Also see Docs > CUDA semantics > <a href=\"https://pytorch.org/docs/stable/notes/cuda.html#cuda-memory-pinning\" style=\"text-decoration:none;\"><font color=maroon>Use pinned memory buffers</font></a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(5, requires_grad=True, device=\"cuda\")\n",
    "b = torch.randn(5, requires_grad=True, device=\"cuda\")\n",
    "c = torch.randn(5, requires_grad=True, device=\"cuda\")\n",
    "\n",
    "def f(a, b, c):\n",
    "    prod_1 = a * b            # a and b are saved on GPU\n",
    "    with torch.autograd.graph.save_on_cpu():\n",
    "        prod_2 = prod_1 * c   # prod_1 and c are saved on CPU\n",
    "    y = prod_2 * a            # prod_2 and a are saved on GPU\n",
    "    return y\n",
    "\n",
    "\n",
    "y = f(a, b, c)\n",
    "del a, b, c                   # for illustration only\n",
    "\n",
    "# the content of a, b, and prod_2 are still alive on GPU\n",
    "# the content of prod_1 and c only live on CPU\n",
    "\n",
    "\n",
    "y.sum().backward()  # all CPU tensors are moved back to GPU, for backward\n",
    "# all intermediary tensors are released (deleted) after the call to backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.0208,  0.0058, -0.0550, -0.1145,  1.6716], device='cuda:0',\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9788/2167009006.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0ma\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'b' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9788/1685013873.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'b' is not defined"
     ]
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'c' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9788/3235490055.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'c' is not defined"
     ]
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=gray size=3>Tutorials > </font>\n",
    "\n",
    "# <font style=\"font-size:120%;color:maroon;font-weight:bold\">Forward-mode Automatic Differentiation (Beta)</font> <a href=\"https://pytorch.org/tutorials/intermediate/forward_ad_usage.html\" style=\"text-decoration:none;\"><font size=2>[link]</font></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial demonstrates how to use <font color=maroon><b>forward-mode AD</b></font> to compute <font color=maroon>directional derivatives</font> (or equivalently, <font color=maroon>Jacobian-vector products</font>).\n",
    "\n",
    "The tutorial below uses some APIs only available in versions >= 1.11 (or nightly builds).\n",
    "\n",
    "<font color=maroon><b>Also note that</b></font> forward-mode AD is currently in beta. The API is subject to change and operator coverage is still incomplete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike reverse-mode AD, forward-mode AD computes gradients eagerly alongside the forward pass. We can use forward-mode AD to compute a directional derivative by performing the forward pass as before, except we first associate our input with another tensor representing the direction of the directional derivative (or equivalently, the `v` in a Jacobian-vector product). <font color=maroon>When an input, which we call `“primal”`, is associated with a `“direction” tensor`, which we call `“tangent”`, the resultant new tensor object is called a **`“dual tensor”`** for its connection to <a href=\"https://en.wikipedia.org/wiki/Dual_number\" style=\"text-decoration:none;\"><b>dual numbers</b></a></font>[0]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color=maroon>As the forward pass is performed, if any input tensors are dual tensors, extra computation is performed to propogate this <b>“sensitivity”</b> of the function.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'tangent'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15420/1570022941.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[0mjvp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfwAD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munpack_dual\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdual_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtangent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m \u001b[1;32massert\u001b[0m \u001b[0mfwAD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munpack_dual\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdual_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtangent\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'tangent'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd.forward_ad as fwAD\n",
    "\n",
    "primal = torch.randn(10, 10)\n",
    "tangent = torch.randn(10, 10)\n",
    "\n",
    "def fn(x, y):\n",
    "    return x ** 2 + y ** 2\n",
    "\n",
    "\n",
    "'''All forward AD computation must be performed in the context of a ``dual_level`` context. '''\n",
    "# All dual tensors created in such a context will have their tangents destroyed upon exit. \n",
    "# This is to ensure that if the output or intermediate results of this computation are reused\n",
    "# in a future forward AD computation, their tangents (which are associated\n",
    "# with this computation) won't be confused with tangents from the later computation.\n",
    "with fwAD.dual_level():\n",
    "    # To create a dual tensor we associate a tensor, which we call the ``primal`` \n",
    "    # with another tensor of the same size, which we call the ``tangent``.\n",
    "    #\n",
    "    # If the layout of the tangent is different from that of the primal,\n",
    "    # The values of the tangent are copied into a new tensor with the same\n",
    "    # metadata as the primal. Otherwise, the tangent itself is used as-is.\n",
    "    ''' It is also important to note that the dual tensor created by ``make_dual`` \n",
    "        is a view of the primal.\n",
    "    '''\n",
    "    dual_input = fwAD.make_dual(primal, tangent)\n",
    "    assert fwAD.unpack_dual(dual_input).tangent is tangent\n",
    "\n",
    "    \n",
    "    # To demonstrate the case where the copy of the tangent happens,\n",
    "    # we pass in a tangent with a layout different from that of the primal\n",
    "    dual_input_alt = fwAD.make_dual(primal, tangent.T)\n",
    "    assert fwAD.unpack_dual(dual_input_alt).tangent is not tangent\n",
    "\n",
    "    \n",
    "    # Tensors that do not not have an associated tangent are automatically\n",
    "    # considered to have a zero-filled tangent of the same shape.\n",
    "    plain_tensor = torch.randn(10, 10)\n",
    "    dual_output = fn(dual_input, plain_tensor)\n",
    "\n",
    "    # Unpacking the dual returns a namedtuple with ``primal`` and ``tangent`` as attributes\n",
    "    jvp = fwAD.unpack_dual(dual_output).tangent\n",
    "\n",
    "assert fwAD.unpack_dual(dual_output).tangent is None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage with Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use **`nn.Module`** with `forward AD`, replace the parameters of your model with `dual tensors` before performing the `forward pass`. At the time of writing, it is not possible to create dual tensor `nn.Parameter`s. As a workaround, one must register the dual tensor as a non-parameter attribute of the module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import torch.nn as nn\n",
    "\n",
    "model = nn.Linear(5, 5)\n",
    "input = torch.randn(16, 5)\n",
    "\n",
    "params = {name: p for name, p in model.named_parameters()}\n",
    "tangents = {name: torch.rand_like(p) for name, p in params.items()}\n",
    "\n",
    "with fwAD.dual_level():\n",
    "    for name, p in params.items():\n",
    "        delattr(model, name)\n",
    "        setattr(model, name, fwAD.make_dual(p, tangents[name]))\n",
    "\n",
    "    out = model(input)\n",
    "    jvp = fwAD.unpack_dual(out).tangent\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Modules stateless API (experimental)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to use **`nn.Module`** with forward AD is to utilize the stateless API. \n",
    "\n",
    "NB: At the time of writing the stateless API is still experimental and may be subject to change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from torch.nn.utils._stateless import functional_call\n",
    "\n",
    "# We need a fresh module because the functional call requires the\n",
    "# the model to have parameters registered.\n",
    "model = nn.Linear(5, 5)\n",
    "\n",
    "dual_params = {}\n",
    "with fwAD.dual_level():\n",
    "    for name, p in params.items():\n",
    "        # Using the same ``tangents`` from the above section\n",
    "        dual_params[name] = fwAD.make_dual(p, tangents[name])\n",
    "    out = functional_call(model, dual_params, input)\n",
    "    jvp2 = fwAD.unpack_dual(out).tangent\n",
    "\n",
    "# Check our results\n",
    "assert torch.allclose(jvp, jvp2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom autograd Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Functions also support `forward-mode AD`. <font size=3 color=maroon>To create custom Function supporting forward-mode AD, register the `jvp()` static method. It is possible, but not mandatory for custom Functions to support both forward and backward AD.</font> See the documentation: <a href=\"https://pytorch.org/docs/master/notes/extending.html#forward-mode-ad\" style=\"text-decoration:none;\"><b>Docs > Extending PyTorch</b></a> `(Docs > PyTorch documentation > Notes: Extending PyTorch)` for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class Fn(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, foo):\n",
    "        result = torch.exp(foo)\n",
    "        # Tensors stored in ctx can be used in the subsequent forward grad\n",
    "        # computation.\n",
    "        ctx.result = result\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def jvp(ctx, gI):\n",
    "        gO = gI * ctx.result\n",
    "        # If the tensor stored in ctx will not also be used in the backward pass,\n",
    "        # one can manually free it using ``del``\n",
    "        del ctx.result\n",
    "        return gO\n",
    "\n",
    "fn = Fn.apply\n",
    "\n",
    "primal = torch.randn(10, 10, dtype=torch.double, requires_grad=True)\n",
    "tangent = torch.randn(10, 10)\n",
    "\n",
    "with fwAD.dual_level():\n",
    "    dual_input = fwAD.make_dual(primal, tangent)\n",
    "    dual_output = fn(dual_input)\n",
    "    jvp = fwAD.unpack_dual(dual_output).tangent\n",
    "\n",
    "# It is important to use ``autograd.gradcheck`` to verify that your\n",
    "# custom autograd Function computes the gradients correctly. By default,\n",
    "# gradcheck only checks the backward-mode (reverse-mode) AD gradients. Specify\n",
    "# ``check_forward_ad=True`` to also check forward grads. If you did not\n",
    "# implement the backward formula for your function, you can also tell gradcheck\n",
    "# to skip the tests that require backward-mode AD by specifying\n",
    "# ``check_backward_ad=False``, ``check_undefined_grad=False``, and\n",
    "# ``check_batched_grad=False``.\n",
    "torch.autograd.gradcheck(Fn.apply, (primal,), check_forward_ad=True,\n",
    "                         check_backward_ad=False, check_undefined_grad=False,\n",
    "                         check_batched_grad=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[0] <a href=\"https://en.wikipedia.org/wiki/Dual_number\" style=\"text-decoration:none;\">https://en.wikipedia.org/wiki/Dual_number</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=gray size=3>Docs > </font>\n",
    "\n",
    "# <font style=\"font-size:120%;color:maroon;font-weight:bold\">Inference Mode</font> <a href=\"https://pytorch.org/cppdocs/notes/inference_mode.html\" style=\"text-decoration:none;\"><font size=2>[link]</font></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`c10::InferenceMode` is a new RAII guard analogous to **`NoGradMode`** to be used when you are certain your operations will have no interactions with <font color=maroon>autograd (e.g. model training)</font>. Compared to `NoGradMode`, code run under this mode gets better performance by disabling autograd related work like view tracking and version counter bumps. However, tensors created inside `c10::InferenceMode` has more limitation when interacting with autograd system as well.<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`InferenceMode`** can be enabled for a given block of code. Inside `InferenceMode` all newly allocated (non-view) tensors are marked as <font size=3 color=blue>inference tensors</font>. Inference tensors:\n",
    "\n",
    "* do not have a <font size=3 color=maroon>version counter</font> so an error will be raised if you try to read their version (e.g., because you saved this tensor for backward).\n",
    "\n",
    "\n",
    "* are <font size=3 color=maroon>immutable</font> outside `InferenceMode`. So an error will be raised if you try to: \n",
    "    * \\- mutate their data outside InferenceMode. \n",
    "    * \\- mutate them into `requires_grad=True` outside InferenceMode. \n",
    "<br>\n",
    "To work around you can make a clone outside `InferenceMode` to get a normal tensor before mutating.<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=maroon>A `non-view tensor` is an inference tensor if and only if it was allocated inside `InferenceMode`. <br>A `view tensor` is an inference tensor if and only if the tensor it is a view of is an inference tensor.</font><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3><font color=maroon>Inside an `InferenceMode block`, we make the following performance guarantees:</font>\n",
    "\n",
    "* Like `NoGradMode`, all operations do not record `grad_fn` even if their inputs have `requires_grad=True`. This applies to both inference tensors and normal tensors.\n",
    "\n",
    "    \n",
    "* View operations on inference tensors do not do view tracking. View and non-view inference tensors are indistinguishable.\n",
    "\n",
    "    \n",
    "* Inplace operations on inference tensors are guaranteed not to do a version bump.\n",
    "\n",
    "For more implementation details of `InferenceMode` please see the <a href=\"https://github.com/pytorch/rfcs/pull/17\" style=\"text-decoration:none;\"><font color=maroon size=2>RFC-0011-InferenceMode</font></a>.</front>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Migration guide from `AutoNonVariableTypeMode`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In production use of PyTorch for inference workload, we have seen a proliferation of uses of the C++ guard `AutoNonVariableTypeMode` (now `AutoDispatchBelowADInplaceOrView`), which disables autograd, view tracking and version counter bumps. \n",
    "\n",
    "Unfortunately, current colloquial of this guard for inference workload is unsafe: it’s possible to use `AutoNonVariableTypeMode` to bypass PyTorch’s safety checks and result in silently wrong results, e.g. PyTorch throws an error when tensors saved for backwards are subsequently mutated, but mutation happens inside `AutoNonVariableTypeMode` will silently bypass the check and returns wrong gradient to users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When current users of `AutoNonVariableTypeMode` think about migrating, the following steps might help you decide the best alternatives: (以下，略)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ptg",
   "language": "python",
   "name": "ptg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "330px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
