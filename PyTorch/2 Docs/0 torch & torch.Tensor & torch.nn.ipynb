{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=center><font color=maroon size=8><b>torch.nn</b></font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4><b>References:</b></font>\n",
    "* `Docs > `<a href=\"https://pytorch.org/docs/stable/tensor_attributes.html\" style=\"text-decoration:none;\">Tensor Attributes</a>\n",
    "* `Docs > `<a href=\"https://pytorch.org/docs/stable/tensor_view.html\" style=\"text-decoration:none;\">Tensor Views</a>\n",
    "* Docs > <a href=\"\" style=\"text-decoration:none;\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `Docs > `<a href=\"https://pytorch.org/docs/stable/torch.html\" style=\"text-decoration:none;\">torch</a>\n",
    "    * Docs > torch > <a href=\"https://pytorch.org/docs/stable/generated/torch.tensor.html\" style=\"text-decoration:none;\">torch.tensor</a> \n",
    "    * Docs > torch > <a href=\"https://pytorch.org/docs/stable/generated/torch.no_grad.html\" style=\"text-decoration:none;\">no_grad</a>\n",
    "    * Docs > torch > <a href=\"https://pytorch.org/docs/stable/generated/torch.numel.html\" style=\"text-decoration:none;\">torch.numel</a>\n",
    "    * Docs > torch > <a href=\"\" style=\"text-decoration:none;\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `Docs > `<a href=\"https://pytorch.org/docs/stable/tensors.html\" style=\"text-decoration:none;\">torch.Tensor</a>\n",
    "* `Docs > torch.Tensor > `<a href=\"https://pytorch.org/docs/stable/generated/torch.Tensor.view.html\" style=\"text-decoration:none;\">torch.Tensor.view</a>\n",
    "* `Docs > torch.Tensor > `<a href=\"https://pytorch.org/docs/stable/generated/torch.Tensor.detach.html\" style=\"text-decoration:none;\">torch.Tensor.detach</a>\n",
    "* `Docs > torch.Tensor > `<a href=\"https://pytorch.org/docs/stable/generated/torch.Tensor.register_hook.html\" style=\"text-decoration:none;\">torch.Tensor.register_hook</a>\n",
    "* `Docs > torch.Tensor > `<a href=\"https://pytorch.org/docs/stable/generated/torch.Tensor.zero_.html\" style=\"text-decoration:none;\">torch.Tensor.zero_</a>\n",
    "* Docs > torch.Tensor > <a href=\"\" style=\"text-decoration:none;\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `Docs > `<a href=\"https://pytorch.org/docs/stable/nn.init.html\" style=\"text-decoration:none;\">torch.nn.init</a>\n",
    "* \n",
    "* `Docs > `<a href=\"https://pytorch.org/docs/stable/nn.html\" style=\"text-decoration:none;\">torch.nn</a>\n",
    "    * `Docs > torch.nn > `<a href=\"https://pytorch.org/docs/stable/generated/torch.nn.Module.html\" style=\"text-decoration:none;\">Module</a>\n",
    "    * \n",
    "    * `Docs > torch.nn > `<a href=\"https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html\" style=\"text-decoration:none;\">BatchNorm2d</a>\n",
    "    * `Docs > torch.nn > `<a href=\"https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html\" style=\"text-decoration:none;\">Dropout</a>\n",
    "    * `Docs > torch.nn > `<a href=\"\" style=\"text-decoration:none;\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3 color=gray>Docs > </font>\n",
    "# Tensor Attributes <a href=\"https://pytorch.org/docs/stable/tensor_attributes.html\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>Each `torch.Tensor` has a <a href=\"https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype\" style=\"text-decoration:none;font-size:120%\">torch.dtype</a>, <a href=\"https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.device\" style=\"text-decoration:none;font-size:120%\">torch.device</a>, and <a href=\"https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.layout|\" style=\"text-decoration:none;font-size:120%\">torch.layout</a>.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.dtype <a href=\"https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<font size=3 color=gray><b>CLASS</b></font>&emsp;\n",
    "<font size=4><b>torch.dtype</b></font>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>A `torch.dtype` is an object that represents the data type of a <a href=\"https://pytorch.org/docs/stable/tensors.html#torch.Tensor\" style=\"text-decoration:none;font-size:120%\">torch.Tensor</a>. PyTorch has <font color=maroon>**twelve**</font> different data types:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./1 PyTorch documentation/1 Notes/images/torch-dtype.jpeg\" width=700px align=left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>1</font> `torch.float16` or `torch.half` Sometimes referred to as `binary16`: uses 1 sign, 5 exponent, and 10 significand bits. <font color=maroon>Useful when **precision** is important.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>2</font> `torch.bfloat16` Sometimes referred to as `Brain Floating Point`: use 1 sign, 8 exponent and 7 significand bits. <font color=maroon>Useful when **range** is important</font>, since it has the same number of exponent bits as float32."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find out if a `torch.dtype` is a floating point data type, the property <a href=\"https://pytorch.org/docs/stable/generated/torch.is_floating_point.html\" style=\"text-decoration:none;font-size:130%\">is_floating_point</a> can be used, which returns True if the data type is a floating point data type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find out if a `torch.dtype` is a complex data type, the property <a href=\"https://pytorch.org/docs/stable/generated/torch.is_complex.html\" style=\"text-decoration:none;font-size:130%\">is_complex</a> can be used, which returns True if the data type is a complex data type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>When the dtypes of inputs to an arithmetic operation (add, sub, div, mul) differ, we promote by finding the minimum dtype that satisfies the following rules:\n",
    "\n",
    "\n",
    "* If the type of a scalar operand is of a <font color=maroon>higher category</font> than tensor operands (where <font color=maroon>complex > floating > integral > boolean</font>), we promote to a type with sufficient size to hold all scalar operands of that category.\n",
    "\n",
    "\n",
    "* If a zero-dimension tensor operand has a higher category than dimensioned operands, we promote to a type with sufficient size and category to hold all zero-dim tensor operands of that category.\n",
    "\n",
    "\n",
    "* If there are no higher-category zero-dim operands, we promote to a type with sufficient size and category to hold all dimensioned operands.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3 color=maroon>A floating point scalar operand has dtype <font color=royalblue>torch.get_default_dtype()</font> and an integral non-boolean scalar operand has dtype torch.int64. Unlike numpy, we do not inspect values when determining the minimum dtypes of an operand. Quantized and complex types are not yet supported.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_tensor = torch.ones(1, dtype=torch.float)\n",
    "double_tensor = torch.ones(1, dtype=torch.double)\n",
    "\n",
    "complex_float_tensor = torch.ones(1, dtype=torch.complex64)\n",
    "complex_double_tensor = torch.ones(1, dtype=torch.complex128)\n",
    "\n",
    "int_tensor = torch.ones(1, dtype=torch.int)\n",
    "long_tensor = torch.ones(1, dtype=torch.long)\n",
    "uint_tensor = torch.ones(1, dtype=torch.uint8)\n",
    "double_tensor = torch.ones(1, dtype=torch.double)\n",
    "bool_tensor = torch.ones(1, dtype=torch.bool)\n",
    "\n",
    "# zero-dim tensors\n",
    "long_zerodim = torch.tensor(1, dtype=torch.long)\n",
    "int_zerodim = torch.tensor(1, dtype=torch.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.add(5, 5).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int32"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5 is an int64, but does not have higher category than int_tensor so is not considered.\n",
    "# 这里标量 5 和 int_tensor 都是 int category\n",
    "(int_tensor + 5).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int32"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(int_tensor + long_zerodim).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(long_tensor + int_tensor).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(bool_tensor + long_tensor).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.uint8"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(bool_tensor + uint_tensor).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(float_tensor + double_tensor).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.complex128"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(complex_float_tensor + complex_double_tensor).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int32"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(bool_tensor + int_tensor).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since long is a different kind than float, result dtype only needs to be large enough\n",
    "# to hold the float.\n",
    "torch.add(long_tensor, float_tensor).dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>When the output tensor of an arithmetic operation is specified, we allow casting to its ***dtype*** except that:\n",
    "\n",
    "* An integral output tensor cannot accept a floating point tensor.\n",
    "\n",
    "\n",
    "* A boolean output tensor cannot accept a non-boolean tensor.\n",
    "\n",
    "\n",
    "* A non-complex output tensor cannot accept a complex tensor.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Casting Examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allowed:\n",
    "float_tensor *= float_tensor\n",
    "float_tensor *= int_tensor\n",
    "float_tensor *= uint_tensor\n",
    "float_tensor *= bool_tensor\n",
    "\n",
    "float_tensor *= double_tensor\n",
    "int_tensor *= long_tensor\n",
    "\n",
    "int_tensor *= uint_tensor\n",
    "\n",
    "uint_tensor *= int_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "result type Float can't be cast to the desired output type Int",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5348/2221064051.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# disallowed (RuntimeError: result type can't be cast to the desired output type):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mint_tensor\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0mfloat_tensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mbool_tensor\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0mint_tensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mbool_tensor\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0muint_tensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mfloat_tensor\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0mcomplex_float_tensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: result type Float can't be cast to the desired output type Int"
     ]
    }
   ],
   "source": [
    "# disallowed (RuntimeError: result type can't be cast to the desired output type):\n",
    "int_tensor *= float_tensor\n",
    "bool_tensor *= int_tensor\n",
    "bool_tensor *= uint_tensor\n",
    "float_tensor *= complex_float_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.device <a href=\"https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.device\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<font size=3 color=gray><b>CLASS</b></font>&emsp;\n",
    "<font size=4><b>torch.device</b></font>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `torch.device` is an object representing the device on which a <a href=\"https://pytorch.org/docs/stable/tensors.html#torch.Tensor\" style=\"text-decoration:none;font-size:120%\">torch.Tensor</a> is or will be allocated.\n",
    "\n",
    "The `torch.device` contains a device type ('<font color=maroon size=3>**cpu**</font>' or '<font color=maroon size=3>**cuda**</font>') <font color=maroon>and optional device ordinal for the device type</font>. If the device ordinal is not present, this object will always represent the current device for the device type, even after <a href=\"https://pytorch.org/docs/stable/generated/torch.cuda.set_device.html\" style=\"text-decoration:none;font-size:120%\">torch.cuda.set_device()</a> is called; e.g., a `torch.Tensor` constructed with device **`'cuda'`** is equivalent to **`'cuda:X'`** where X is the result of <a href=\"https://pytorch.org/docs/stable/generated/torch.cuda.current_device.html#torch.cuda.current_device\" style=\"text-decoration:none;font-size:120%\">torch.cuda.current_device()</a>.\n",
    "\n",
    "A `torch.Tensor`’s device can be accessed via the <a href=\"https://pytorch.org/docs/stable/generated/torch.Tensor.device.html\" style=\"text-decoration:none;font-size:120%\">Tensor.device</a> property.\n",
    "\n",
    "A `torch.device` can be constructed via a `string` or via a `string and device ordinal`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Via a string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device('cuda')  # current cuda device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Via a string and device ordinal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device('cuda', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu', index=0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device('cpu', 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<font size=3 color=red><b>NOTE: </b></font>\n",
    "<br><br>\n",
    "The `torch.device` argument in functions can generally be substituted with a string. This allows for fast prototyping of code.\n",
    "<br>\n",
    "\n",
    "    # Example of a function that takes in a torch.device\n",
    "    >>> cuda1 = torch.device('cuda:1')\n",
    "    >>> torch.randn((2,3), device=cuda1)\n",
    "\n",
    "    # You can substitute the torch.device with a string\n",
    "    >>> torch.randn((2,3), device='cuda:1')\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<font size=3 color=red><b>NOTE: </b></font>\n",
    "<br>\n",
    "<br>\n",
    "For legacy reasons, a device can be constructed via a single device ordinal, which is treated as a cuda device. This matches <a href=\"https://pytorch.org/docs/stable/generated/torch.Tensor.get_device.html\" style=\"text-decoration:none;color:blue;font-size:120%\">Tensor.get_device()</a>, which returns an ordinal for cuda tensors and is not supported for cpu tensors.\n",
    "\n",
    "    >>> torch.device(1)\n",
    "    device(type='cuda', index=1)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<font size=3 color=red><b>NOTE: </b></font>\n",
    "<br>\n",
    "<br>\n",
    "Methods which take a device will generally accept a (properly formatted) string or (legacy) integer device ordinal, i.e. the following are all equivalent:\n",
    "\n",
    "    >>> torch.randn((2,3), device=torch.device('cuda:1'))\n",
    "    >>> torch.randn((2,3), device='cuda:1')\n",
    "    >>> torch.randn((2,3), device=1)  # legacy\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.layout <a href=\"https://pytorch.org/docs/stable/tensor_attributes.html#torch-layout\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<font size=3 color=gray><b>CLASS</b></font>&emsp;\n",
    "<font size=4><b>torch.layout</b></font>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "<font size=3 color=red><b>WARNING: </b></font>\n",
    "\n",
    "The `torch.layout` class is in beta and subject to change.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>A `torch.layout` is an object that represents the memory layout of a <a href=\"https://pytorch.org/docs/stable/tensors.html#torch.Tensor\" style=\"text-decoration:none;font-size:120%\">torch.Tensor</a>. Currently, we support <font size=4 color=blue><b>torch.strided</b> (dense Tensors)</font> and have ***beta support*** for <font size=4 color=blue><b>torch.sparse_coo</b> (sparse COO Tensors)</font>.</font>\n",
    "\n",
    "<font size=3>`torch.strided` represents dense Tensors and is the memory layout that is most commonly used. Each strided tensor has an associated <font color=blue>**torch.Storage**</font>, which holds its data. These tensors provide multi-dimensional, <a href=\"https://en.wikipedia.org/wiki/Stride_of_an_array\" style=\"text-decoration:none;color:maroon;font-size:120%;\">strided</a> view of a storage. \n",
    "<br>\n",
    "<br>\n",
    "<font color=maroon><b>Strides are a list of integers:</b> the k-th stride represents the jump in the memory necessary to go from one element to the next one in the k-th dimension of the Tensor. This concept makes it possible to perform many tensor operations efficiently.</font></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])\n",
    "x.stride()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 5)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.t().stride()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>For more information on `torch.sparse_coo` tensors, see <a href=\"https://pytorch.org/docs/stable/sparse.html#sparse-docs\" style=\"text-decoration:none;color:maroon;font-size:120%;\">torch.sparse</a>.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./1 PyTorch documentation/1 Notes/images/Tensor.png\" width=500px>\n",
    "Cited from <a href=\"http://blog.ezyang.com/2019/05/pytorch-internals/\" style=\"text-decoration:none;color:maroon;font-size:110%;\">ezyang’s blogpost about PyTorch Internals</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.memory_format <a href=\"https://pytorch.org/docs/stable/tensor_attributes.html#torch-memory-format\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<font size=3 color=gray><b>CLASS</b></font>&emsp;\n",
    "<font size=4><b>torch.memory_format</b></font>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>A `torch.memory_format` is an object representing the memory format on which a <a href=\"https://pytorch.org/docs/stable/tensors.html#torch.Tensor\" style=\"text-decoration:none;font-size:120%\">torch.Tensor</a> is or will be allocated.</font>\n",
    "<br>\n",
    "<br>\n",
    "* <font size=4>**`torch.contiguous_format`**</font>: Tensor is or will be allocated in dense non-overlapping memory. Strides represented by values in decreasing order.\n",
    "<br>\n",
    "<br>\n",
    "* <font size=4>**`torch.channels_last`**</font>: Tensor is or will be allocated in dense non-overlapping memory. Strides represented by values in `strides[0] > strides[2] > strides[3] > strides[1] == 1` aka ***NHWC order***.\n",
    "<br>\n",
    "<br>\n",
    "* <font size=4>**`torch.preserve_format`**</font>: Used in functions like ***clone*** to preserve the memory format of the input tensor. If input tensor is allocated in dense non-overlapping memory, the output tensor strides will be copied from the input. Otherwise output strides will follow `torch.contiguous_format`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3 color=gray>Docs > </font>\n",
    "# Tensor Views <a href=\"https://pytorch.org/docs/stable/tensor_view.html\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>PyTorch allows a tensor to be a **`View`** of an existing tensor. <font color=maroon>View tensor shares the same underlying data with its base tensor.</font> Supporting **`View`** avoids explicit data copy, thus allows us to do fast and memory efficient reshaping, slicing and element-wise operations.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, to get a view of an existing tensor `t`, you can call `t.view(...)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.rand(4, 4)\n",
    "b = t.view(2, 8)\n",
    "\n",
    "# `t` and `b` share the same underlying data.\n",
    "t.storage().data_ptr() == b.storage().data_ptr()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.1400)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modifying view tensor changes base tensor as well.\n",
    "b[0][0] = 3.14\n",
    "t[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since views share underlying data with its base tensor, if you edit the data in the view, it will be reflected in the base tensor as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "Typically a PyTorch op returns a new tensor as output, e.g.  <a href=\"https://pytorch.org/docs/stable/generated/torch.Tensor.add.html\" style=\"text-decoration:none;font-size:110%\">torch.Tensor.add()</a>. \n",
    "\n",
    "But in case of view ops, outputs are views of input tensors to avoid unnecessary data copy. No data movement occurs when creating a view, view tensor just changes the way it interprets the same data. <font color=maroon>Taking a view of contiguous tensor could **potentially produce a non-contiguous tensor**. Users should be pay additional attention as contiguity might have **implicit performance impact**.</font> <a href=\"https://pytorch.org/docs/stable/generated/torch.Tensor.transpose.html\" style=\"text-decoration:none;font-size:110%\">torch.Tensor.transpose()</a> is a common example.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base = torch.tensor([[0, 1],[2, 3]])\n",
    "base.is_contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `t` is a view of `base`. No data movement happened here.\n",
    "t = base.transpose(0, 1)\n",
    "\n",
    "# View tensors might be non-contiguous.\n",
    "t.is_contiguous()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1)\n",
      "(1, 2)\n"
     ]
    }
   ],
   "source": [
    "print(base.stride())\n",
    "print(t.stride())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `t` and `base` share the same underlying data.\n",
    "t.storage().data_ptr() == base.storage().data_ptr()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1706612480256"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id(base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1706609599888"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To get a contiguous tensor, call `.contiguous()` to enforce\n",
    "# copying data when `t` is not contiguous.\n",
    "c = t.contiguous()\n",
    "c.stride()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `t` and `base` share the same underlying data.\n",
    "t.storage().data_ptr() == base.storage().data_ptr()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `t` and `base` share the same underlying data.\n",
    "t.storage().data_ptr() == c.storage().data_ptr()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1706612480256\n",
      "1706609599888\n",
      "1706620570896\n"
     ]
    }
   ],
   "source": [
    "print(id(base))  # 和前面 id(base) 一样\n",
    "print(id(t))     # 和前面 id(t) 一样\n",
    "print(id(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3><font color=maroon>For reference, here’s a full list of view ops in PyTorch:</font>\n",
    "\n",
    "* Basic slicing and indexing op, e.g. `tensor[0, 2:, 1:7:2]` returns a view of base tensor, see note below.\n",
    "<br>\n",
    "<br>\n",
    "* （略）\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<font size=3 color=red><b>NOTE: </b></font>\n",
    "\n",
    "When accessing the contents of a tensor via indexing, PyTorch follows Numpy behaviors that basic indexing returns views, while advanced indexing returns a copy. Assignment via either basic or advanced indexing is in-place. See more examples in <a href=\"https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html\" style=\"text-decoration:none;color:maroon;font-size:120%;\">Numpy indexing documentation</a>.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3 color=maroon>It’s also worth mentioning a few ops with special behaviors:</font>\n",
    "* <a href=\"https://pytorch.org/docs/stable/generated/torch.Tensor.reshape.html\" style=\"text-decoration:none;font-size:120%\">reshape()</a>, <a href=\"https://pytorch.org/docs/stable/generated/torch.Tensor.reshape_as.html\" style=\"text-decoration:none;font-size:120%\">reshape_as()</a> and <a href=\"https://pytorch.org/docs/stable/generated/torch.Tensor.flatten.html\" style=\"text-decoration:none;font-size:120%\">flatten()</a> can return either a view or new tensor, user code shouldn’t rely on whether it’s view or not.\n",
    "<br>\n",
    "<br>\n",
    "* <a href=\"https://pytorch.org/docs/stable/generated/torch.Tensor.contiguous.html\" style=\"text-decoration:none;font-size:120%\">contiguous()</a> returns **itself** if input tensor is already contiguous, otherwise it returns a new contiguous tensor by copying data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>For a more detailed walk-through of PyTorch internal implementation, please refer to <a href=\"http://blog.ezyang.com/2019/05/pytorch-internals/\" style=\"text-decoration:none;color:maroon;font-size:120%;\">ezyang’s blogpost about PyTorch Internals</a>.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3 color=gray>Docs > </font>\n",
    "# torch <a href=\"https://pytorch.org/docs/stable/torch.html\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>The torch package contains data structures for multi-dimensional tensors and defines mathematical operations over these tensors. Additionally, it provides many utilities for efficient serializing of Tensors and arbitrary types, and other useful utilities.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has a CUDA counterpart, that enables you to run your tensor computations on an NVIDIA GPU with compute capability >= 3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors    <a href=\"https://pytorch.org/docs/stable/torch.html#tensors\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch.numel() <a href=\"https://pytorch.org/docs/stable/generated/torch.numel.html\" style=\"text-decoration:none;font-size:70%\">[link]</a>\n",
    "#### torch.is_tensor() <a href=\"https://pytorch.org/docs/stable/generated/torch.is_tensor.html\" style=\"text-decoration:none;font-size:70%\">[link]</a>\n",
    "#### torch.is_storage() <a href=\"https://pytorch.org/docs/stable/generated/torch.is_storage.html\" style=\"text-decoration:none;font-size:70%\">[link]</a>\n",
    "#### torch.is_complex() <a href=\"https://pytorch.org/docs/stable/generated/torch.is_complex.html\" style=\"text-decoration:none;font-size:70%\">[link]</a>\n",
    "#### torch.is_floating_point() <a href=\"https://pytorch.org/docs/stable/generated/torch.is_floating_point.html\" style=\"text-decoration:none;font-size:70%\">[link]</a>\n",
    "#### torch.set_default_dtype() <a href=\"https://pytorch.org/docs/stable/generated/torch.set_default_dtype.html\" style=\"text-decoration:none;font-size:70%\">[link]</a>\n",
    "#### torch.set_printoptions() <a href=\"https://pytorch.org/docs/stable/generated/torch.set_printoptions.html\" style=\"text-decoration:none;font-size:70%\">[link]</a>\n",
    "#### torch.set_flush_denormal() <a href=\"https://pytorch.org/docs/stable/generated/torch.set_flush_denormal.html\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "。\n",
    "\n",
    "。\n",
    "\n",
    "。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation Ops <a href=\"https://pytorch.org/docs/stable/torch.html#creation-ops\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing, Slicing, Joining, Mutating Ops <a href=\"https://pytorch.org/docs/stable/torch.html#indexing-slicing-joining-mutating-ops\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generators <a href=\"https://pytorch.org/docs/stable/torch.html#generators\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random sampling <a href=\"https://pytorch.org/docs/stable/torch.html#random-sampling\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch.seed() <a href=\"https://pytorch.org/docs/stable/generated/torch.seed.html#torch.seed\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch.manual_seed(seed) <a href=\"https://pytorch.org/docs/stable/generated/torch.manual_seed.html#torch.manual_seed\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "。\n",
    "\n",
    "。\n",
    "\n",
    "。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=maroon>torch.default_generator</font> Returns the default CPU torch.Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch.bernoulli(input, *, generator=None, out=None) <a href=\"https://pytorch.org/docs/stable/generated/torch.bernoulli.html\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch.rand() <a href=\"https://pytorch.org/docs/stable/generated/torch.rand.html\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch.rand_like(input, *, dtype=None, ...) <a href=\"https://pytorch.org/docs/stable/generated/torch.rand_like.html\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch.normal(mean, std, *, generator=None, out=None) <a href=\"https://pytorch.org/docs/stable/generated/torch.normal.html\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch.randint(low=0, high, size, ...) <a href=\"https://pytorch.org/docs/stable/generated/torch.randint.html\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch.randn() <a href=\"https://pytorch.org/docs/stable/generated/torch.randn.html\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "。\n",
    "\n",
    "。\n",
    "\n",
    "。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-place random sampling <a href=\"https://pytorch.org/docs/stable/torch.html#in-place-random-sampling\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "。\n",
    "\n",
    "。\n",
    "\n",
    "。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quasi-random sampling <a href=\"https://pytorch.org/docs/stable/torch.html#quasi-random-sampling\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "。\n",
    "\n",
    "。\n",
    "\n",
    "。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=red>Serialization</font> <a href=\"https://pytorch.org/docs/stable/torch.html#serialization\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.save <a href=\"https://pytorch.org/docs/stable/generated/torch.save.html\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.load() <a href=\"https://pytorch.org/docs/stable/generated/torch.load.html\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=red>Parallelism</font> <a href=\"https://pytorch.org/docs/stable/torch.html#parallelism\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=red>Locally disabling gradient computation</font> <a href=\"https://pytorch.org/docs/stable/torch.html#locally-disabling-gradient-computation\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>The context managers <a href=\"https://pytorch.org/docs/stable/generated/torch.no_grad.html\" style=\"text-decoration:none;font-size:120%\">torch.no_grad()</a>, <a href=\"https://pytorch.org/docs/stable/generated/torch.enable_grad.html\" style=\"text-decoration:none;font-size:120%\">torch.enable_grad()</a>, and <a href=\"https://pytorch.org/docs/stable/generated/torch.set_grad_enabled.html\" style=\"text-decoration:none;font-size:120%\">torch.set_grad_enabled()</a> are helpful for locally disabling and enabling gradient computation. See <a href=\"https://pytorch.org/docs/stable/autograd.html#locally-disable-grad\" style=\"text-decoration:none;color:maroon;font-size:110%;\">Locally disabling gradient computation</a> for more details on their usage. These context managers are thread local, so they won’t work if you send work to another thread using the threading module, etc.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.zeros(1, requires_grad=True)\n",
    "with torch.no_grad():\n",
    "    y = x * 2\n",
    "y.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_train = False\n",
    "with torch.set_grad_enabled(is_train):\n",
    "    y = x * 2\n",
    "y.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_grad_enabled(True)  # this can also be used as a function\n",
    "y = x * 2\n",
    "y.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "y = x * 2\n",
    "y.requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.no_grad() <a href=\"https://pytorch.org/docs/stable/generated/torch.no_grad.html\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.enable_grad() <a href=\"https://pytorch.org/docs/stable/generated/torch.enable_grad.html\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.set_grad_enabled() <a href=\"https://pytorch.org/docs/stable/generated/torch.set_grad_enabled.html\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.is_grad_enabled() <a href=\"https://pytorch.org/docs/stable/generated/torch.is_grad_enabled.html\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.inference_mode() <a href=\"https://pytorch.org/docs/stable/generated/torch.inference_mode.html\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.is_inference_mode_enabled() <a href=\"https://pytorch.org/docs/stable/generated/torch.is_inference_mode_enabled.html\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Math operations <a href=\"https://pytorch.org/docs/stable/torch.html#math-operations\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pointwise Ops <a href=\"https://pytorch.org/docs/stable/torch.html#pointwise-ops\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduction Ops <a href=\"https://pytorch.org/docs/stable/torch.html#reduction-ops\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch.argmax() <a href=\"https://pytorch.org/docs/stable/generated/torch.argmax.html\" style=\"text-decoration:none;font-size:70%\">[link]</a>\n",
    "#### torch.amax() <a href=\"https://pytorch.org/docs/stable/generated/torch.amax.html\" style=\"text-decoration:none;font-size:70%\">[link]</a>\n",
    "#### torch.aminmax() <a href=\"https://pytorch.org/docs/stable/generated/torch.aminmax.html\" style=\"text-decoration:none;font-size:70%\">[link]</a>\n",
    "#### torch.all() <a href=\"https://pytorch.org/docs/stable/generated/torch.all.html\" style=\"text-decoration:none;font-size:70%\">[link]</a>\n",
    "#### torch.any() <a href=\"https://pytorch.org/docs/stable/generated/torch.any.html\" style=\"text-decoration:none;font-size:70%\">[link]</a>\n",
    "#### torch.max() <a href=\"https://pytorch.org/docs/stable/generated/torch.max.html\" style=\"text-decoration:none;font-size:70%\">[link]</a>\n",
    "#### torch.dist() <a href=\"https://pytorch.org/docs/stable/generated/torch.dist.html\" style=\"text-decoration:none;font-size:70%\">[link]</a>\n",
    "#### torch.logsumexp() <a href=\"https://pytorch.org/docs/stable/generated/torch.logsumexp.html\" style=\"text-decoration:none;font-size:70%\">[link]</a>\n",
    "#### torch.mean() <a href=\"https://pytorch.org/docs/stable/generated/torch.mean.html\" style=\"text-decoration:none;font-size:70%\">[link]</a>\n",
    "#### torch.nanmean() <a href=\"https://pytorch.org/docs/stable/generated/torch.nanmean.html\" style=\"text-decoration:none;font-size:70%\">[link]</a>\n",
    "#### torch.median() <a href=\"https://pytorch.org/docs/stable/generated/torch.median.html\" style=\"text-decoration:none;font-size:70%\">[link]</a>\n",
    "#### torch.nanmedian() <a href=\"https://pytorch.org/docs/stable/generated/torch.nanmedian.html\" style=\"text-decoration:none;font-size:70%\">[link]</a>\n",
    "#### torch.mode() <a href=\"https://pytorch.org/docs/stable/generated/torch.mode.html\" style=\"text-decoration:none;font-size:70%\">[link]</a>\n",
    "#### torch.norm() <a href=\"https://pytorch.org/docs/stable/generated/torch.norm.html\" style=\"text-decoration:none;font-size:70%\">[link]</a>\n",
    "#### torch.nansum() <a href=\"https://pytorch.org/docs/stable/generated/torch.nansum.html\" style=\"text-decoration:none;font-size:70%\">[link]</a>\n",
    "#### torch.prod() <a href=\"https://pytorch.org/docs/stable/generated/torch.prod.html\" style=\"text-decoration:none;font-size:70%\">[link]</a>\n",
    "#### torch.std() <a href=\"https://pytorch.org/docs/stable/generated/torch.std.html\" style=\"text-decoration:none;font-size:70%\">[link]</a>\n",
    "#### torch.std_mean() <a href=\"https://pytorch.org/docs/stable/generated/torch.std_mean.html\" style=\"text-decoration:none;font-size:70%\">[link]</a>\n",
    "#### torch.unique() <a href=\"https://pytorch.org/docs/stable/generated/torch.unique.html\" style=\"text-decoration:none;font-size:70%\">[link]</a>\n",
    "#### torch.var() <a href=\"https://pytorch.org/docs/stable/generated/torch.var.html\" style=\"text-decoration:none;font-size:70%\">[link]</a>\n",
    "#### torch.var_mean() <a href=\"https://pytorch.org/docs/stable/generated/torch.var_mean.html\" style=\"text-decoration:none;font-size:70%\">[link]</a>\n",
    "#### torch.count_nonzero() <a href=\"https://pytorch.org/docs/stable/generated/torch.count_nonzero.html\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "。\n",
    "\n",
    "。\n",
    "\n",
    "。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison Ops <a href=\"https://pytorch.org/docs/stable/torch.html#spectral-ops\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch.allclose() <a href=\"https://pytorch.org/docs/stable/generated/torch.allclose.html\" style=\"text-decoration:none;font-size:70%\">[link]</a>\n",
    "#### torch.argsort() <a href=\"https://pytorch.org/docs/stable/generated/torch.argsort.html\" style=\"text-decoration:none;font-size:70%\">[link]</a>\n",
    "#### torch.eq() <a href=\"https://pytorch.org/docs/stable/generated/torch.eq.html\" style=\"text-decoration:none;font-size:70%\">[link]</a>\n",
    "#### torch.equal() <a href=\"https://pytorch.org/docs/stable/generated/torch.equal.html\" style=\"text-decoration:none;font-size:70%\">[link]</a>\n",
    "#### torch.ge() <a href=\"https://pytorch.org/docs/stable/generated/torch.ge.html\" style=\"text-decoration:none;font-size:70%\">[link]</a>\n",
    "#### torch.greater_equal() <a href=\"https://pytorch.org/docs/stable/generated/torch.greater_equal.html\" style=\"text-decoration:none;font-size:70%\">[link]</a>\n",
    "#### torch.isnan() <a href=\"https://pytorch.org/docs/stable/generated/torch.isnan.html\" style=\"text-decoration:none;font-size:70%\">[link]</a>\n",
    "#### torch.sort() <a href=\"https://pytorch.org/docs/stable/generated/torch.sort.html\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "。\n",
    "\n",
    "。\n",
    "\n",
    "。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectral Ops <a href=\"https://pytorch.org/docs/stable/torch.html#spectral-ops\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Operations <a href=\"https://pytorch.org/docs/stable/torch.html#other-operations\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLAS and LAPACK Operations <a href=\"https://pytorch.org/docs/stable/torch.html#blas-and-lapack-operations\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities <a href=\"https://pytorch.org/docs/stable/torch.html#utilities\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3 color=gray>Docs > </font>\n",
    "# torch.Tensor <a href=\"https://pytorch.org/docs/stable/tensors.html\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A <a href=\"https://pytorch.org/docs/stable/tensors.html#torch.Tensor\" style=\"text-decoration:none;font-size:120%\">torch.Tensor</a> is a multi-dimensional matrix containing elements of a single data type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data types <a href=\"https://pytorch.org/docs/stable/tensors.html#data-types\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "。\n",
    "\n",
    "。\n",
    "\n",
    "。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://pytorch.org/docs/stable/tensors.html#torch.Tensor\" style=\"text-decoration:none;font-size:120%\">torch.Tensor</a> is an alias for the default tensor type (`torch.FloatTensor`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing and basic operations <a href=\"https://pytorch.org/docs/stable/tensors.html#initializing-and-basic-operations\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tensor can be constructed from a Python <a href=\"https://docs.python.org/3/library/stdtypes.html#list\" style=\"text-decoration:none;font-size:120%\">list</a> or sequence using the <a href=\"https://pytorch.org/docs/stable/generated/torch.tensor.html\" style=\"text-decoration:none;font-size:120%\">torch.tensor()</a> constructor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1., -1.],\n",
       "        [ 1., -1.]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[1., -1.], [1., -1.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]], dtype=torch.int32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(np.array([[1, 2, 3], [4, 5, 6]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "<font size=3 color=red><b>WARNING: </b></font>\n",
    "\n",
    "`torch.tensor()` always copies data. If you have a Tensor data and just want to change its ***requires_grad*** flag, use <a href=\"https://pytorch.org/docs/stable/generated/torch.Tensor.requires_grad_.html\" style=\"text-decoration:none;font-size:120%\">requires_grad_()</a> or <a href=\"https://pytorch.org/docs/stable/generated/torch.Tensor.detach.html\" style=\"text-decoration:none;font-size:120%\">detach()</a> to avoid a copy. If you have a numpy array and want to avoid a copy, use <a href=\"https://pytorch.org/docs/stable/generated/torch.as_tensor.html\" style=\"text-decoration:none;font-size:120%\">torch.as_tensor()</a>.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tensor of specific data type can be constructed by passing a <a href=\"https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype\" style=\"text-decoration:none;font-size:120%\">torch.dtype</a> and/or a <a href=\"https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.device\" style=\"text-decoration:none;font-size:120%\">torch.device</a> to a constructor or tensor creation op:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0],\n",
       "        [0, 0, 0, 0]], dtype=torch.int32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros([2, 4], dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]], device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuda0 = torch.device('cuda:0')\n",
    "torch.ones([2, 4], dtype=torch.float64, device=cuda0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>For more information about building Tensors, see <a href=\"https://pytorch.org/docs/stable/torch.html#tensor-creation-ops\" style=\"text-decoration:none;color:maroon;font-size:120%;\">Creation Ops</a></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The contents of a tensor can be accessed and modified using Python’s indexing and slicing notation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6)\n",
      "tensor([[1, 8, 3],\n",
      "        [4, 5, 6]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "print(x[1][2])\n",
    "\n",
    "x[0][1] = 8\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use <a href=\"https://pytorch.org/docs/stable/generated/torch.Tensor.item.html#torch.Tensor.item\" style=\"text-decoration:none;font-size:120%\">torch.Tensor.item()</a> to get a Python number from a tensor containing a single value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[1]])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.5000)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(2.5)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>For more information about indexing, see <a href=\"https://pytorch.org/docs/stable/torch.html#indexing-slicing-joining\" style=\"text-decoration:none;color:maroon;font-size:120%;\">Indexing, Slicing, Joining, Mutating Ops</a></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tensor can be created with ***requires_grad=True*** so that <a href=\"https://pytorch.org/docs/stable/autograd.html#module-torch.autograd\" style=\"text-decoration:none;font-size:120%\">torch.autograd</a> records operations on them for automatic differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2., -2.],\n",
       "        [ 2.,  2.]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[1., -1.], [1., 1.]], requires_grad=True)\n",
    "out = x.pow(2).sum()\n",
    "out.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each tensor has an associated `torch.Storage`, which holds its data. The tensor class also provides multi-dimensional, <a href=\"https://en.wikipedia.org/wiki/Stride_of_an_array\" style=\"text-decoration:none;color:maroon;font-size:120%;\">strided</a> view of a storage and defines numeric operations on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<font size=3 color=red><b>NOTE: </b></font>\n",
    "\n",
    "For more information on tensor views, see <a href=\"https://pytorch.org/docs/stable/tensor_view.html\" style=\"text-decoration:none;color:maroon;font-size:120%;\">Tensor Views</a>.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<font size=3 color=red><b>NOTE: </b></font>\n",
    "\n",
    "For more information on the `torch.dtype`, `torch.device`, and `torch.layout` attributes of a <a href=\"https://pytorch.org/docs/stable/tensors.html#torch.Tensor\" style=\"text-decoration:none;font-size:120%;color:blue;\">torch.Tensor</a>, see <a href=\"https://pytorch.org/docs/stable/tensor_attributes.html\" style=\"text-decoration:none;color:maroon;font-size:120%;\">Tensor Attributes</a>.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<font size=3 color=red><b>NOTE: </b></font>\n",
    "\n",
    "Methods which mutate a tensor are marked with an <font color=red>underscore suffix</font>. For example, `torch.FloatTensor.abs_()` computes the absolute value in-place and returns the modified tensor, while `torch.FloatTensor.abs()` computes the result in a new tensor.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<font size=3 color=red><b>NOTE: </b></font>\n",
    "\n",
    "To change an existing tensor’s `torch.device` and/or `torch.dtype`, consider using <a href=\"https://pytorch.org/docs/stable/generated/torch.Tensor.to.html\" style=\"text-decoration:none;font-size:120%;color:blue;\">to()</a> method on the tensor.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "<font size=3 color=red><b>WARNING: </b></font>\n",
    "\n",
    "Current implementation of <a href=\"https://pytorch.org/docs/stable/tensors.html#torch.Tensor\" style=\"text-decoration:none;font-size:120%\">torch.Tensor</a> introduces memory overhead, thus it might lead to unexpectedly high memory usage in the applications with many tiny tensors. If this is your case, consider using one large structure.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor class reference <a href=\"https://pytorch.org/docs/stable/tensors.html#tensor-class-reference\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<font size=3 color=gray><b>CLASS</b></font>&emsp;\n",
    "<font size=4><b>torch.Tensor</b></font>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>There are a few main ways to create a tensor, depending on your use case.\n",
    "<br>\n",
    "<br>\n",
    "* To create a tensor with pre-existing data, use <a href=\"https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor\" style=\"text-decoration:none;font-size:120%\">torch.tensor()</a>.\n",
    "<br>\n",
    "<br>\n",
    "* To create a tensor with specific size, use `torch.*` tensor creation ops (see <a href=\"https://pytorch.org/docs/stable/torch.html#tensor-creation-ops\" style=\"text-decoration:none;color:maroon;font-size:100%;\">Creation Ops</a>).\n",
    "<br>\n",
    "    <br>\n",
    "* To create a tensor with the same size (and similar types) as another tensor, use `torch.*_like` tensor creation ops (see <a href=\"https://pytorch.org/docs/stable/torch.html#tensor-creation-ops\" style=\"text-decoration:none;color:maroon;font-size:100%;\">Creation Ops</a>).\n",
    "<br>\n",
    "<br>\n",
    "* To create a tensor with similar type but different size as another tensor, use `tensor.new_*` creation ops.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<font size=4><b>Tensor.T</b></font>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns a view of this tensor with its dimensions reversed.\n",
    "\n",
    "If `n` is the number of dimensions in `x`, `x.T` is equivalent to `x.permute(n-1, n-2, ..., 0)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "<font size=3 color=red><b>WARNING: </b></font>\n",
    "\n",
    "The use of <a href=\"https://pytorch.org/docs/stable/tensors.html#torch.Tensor.T\" style=\"text-decoration:none;font-size:120%\">Tensor.T()</a> on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider <a href=\"https://pytorch.org/docs/stable/tensors.html#torch.Tensor.mT\" style=\"text-decoration:none;font-size:120%\">mT</a> to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<font size=4><b>Tensor.H</b></font>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns a view of a matrix (2-D tensor) conjugated and transposed.\n",
    "\n",
    "`x.H` is equivalent to `x.transpose(0, 1).conj()` for complex matrices and `x.transpose(0, 1)` for real matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SEE ALSO: <br>\n",
    "<a href=\"https://pytorch.org/docs/stable/tensors.html#torch.Tensor.mH\" style=\"text-decoration:none;font-size:120%\">mH</a>: An attribute that also works on batches of matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<font size=4><b>Tensor.mT</b></font>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns a view of this tensor with the last two dimensions transposed.\n",
    "\n",
    "`x.mT` is equivalent to `x.transpose(-2, -1)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<font size=4><b>Tensor.mH</b></font>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing this property is equivalent to calling <a href=\"https://pytorch.org/docs/stable/generated/torch.adjoint.html#torch.adjoint\" style=\"text-decoration:none;font-size:120%\">adjoint()</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color=maroon>(原网址此处有一张 Tensor.** 的函数表，非常长)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (无)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3 color=gray>Docs > torch.Tensor > </font>\n",
    "## torch.Tensor.view <a href=\"https://pytorch.org/docs/stable/generated/torch.Tensor.view.html\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### view(*shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<font size=4><b>Tensor.view(*shape)</b> → Tensor</font>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>Returns <font color=maroon>a **new** tensor with the **same** data</font> as the `self` tensor but of a different `shape`.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>The returned tensor shares the same data and must have the same number of elements, but may have a different size. For a tensor to be viewed, the new view size must be compatible with its original size and stride, i.e., \n",
    "<br>\n",
    "<br>\n",
    "* each new view dimension must either be a ***subspace*** of an original dimension, \n",
    "<br>\n",
    "<br>\n",
    "* or only **span** across original dimensions <font size=4 color=maroon>$d, d+1, \\dots, d+k$</font> that satisfy the following contiguity-like condition that <font size=4 color=maroon>$\\forall i = d, \\dots, d+k-1$</font>,\n",
    "<br>\n",
    "<br>\n",
    "<font size=5 color=maroon>$$stride[i]=stride[i+1]×size[i+1]$$</font>\n",
    "\n",
    "Otherwise, it will not be possible to view `self` tensor as `shape` without copying it (e.g., via <a href=\"https://pytorch.org/docs/stable/generated/torch.Tensor.contiguous.html\" style=\"text-decoration:none;font-size:120%\">contiguous()</a>).\n",
    "\n",
    "\n",
    "When it is unclear whether a `view()` can be performed, it is advisable to use <a href=\"https://pytorch.org/docs/stable/generated/torch.reshape.html\" style=\"text-decoration:none;font-size:120%\">reshape()</a>, which returns a view if the shapes are compatible, and copies (equivalent to calling `contiguous()`) otherwise.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters**\n",
    "* `shape` (torch.Size or int...) – the desired size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(4, 4)\n",
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x.view(16)\n",
    "y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = x.view(-1, 8)  # the size -1 is inferred from other dimensions\n",
    "z.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `x`、`y` and `z` share the same underlying data.\n",
    "x.storage().data_ptr() == y.storage().data_ptr() == z.storage().data_ptr()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, False, False)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.equal(x, y), torch.equal(x, z), torch.equal(y, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 3, 4])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(1, 2, 3, 4)\n",
    "a.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 2, 4])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a.transpose(1, 2)  # Swaps 2nd and 3rd dimension\n",
    "b.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 2, 4])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = a.view(1, 3, 2, 4)  # Does not change tensor layout in memory\n",
    "c.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, False, False)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.equal(b, c), torch.equal(a, b), torch.equal(a, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `a`、`b` and `c` share the same underlying data.\n",
    "a.storage().data_ptr() == b.storage().data_ptr() == c.storage().data_ptr()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1706614840752\n",
      "1706620448176\n",
      "1706561625840\n"
     ]
    }
   ],
   "source": [
    "print(id(a))\n",
    "print(id(b))\n",
    "print(id(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### view(dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<font size=4><b>Tensor.view(dtype)</b> → Tensor</font>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>Returns <font color=maroon>a **new** tensor with the **same** data</font> as the `self` tensor but of a different `dtype`.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(略)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3 color=gray>Docs > torch.Tensor > </font>\n",
    "##  <font color=red>torch.Tensor.detach</font> <a href=\"https://pytorch.org/docs/stable/generated/torch.Tensor.detach.html\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<font size=4><b>Tensor.detach()</b></font>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>Returns a new Tensor, detached from the current graph.\n",
    "\n",
    "The result will never require gradient.\n",
    "\n",
    "This method also affects forward mode AD gradients and the result will never have forward mode AD gradients.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<font size=3 color=red><b>NOTE: </b></font>\n",
    "\n",
    "Returned Tensor shares the same storage with the original one. In-place modifications on either of them will be seen, and may trigger errors in correctness checks. \n",
    "<br>\n",
    "<br>\n",
    "**`IMPORTANT NOTE:`** \n",
    "\n",
    "Previously, in-place size / stride / storage changes (such as `resize_ / resize_as_ / set_ / transpose_`) to the returned tensor also update the original tensor. <font color=maroon>Now, these in-place changes will not update the original tensor anymore, and will instead trigger an error.</font> \n",
    "\n",
    "For sparse tensors: In-place indices / values changes (such as `zero_ / copy_ / add_`) to the returned tensor <font color=maroon>will not update the original tensor anymore, and will instead trigger an error.</font>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3 color=gray>Docs > torch.Tensor > </font>\n",
    "##  torch.Tensor.register_hook <a href=\"https://pytorch.org/docs/stable/generated/torch.Tensor.register_hook.html\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<font size=4><b>Tensor.register_hook</b>(hook)</font>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>Registers a backward hook.\n",
    "\n",
    "The hook will be called every time a gradient with respect to the Tensor is computed. The hook should have the following signature:</font>\n",
    "\n",
    "<font size=4>$$hook(grad) → Tensor \\ or \\ None$$</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<font size=3>The hook should not modify its argument, but it can optionally return a new gradient which will be used in place of <a href=\"https://pytorch.org/docs/stable/generated/torch.Tensor.grad.html\" style=\"text-decoration:none;font-size:120%\">grad</a>.\n",
    "\n",
    "This function returns a handle with a method <font color=blue>***handle.remove()***</font> that removes the hook from the module.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 4., 6.])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = torch.tensor([0., 0., 0.], requires_grad=True)\n",
    "h = v.register_hook(lambda grad: grad * 2)    # double the gradient\n",
    "v.backward(torch.tensor([1., 2., 3.]))\n",
    "v.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "h.remove()  # removes the hook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<font size=4><b> torch.Tensor.zero_</b>() → Tensor</font>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>Fills `self` tensor with zeros.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3 color=gray>Docs > </font>\n",
    "#  torch.nn.init <a href=\"https://pytorch.org/docs/stable/nn.init.html\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<font size=4><b> torch.nn.init.calculate_gain</b>(nonlinearity, param=None)</font>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return the recommended gain value for the given nonlinearity function. The values are as follows: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./1 PyTorch documentation/1 Notes/images/init-calculate_gain.jpeg\" width=600px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "<font size=3 color=red><b>WARNING: </b></font>\n",
    "\n",
    "In order to implement <a href=\"https://papers.nips.cc/paper/2017/hash/5d44ee6f2c3f71b73125876103c8f6c4-Abstract.html\" style=\"text-decoration:none;color:maroon;font-size:120%;\">Self-Normalizing Neural Networks</a> , you should use `nonlinearity='linear'` instead of `nonlinearity='selu'`. This gives the initial weights a variance of `1 / N`, which is necessary to induce a stable fixed point in the forward pass. In contrast, the default gain for `SELU` sacrifices the normalisation effect for more stable gradient flow in rectangular layers.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters**\n",
    "* `nonlinearity` – the non-linear function (*nn.functional* name)\n",
    "\n",
    "* `param` – optional parameter for the non-linear function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3867504905630728"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gain = nn.init.calculate_gain('leaky_relu', 0.2)  # leaky_relu with negative_slope=0.2\n",
    "gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6666666666666667"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.init.calculate_gain('tanh')   # 'Tanh' 为无效参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<font size=4><b> torch.nn.init.uniform_</b>(tensor, a=0.0, b=1.0)</font>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>Fills the input Tensor with values drawn from the uniform distribution $U(a, b)$.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0194e-38, 1.0469e-38, 1.0010e-38, 8.4490e-39, 1.0102e-38],\n",
       "        [9.0919e-39, 1.0102e-38, 8.9082e-39, 8.4489e-39, 1.0102e-38],\n",
       "        [1.0561e-38, 1.0286e-38, 9.4592e-39, 9.9184e-39, 9.0000e-39]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.empty(3, 5)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7576, 0.2793, 0.4031, 0.7347, 0.0293],\n",
       "        [0.7999, 0.3971, 0.7544, 0.5695, 0.4388],\n",
       "        [0.6387, 0.5247, 0.6826, 0.3051, 0.4635]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "nn.init.uniform_(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7576, 0.2793, 0.4031, 0.7347, 0.0293],\n",
       "        [0.7999, 0.3971, 0.7544, 0.5695, 0.4388],\n",
       "        [0.6387, 0.5247, 0.6826, 0.3051, 0.4635]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<font size=4><b> torch.nn.init.normal_</b>(tensor, mean=0.0, std=1.0)</font>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>Fills the input Tensor with values drawn from the normal distribution $\\mathcal{N}(\\text{mean}, \\text{std}^2)$.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(略)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<font size=4><b> torch.nn.init.constant_</b>(tensor, val)</font>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3>Fills the input Tensor with the value $\\text{val}$.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(略)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(还有其它一些函数，详见原网址)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3 color=gray>Docs > </font>\n",
    "#  torch.nn <a href=\"https://pytorch.org/docs/stable/nn.html\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html\" style=\"text-decoration:none;font-size:120%\">Parameter</a> &emsp; A kind of Tensor that is to be considered a module parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://pytorch.org/docs/stable/generated/torch.nn.parameter.UninitializedParameter.html\" style=\"text-decoration:none;font-size:120%\">UninitializedParameter</a> &emsp; A parameter that is not initialized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://pytorch.org/docs/stable/generated/torch.nn.parameter.UninitializedBuffer.html\" style=\"text-decoration:none;font-size:120%\">UninitializedBuffer</a> &emsp; A buffer that is not initialized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"font-size:110%\">These are the basic building blocks for graphs:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a href=\"https://pytorch.org/docs/stable/nn.html#containers\" style=\"text-decoration:none;font-size:110%\">Containers</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a href=\"https://pytorch.org/docs/stable/nn.html#convolution-layers\" style=\"text-decoration:none;font-size:110%\">Convolution Layers</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a href=\"https://pytorch.org/docs/stable/nn.html#pooling-layers\" style=\"text-decoration:none;font-size:110%\">Pooling layers</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a href=\"https://pytorch.org/docs/stable/nn.html#padding-layers\" style=\"text-decoration:none;font-size:110%\">Padding Layers</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a href=\"https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity\" style=\"text-decoration:none;font-size:110%\">Non-linear Activations (weighted sum, nonlinearity)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a href=\"https://pytorch.org/docs/stable/nn.html#non-linear-activations-other\" style=\"text-decoration:none;font-size:110%\">Non-linear Activations (other)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a href=\"https://pytorch.org/docs/stable/nn.html#normalization-layers\" style=\"text-decoration:none;font-size:110%\">Normalization Layers</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;&emsp;&ensp;\n",
    "<a href=\"https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html\" style=\"text-decoration:none;font-size:140%;color:maroon;font-weight:bold;\">nn.BatchNorm2d</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a href=\"https://pytorch.org/docs/stable/nn.html#recurrent-layers\" style=\"text-decoration:none;font-size:110%\">Recurrent Layers</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a href=\"https://pytorch.org/docs/stable/nn.html#transformer-layers\" style=\"text-decoration:none;font-size:110%\">Transformer Layers</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a href=\"https://pytorch.org/docs/stable/nn.html#linear-layers\" style=\"text-decoration:none;font-size:110%\">Linear Layers</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a href=\"https://pytorch.org/docs/stable/nn.html#dropout-layers\" style=\"text-decoration:none;font-size:110%\">Dropout Layers</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;&emsp;&ensp;\n",
    "<a href=\"https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html\" style=\"text-decoration:none;color:maroon;font-size:140%;font-weight:bold;\">nn.Dropout</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;&emsp;&ensp;\n",
    "<a href=\"https://pytorch.org/docs/stable/generated/torch.nn.Dropout2d.html\" style=\"text-decoration:none;color:maroon;font-size:140%;font-weight:bold;\">nn.Dropout2d</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a href=\"https://pytorch.org/docs/stable/nn.html#sparse-layers\" style=\"text-decoration:none;font-size:110%\">Sparse Layers</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a href=\"https://pytorch.org/docs/stable/nn.html#distance-functions\" style=\"text-decoration:none;font-size:110%\">Distance Functions</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a href=\"https://pytorch.org/docs/stable/nn.html#loss-functions\" style=\"text-decoration:none;font-size:110%\">Loss Functions</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a href=\"https://pytorch.org/docs/stable/nn.html#vision-layers\" style=\"text-decoration:none;font-size:110%\">Vision Layers</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a href=\"https://pytorch.org/docs/stable/nn.html#shuffle-layers\" style=\"text-decoration:none;font-size:110%\">Shuffle Layers</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a href=\"https://pytorch.org/docs/stable/nn.html#dataparallel-layers-multi-gpu-distributed\" style=\"text-decoration:none;font-size:110%\">DataParallel Layers (multi-GPU, distributed)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a href=\"https://pytorch.org/docs/stable/nn.html#utilities\" style=\"text-decoration:none;font-size:110%\">Utilities</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a href=\"https://pytorch.org/docs/stable/nn.html#quantized-functions\" style=\"text-decoration:none;font-size:110%\">Quantized Functions</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a href=\"https://pytorch.org/docs/stable/nn.html#lazy-modules-initialization\" style=\"text-decoration:none;font-size:110%\">Lazy Modules Initialization</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch.nn.Module <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.Module.html\" style=\"text-decoration:none;font-size:70%\">[link]</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3 color=gray>Docs > torch.nn > </font>\n",
    "<font size=4>**Module**</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***(待完善)***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.nn.Module.train\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.nn.Module.eval\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.eval"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<font size=3></font>\n",
    "\n",
    "<a href=\"\" style=\"text-decoration:none;font-size:70%\">[link]</a>\n",
    "<a href=\"\" style=\"text-decoration:none;font-size:120%\"></a>\n",
    "\n",
    "\n",
    "<a href=\"\" style=\"text-decoration:none;color:maroon;font-size:120%;\"></a>\n",
    "\n",
    "&emsp;&emsp;&emsp;&ensp;\n",
    "<a href=\"\" style=\"text-decoration:none;color:maroon;font-size:140%;font-weight:bold;\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ptg",
   "language": "python",
   "name": "ptg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "274px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
