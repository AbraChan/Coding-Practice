{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=center><font color=maroon size=6><b>Training and evaluation with the built-in methods</b></font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4><b>References:</b></font>\n",
    "1. TF2 Core: <a href=\"https://www.tensorflow.org/guide\" style=\"text-decoration:none;\">TensorFlow Guide</a> \n",
    "    * `TensorFlow > Learn > TensorFlow Core > `Guide > <a href=\"https://www.tensorflow.org/guide/keras/train_and_evaluate\" style=\"text-decoration:none;\">Training and evaluation with the built-in methods</a>\n",
    "        * Run in <a href=\"https://colab.research.google.com/github/tensorflow/docs/blob/snapshot-keras/site/en/guide/keras/train_and_evaluate.ipynb\" style=\"text-decoration:none;\">Google Colab</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "<font size=3 color=maroon>This guide covers **training**, **evaluation**, and **prediction (inference)** models when using built-in APIs for training & validation (such as `Model.fit()`, `Model.evaluate()` and `Model.predict()`).\n",
    "\n",
    "* If you are interested in leveraging `fit()` while specifying your own training step function, see the\n",
    "[Customizing what happens in `fit()` guide](https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit/).\n",
    "\n",
    "* If you are interested in writing your own training & evaluation loops from scratch, see the guide\n",
    "[\"writing a training loop from scratch\"](https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch/).\n",
    "\n",
    "In general, whether you are using built-in loops or writing your own, model training & evaluation works strictly in the same way across every kind of Keras model -- Sequential models, models built with the Functional API, and models written from scratch via model subclassing.</font>\n",
    "\n",
    "This guide doesn't cover distributed training, which is covered in our\n",
    "[guide to multi-GPU & distributed training](https://keras.io/guides/distributed_training/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API overview: a first end-to-end example\n",
    "\n",
    "<font size=3 color=maroon>When passing data to the built-in training loops of a model, you should either use **NumPy arrays** (if your data is small and fits in memory) or **`tf.data Dataset` objects**. </font>\n",
    "\n",
    "In the next few paragraphs, we'll use the MNIST dataset as NumPy arrays, in order to demonstrate how to use optimizers, losses, and metrics.\n",
    "\n",
    "Let's consider the following model (here, we build in with the Functional API, but it could be a Sequential model or a subclassed model as well):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(784, ), name='digits')\n",
    "x = layers.Dense(64, activation='relu', name='dense_1')(inputs)\n",
    "x = layers.Dense(64, activation='relu', name='dense_2')(x)\n",
    "outputs = layers.Dense(10, activation='softmax', name='predictions')(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3 color=maroon>Here's what the typical end-to-end workflow looks like, consisting of:\n",
    "\n",
    "- Training\n",
    "- Validation on a holdout set generated from the original training data\n",
    "- Evaluation on the test data</font>\n",
    "\n",
    "We'll use MNIST data for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Preprocess the data (these are NumPy arrays)\n",
    "x_train = x_train.reshape(60000, 784).astype(\"float32\") / 255\n",
    "x_test = x_test.reshape(10000, 784).astype(\"float32\") / 255\n",
    "\n",
    "y_train = y_train.astype(\"float32\")\n",
    "y_test = y_test.astype(\"float32\")\n",
    "\n",
    "# Reserve 10,000 samples for validation\n",
    "x_val = x_train[-10000:]\n",
    "y_val = y_train[-10000:]\n",
    "x_train = x_train[:-10000]\n",
    "y_train = y_train[:-10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We specify the training configuration (optimizer, loss, metrics):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=keras.optimizers.RMSprop(),  # Optimizer\n",
    "              # Loss function to minimize\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "              # List of metrics to monitor\n",
    "              metrics=[keras.metrics.SparseCategoricalAccuracy()],)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3 color=maroon>We call `fit()`, which will train the model by slicing the data into \"batches\" of size `batch_size`, and repeatedly iterating over the entire dataset for a given number of `epochs`.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit model on training data\n",
      "Epoch 1/2\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 0.3496 - sparse_categorical_accuracy: 0.8993 - val_loss: 0.2051 - val_sparse_categorical_accuracy: 0.9399\n",
      "Epoch 2/2\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.1624 - sparse_categorical_accuracy: 0.9515 - val_loss: 0.1571 - val_sparse_categorical_accuracy: 0.9539\n"
     ]
    }
   ],
   "source": [
    "print(\"Fit model on training data\")\n",
    "history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    batch_size=64,\n",
    "                    epochs=2,\n",
    "                    # We pass some validation for\n",
    "                    # monitoring validation loss and metrics\n",
    "                    # at the end of each epoch\n",
    "                    validation_data=(x_val, y_val),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3 color=maroon>The returned `history` object holds a record of the loss values and metric values during training:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.34955042600631714, 0.16235584020614624],\n",
       " 'sparse_categorical_accuracy': [0.8992999792098999, 0.951479971408844],\n",
       " 'val_loss': [0.20511212944984436, 0.15706193447113037],\n",
       " 'val_sparse_categorical_accuracy': [0.9398999810218811, 0.9538999795913696]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3 color=maroon>We evaluate the model on the test data via `evaluate()`:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on test data\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.1576 - sparse_categorical_accuracy: 0.9516\n",
      "test loss, test acc: [0.15757670998573303, 0.9516000151634216]\n",
      "Generate predictions for 3 samples\n",
      "predictions shape: (3, 10)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data using `evaluate`\n",
    "print(\"Evaluate on test data\")\n",
    "results = model.evaluate(x_test, y_test, batch_size=128)\n",
    "print(\"test loss, test acc:\", results)\n",
    "\n",
    "# Generate predictions (probabilities -- the output of the last layer)\n",
    "# on new data using `predict`\n",
    "print(\"Generate predictions for 3 samples\")\n",
    "predictions = model.predict(x_test[:3])\n",
    "print(\"predictions shape:\", predictions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's review each piece of this workflow in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `compile()` method: specifying a loss, metrics, and an optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train a model with `fit()`, you need to specify a loss function, an optimizer, and\n",
    "optionally, some metrics to monitor.\n",
    "\n",
    "You pass these to the model as arguments to the `compile()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=[keras.metrics.SparseCategoricalAccuracy()],)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3 color=maroon>The `metrics` argument should be a list -- your model can have any number of metrics.\n",
    "\n",
    "If your model has multiple outputs, you can specify different losses and metrics for each output, and you can modulate the contribution of each output to the total loss of the model.</font> You will find more details about this in the **Passing data to multi-input, multi-output models** section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3 color=maroon>**Note that** if you're satisfied with the default settings, in many cases the optimizer, loss, and metrics can be specified via string identifiers as a shortcut:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"sparse_categorical_accuracy\"],)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For later reuse, let's put our model definition and compile step in functions; we will call them several times across different examples in this guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uncompiled_model():\n",
    "    inputs = keras.Input(shape=(784,), name=\"digits\")\n",
    "    x = layers.Dense(64, activation=\"relu\", name=\"dense_1\")(inputs)\n",
    "    x = layers.Dense(64, activation=\"relu\", name=\"dense_2\")(x)\n",
    "    outputs = layers.Dense(10, activation=\"softmax\", name=\"predictions\")(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_compiled_model():\n",
    "    model = get_uncompiled_model()\n",
    "    model.compile(optimizer=\"rmsprop\",\n",
    "                  loss=\"sparse_categorical_crossentropy\",\n",
    "                  metrics=[\"sparse_categorical_accuracy\"],)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Many built-in optimizers, losses, and metrics are available\n",
    "\n",
    "In general, you won't have to create your own losses, metrics, or optimizers from scratch, because what you need is likely to be already part of the Keras API:\n",
    "\n",
    "Optimizers:\n",
    "\n",
    "- `SGD()` (with or without momentum)\n",
    "- `RMSprop()`\n",
    "- `Adam()`\n",
    "- etc.\n",
    "\n",
    "Losses:\n",
    "\n",
    "- `MeanSquaredError()`\n",
    "- `KLDivergence()`\n",
    "- `CosineSimilarity()`\n",
    "- etc.\n",
    "\n",
    "Metrics:\n",
    "\n",
    "- `AUC()`\n",
    "- `Precision()`\n",
    "- `Recall()`\n",
    "- etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom losses\n",
    "\n",
    "<font size=3 color=maroon>If you need to create a custom loss, Keras provides two ways to do so.\n",
    "\n",
    "The first method involves creating a function that accepts inputs `y_true` and `y_pred`.</font> The following example shows a loss function that computes the mean squared error between the real data and the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0162\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1555876d7f0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def custom_mean_squared_error(y_true, y_pred):\n",
    "    \n",
    "    return tf.math.reduce_mean(tf.square(y_true - y_pred))\n",
    "\n",
    "\n",
    "model = get_uncompiled_model()\n",
    "model.compile(optimizer=keras.optimizers.Adam(), loss=custom_mean_squared_error)\n",
    "\n",
    "# We need to one-hot encode the labels to use MSE\n",
    "y_train_one_hot = tf.one_hot(y_train, depth=10)\n",
    "model.fit(x_train, y_train_one_hot, batch_size=64, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5. 0. 4. ... 8. 4. 8.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(50000,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(y_train)\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(50000, 10), dtype=float32, numpy=\n",
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.one_hot(y_train, depth=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3 color=maroon>If you need a loss function that takes in parameters beside `y_true` and `y_pred`, you can subclass the `tf.keras.losses.Loss` class and implement the following two methods:\n",
    "\n",
    "- `__init__(self)`: accept parameters to pass during the call of your loss function\n",
    "- `call(self, y_true, y_pred)`: use the targets (y_true) and the model predictions (y_pred) to compute the model's loss</font>\n",
    "\n",
    "Let's say you want to use mean squared error, but with an added term that will de-incentivize  prediction values far from 0.5 (we assume that the categorical targets are one-hot encoded and take values between 0 and 1). This creates an incentive for the model not to be too confident, which may help reduce overfitting (we won't know if it works until we try!).\n",
    "\n",
    "Here's how you would do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0387\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x155531ca8e0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CustomMSE(keras.losses.Loss):\n",
    "    def __init__(self, regularization_factor=0.1, name=\"custom_mse\"):\n",
    "        super().__init__(name=name)\n",
    "        self.regularization_factor = regularization_factor\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        mse = tf.math.reduce_mean(tf.square(y_true - y_pred))\n",
    "        reg = tf.math.reduce_mean(tf.square(0.5 - y_pred))\n",
    "        return mse + reg * self.regularization_factor\n",
    "\n",
    "\n",
    "model = get_uncompiled_model()\n",
    "model.compile(optimizer=keras.optimizers.Adam(), loss=CustomMSE())\n",
    "\n",
    "y_train_one_hot = tf.one_hot(y_train, depth=10)\n",
    "model.fit(x_train, y_train_one_hot, batch_size=64, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom metrics\n",
    "\n",
    "<font size=3 color=maroon>If you need a metric that isn't part of the API, you can easily create custom metrics by subclassing the `tf.keras.metrics.Metric` class. You will need to implement 4 methods:\n",
    "\n",
    "- `__init__(self)`, in which you will create state variables for your metric.\n",
    "- `update_state(self, y_true, y_pred, sample_weight=None)`, which uses the targets y_true and the model predictions y_pred to update the state variables.\n",
    "- `result(self)`, which uses the state variables to compute the final results.\n",
    "- `reset_state(self)`, which reinitializes the state of the metric.\n",
    "\n",
    "State update and results computation are kept separate (in `update_state()` and `result()`, respectively) because in some cases, the results computation might be very expensive and would only be done periodically.</font>\n",
    "\n",
    "Here's a simple example showing how to implement a `CategoricalTruePositives` metric that counts how many samples were correctly classified as belonging to a given class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.3404 - categorical_true_positives: 45123.0000\n",
      "Epoch 2/3\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1576 - categorical_true_positives: 47660.0000\n",
      "Epoch 3/3\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1143 - categorical_true_positives: 48304.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x155882dda30>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CategoricalTruePositives(keras.metrics.Metric):\n",
    "    def __init__(self, name=\"categorical_true_positives\", **kwargs):\n",
    "        super(CategoricalTruePositives, self).__init__(name=name, **kwargs)\n",
    "        self.true_positives = self.add_weight(name=\"ctp\", initializer=\"zeros\")\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred = tf.reshape(tf.argmax(y_pred, axis=1), shape=(-1, 1))\n",
    "        values = tf.cast(y_true, \"int32\") == tf.cast(y_pred, \"int32\")\n",
    "        values = tf.cast(values, \"float32\")\n",
    "        \n",
    "        if sample_weight is not None:\n",
    "            sample_weight = tf.cast(sample_weight, \"float32\")\n",
    "            values = tf.multiply(values, sample_weight)\n",
    "        \n",
    "        self.true_positives.assign_add(tf.reduce_sum(values))\n",
    "\n",
    "    \n",
    "    def result(self):\n",
    "        \n",
    "        return self.true_positives\n",
    "\n",
    "    \n",
    "    def reset_state(self):\n",
    "        # The state of the metric will be reset at the start of each epoch.\n",
    "        self.true_positives.assign(0.0)\n",
    "\n",
    "\n",
    "model = get_uncompiled_model()\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=[CategoricalTruePositives()],)\n",
    "model.fit(x_train, y_train, batch_size=64, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling losses and metrics that don't fit the standard signature\n",
    "\n",
    "The overwhelming majority of losses and metrics can be computed from `y_true` and `y_pred`, where `y_pred` is an output of your model -- <font size=3 color=maroon>but not all of them. For instance, a regularization loss may only require the activation of a layer (there are no targets in this case), and this activation may not be a model output.\n",
    "\n",
    "In such cases, you can call `self.add_loss(loss_value)` from inside the call method of a **custom layer**. Losses added in this way get added to the \"main\" loss during training (the one passed to `compile()`).</font> Here's a simple example that adds activity regularization (**note that** activity regularization is built-in in all Keras layers -- this layer is just for the sake of providing a concrete example):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 3s 3ms/step - loss: 2.4700\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1557b936700>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ActivityRegularizationLayer(layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        self.add_loss(tf.reduce_sum(inputs) * 0.1)\n",
    "        return inputs   # Pass-through layer.\n",
    "\n",
    "\n",
    "inputs = keras.Input(shape=(784,), name=\"digits\")\n",
    "x = layers.Dense(64, activation=\"relu\", name=\"dense_1\")(inputs)\n",
    "\n",
    "# Insert activity regularization as a layer\n",
    "x = ActivityRegularizationLayer()(x)\n",
    "\n",
    "x = layers.Dense(64, activation=\"relu\", name=\"dense_2\")(x)\n",
    "outputs = layers.Dense(10, name=\"predictions\")(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),)\n",
    "\n",
    "# The displayed loss will be much higher than before\n",
    "# due to the regularization component.\n",
    "model.fit(x_train, y_train, batch_size=64, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3 color=maroon>You can do the same for logging metric values, using `add_metric()`:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 3s 3ms/step - loss: 0.3483 - std_of_activation: 1.0416\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15598883d00>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MetricLoggingLayer(layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        # The `aggregation` argument defines how to aggregate the per-batch values over each epoch:\n",
    "        # in this case we simply average them.\n",
    "        self.add_metric(keras.backend.std(inputs), \n",
    "                        name=\"std_of_activation\", \n",
    "                        aggregation=\"mean\")\n",
    "        \n",
    "        return inputs  # Pass-through layer.\n",
    "\n",
    "\n",
    "inputs = keras.Input(shape=(784,), name=\"digits\")\n",
    "x = layers.Dense(64, activation=\"relu\", name=\"dense_1\")(inputs)\n",
    "\n",
    "# Insert std logging as a layer.\n",
    "x = MetricLoggingLayer()(x)\n",
    "\n",
    "x = layers.Dense(64, activation=\"relu\", name=\"dense_2\")(x)\n",
    "outputs = layers.Dense(10, name=\"predictions\")(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),)\n",
    "model.fit(x_train, y_train, batch_size=64, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3 color=maroon>In the [Functional API](https://www.tensorflow.org/guide/keras/functional/), you can also call `model.add_loss(loss_tensor)`, or `model.add_metric(metric_tensor, name, aggregation)`.</font>\n",
    "\n",
    "Here's a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 3s 3ms/step - loss: 2.5176 - std_of_activation: 0.0021\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1553540e310>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(784,), name=\"digits\")\n",
    "x1 = layers.Dense(64, activation=\"relu\", name=\"dense_1\")(inputs)\n",
    "x2 = layers.Dense(64, activation=\"relu\", name=\"dense_2\")(x1)\n",
    "outputs = layers.Dense(10, name=\"predictions\")(x2)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.add_loss(tf.reduce_sum(x1) * 0.1)\n",
    "\n",
    "model.add_metric(keras.backend.std(x1), name=\"std_of_activation\", aggregation=\"mean\")\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),)\n",
    "model.fit(x_train, y_train, batch_size=64, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3 color=maroon>**Note that** when you pass losses via `add_loss()`, it becomes possible to call `compile()` without a loss function, since the model already has a loss to minimize.</font>\n",
    "\n",
    "Consider the following `LogisticEndpoint` layer: it takes as inputs targets & logits, and it tracks a crossentropy loss via `add_loss()`. It also tracks classification accuracy via `add_metric()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticEndpoint(keras.layers.Layer):\n",
    "    def __init__(self, name=None):\n",
    "        super(LogisticEndpoint, self).__init__(name=name)\n",
    "        self.loss_fn = keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "        self.accuracy_fn = keras.metrics.BinaryAccuracy()\n",
    "\n",
    "    def call(self, targets, logits, sample_weights=None):\n",
    "        # Compute the training-time loss value and add it to the layer using `self.add_loss()`.\n",
    "        loss = self.loss_fn(targets, logits, sample_weights)\n",
    "        self.add_loss(loss)\n",
    "\n",
    "        # Log accuracy as a metric and add it to the layer using `self.add_metric()`.\n",
    "        acc = self.accuracy_fn(targets, logits, sample_weights)\n",
    "        self.add_metric(acc, name=\"accuracy\")\n",
    "\n",
    "        # Return the inference-time prediction tensor (for `.predict()`).\n",
    "        return tf.nn.softmax(logits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use it in a model with two inputs (input data & targets), compiled without a `loss` argument, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 227ms/step - loss: 1.0194 - binary_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15536c09c40>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "inputs = keras.Input(shape=(3,), name=\"inputs\")\n",
    "targets = keras.Input(shape=(10,), name=\"targets\")\n",
    "logits = keras.layers.Dense(10)(inputs)\n",
    "predictions = LogisticEndpoint(name=\"predictions\")(logits, targets) \n",
    "# 根据上面 call() 的定义，这里 logits 和 targets 两个实参的位置可能应该对调。\n",
    "\n",
    "model = keras.Model(inputs=[inputs, targets], outputs=predictions)\n",
    "model.compile(optimizer=\"adam\")  # No loss argument!\n",
    "\n",
    "data = {\"inputs\": np.random.random((3, 3)),\n",
    "        \"targets\": np.random.random((3, 10)),}\n",
    "model.fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information about training multi-input models, see the section **Passing data to multi-input, multi-output models**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatically setting apart a validation holdout set\n",
    "\n",
    "<font size=3 color=maroon>In the first end-to-end example you saw, we used the `validation_data` argument to pass a tuple of NumPy arrays `(x_val, y_val)` to the model for evaluating a validation loss and validation metrics at the end of each epoch.\n",
    "\n",
    "Here's another option: the argument `validation_split` allows you to automatically reserve part of your training data for validation. The argument value represents the fraction of the data to be reserved for validation, so it should be set to a number higher than 0 and lower than 1.</font><br> \n",
    "For instance, `validation_split=0.2` means \"use 20% of the data for validation\", and `validation_split=0.6` means \"use 60% of the data for validation\".\n",
    "\n",
    "<font size=3 color=maroon>The way the validation is computed is by taking the last x% samples of the arrays received by the `fit()` call, before any shuffling.\n",
    "\n",
    "**Note that** you can only use `validation_split` when training with NumPy data.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 786 calls to <function Model.make_train_function.<locals>.train_function at 0x0000015537E193A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "625/625 [==============================] - 3s 4ms/step - loss: 0.3664 - sparse_categorical_accuracy: 0.8959 - val_loss: 0.2408 - val_sparse_categorical_accuracy: 0.9285\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15537e147c0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_compiled_model()\n",
    "model.fit(x_train, y_train, batch_size=64, validation_split=0.2, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & evaluation from tf.data Datasets\n",
    "\n",
    "In the past few paragraphs, you've seen how to handle losses, metrics, and optimizers, and you've seen how to use the `validation_data` and `validation_split` arguments in `fit()`, <font size=3 color=maroon>when your data is passed as NumPy arrays.</font>\n",
    "\n",
    "Let's now take a look at the case where your <font size=3 color=maroon>data comes in the form of a `tf.data.Dataset` object.\n",
    "\n",
    "The `tf.data` API is a set of utilities in TensorFlow 2.0 for loading and preprocessing data in a way that's fast and scalable.</font>\n",
    "\n",
    "For a complete guide about creating `Datasets`, see the\n",
    "[tf.data documentation](https://www.tensorflow.org/guide/data).\n",
    "\n",
    "<font size=3 color=maroon>You can pass a `Dataset` instance directly to the methods `fit()`, `evaluate()`, and `predict()`:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.3417 - sparse_categorical_accuracy: 0.9012\n",
      "Epoch 2/3\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1611 - sparse_categorical_accuracy: 0.9516\n",
      "Epoch 3/3\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1190 - sparse_categorical_accuracy: 0.9636\n",
      "Evaluate\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.1236 - sparse_categorical_accuracy: 0.9633\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 0.12363167852163315,\n",
       " 'sparse_categorical_accuracy': 0.9632999897003174}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_compiled_model()\n",
    "\n",
    "# First, let's create a training Dataset instance.\n",
    "# For the sake of our example, we'll use the same MNIST data as before.\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "# Shuffle and slice the dataset.\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "# Now we get a test dataset.\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "test_dataset = test_dataset.batch(64)\n",
    "\n",
    "# Since the dataset already takes care of batching,\n",
    "# we don't pass a `batch_size` argument.\n",
    "model.fit(train_dataset, epochs=3)\n",
    "\n",
    "# You can also evaluate or predict on a dataset.\n",
    "print(\"Evaluate\")\n",
    "result = model.evaluate(test_dataset)\n",
    "dict(zip(model.metrics_names, result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3 color=maroon>**Note that** the Dataset is ***reset*** at the end of each epoch, so it can be reused of the next epoch.\n",
    "\n",
    "If you want to run training only on a specific number of batches from this Dataset, you can pass the `steps_per_epoch` argument, which specifies how many training steps the model should run using this Dataset before moving on to the next epoch.\n",
    "\n",
    "If you do this, the dataset is ***not reset*** at the end of each epoch, instead we just keep drawing the next batches. The dataset will eventually run out of data (unless it is an infinitely-looping dataset).</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "100/100 [==============================] - 1s 3ms/step - loss: 0.7559 - sparse_categorical_accuracy: 0.8083\n",
      "Epoch 2/3\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.3742 - sparse_categorical_accuracy: 0.8931\n",
      "Epoch 3/3\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.3279 - sparse_categorical_accuracy: 0.9053\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1553ac959d0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_compiled_model()\n",
    "\n",
    "# Prepare the training dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "# Only use the 100 batches per epoch (that's 64 * 100 samples)\n",
    "model.fit(train_dataset, epochs=3, steps_per_epoch=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a validation dataset\n",
    "\n",
    "You can pass a `Dataset` instance as the `validation_data` argument in `fit()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 3s 4ms/step - loss: 0.3357 - sparse_categorical_accuracy: 0.9055 - val_loss: 0.1812 - val_sparse_categorical_accuracy: 0.9482\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1553d0e7640>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_compiled_model()\n",
    "\n",
    "# Prepare the training dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "# Prepare the validation dataset\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "val_dataset = val_dataset.batch(64)\n",
    "\n",
    "model.fit(train_dataset, epochs=1, validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of each epoch, the model will iterate over the validation dataset and compute the validation loss and validation metrics.\n",
    "\n",
    "<font size=3 color=maroon>If you want to run validation only on a specific number of batches from this dataset, you can pass the `validation_steps` argument, which specifies how many validation steps the model should run with the validation dataset before interrupting validation\n",
    "and moving on to the next epoch:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 3s 4ms/step - loss: 0.3387 - sparse_categorical_accuracy: 0.9033 - val_loss: 0.3029 - val_sparse_categorical_accuracy: 0.9187\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1553d0e70d0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_compiled_model()\n",
    "\n",
    "# Prepare the training dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "# Prepare the validation dataset\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "val_dataset = val_dataset.batch(64)\n",
    "\n",
    "model.fit(train_dataset,\n",
    "          epochs=1,\n",
    "          # Only run validation using the first 10 batches of the dataset\n",
    "          # using the `validation_steps` argument\n",
    "          validation_data=val_dataset,\n",
    "          validation_steps=10,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3 color=maroon>**Note that** the validation dataset will be reset after each use (so that you will always be evaluating on the same samples from epoch to epoch).\n",
    "\n",
    "The argument `validation_split` (generating a holdout set from the training data) is not supported when training from `Dataset` objects, since this feature requires the ability to index the samples of the datasets, which is not possible in general with the `Dataset` API.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Other input formats supported**\n",
    "\n",
    "<font size=3 color=maroon>Besides NumPy arrays, eager tensors, and TensorFlow `Datasets`, it's possible to train a Keras model using Pandas dataframes, or from Python generators that yield batches of data & labels.\n",
    "\n",
    "In particular, the `keras.utils.Sequence` class offers a simple interface to build Python data generators that are multiprocessing-aware and can be shuffled.<br><br>\n",
    "\n",
    "***In general, we recommend that you use:***\n",
    "\n",
    "- NumPy input data if your data is small and fits in memory\n",
    "- `Dataset` objects if you have large datasets and you need to do distributed training\n",
    "- `Sequence` objects if you have large datasets and you need to do a lot of custom Python-side processing that cannot be done in TensorFlow (e.g. if you rely on external libraries for data loading or preprocessing).</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a `keras.utils.Sequence` object as input\n",
    "\n",
    "<font size=3 color=maroon>`keras.utils.Sequence` is a utility that you can subclass to obtain a Python generator with two important properties:\n",
    "\n",
    "- It works well with multiprocessing.\n",
    "- It can be shuffled (e.g. when passing `shuffle=True` in `fit()`).\n",
    "\n",
    "A `Sequence` must implement two methods:\n",
    "\n",
    "- `__getitem__`\n",
    "- `__len__`\n",
    "\n",
    "The method `__getitem__` should return a complete batch. \n",
    "If you want to modify your dataset between epochs, you may implement `on_epoch_end`.</font>\n",
    "\n",
    "\n",
    "\n",
    "Here's a quick example:\n",
    "\n",
    "```python\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "import numpy as np\n",
    "\n",
    "# Here, `filenames` is list of path to the images\n",
    "# and `labels` are the associated labels.\n",
    "\n",
    "class CIFAR10Sequence(Sequence):\n",
    "    def __init__(self, filenames, labels, batch_size):\n",
    "        self.filenames, self.labels = filenames, labels\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.filenames) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.filenames[idx*self.batch_size : (idx + 1)*self.batch_size]\n",
    "        batch_y = self.labels[idx*self.batch_size : (idx + 1)*self.batch_size]\n",
    "        return np.array([resize(imread(filename), (200, 200)) for filename in batch_x]),\n",
    "               np.array(batch_y)\n",
    "\n",
    "sequence = CIFAR10Sequence(filenames, labels, batch_size)\n",
    "model.fit(sequence, epochs=10)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using sample weighting and class weighting\n",
    "\n",
    "<font size=3 color=maroon>With the default settings the weight of a sample is decided by its frequency in the dataset. There are two methods to weight the data, independent of sample frequency:\n",
    "\n",
    "* Class weights\n",
    "* Sample weights</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class weights\n",
    "\n",
    "<font size=3 color=maroon>This is set by passing a dictionary to the `class_weight` argument to`Model.fit()`. This dictionary maps class indices to the weight that should be used for samples belonging to this class.\n",
    "\n",
    "This can be used to balance classes without resampling, or to train a model that gives more importance to a particular class.</font>\n",
    "\n",
    "For instance, if class \"0\" is half as represented as class \"1\" in your data, you could use `Model.fit(..., class_weight={0: 1., 1: 0.5})`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a NumPy example where we use class weights or sample weights to give more importance to the correct classification of class #5 (which is the digit \"5\" in the MNIST dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class_weight = {\n",
    "    0: 1.0,\n",
    "    1: 1.0,\n",
    "    2: 1.0,\n",
    "    3: 1.0,\n",
    "    4: 1.0,\n",
    "    # Set weight \"2\" for class \"5\",\n",
    "    # making this class 2x more important\n",
    "    5: 2.0,\n",
    "    6: 1.0,\n",
    "    7: 1.0,\n",
    "    8: 1.0,\n",
    "    9: 1.0,\n",
    "}\n",
    "\n",
    "print(\"Fit with class weight\")\n",
    "model = get_compiled_model()\n",
    "model.fit(x_train, y_train, class_weight=class_weight, batch_size=64, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample weights\n",
    "\n",
    "<font size=3 color=maroon>For fine grained control, or if you are not building a classifier, you can use \"sample weights\".\n",
    "\n",
    "- When training from NumPy data: Pass the `sample_weight`   argument to `Model.fit()`.\n",
    "- When training from `tf.data` or any other sort of iterator:   Yield `(input_batch, label_batch, sample_weight_batch)` tuples.\n",
    "\n",
    "A \"sample weights\" array is an array of numbers that specify how much weight each sample in a batch should have in computing the total loss. It is commonly used in imbalanced classification problems (the idea being to give more weight to rarely-seen classes).\n",
    "\n",
    "When the weights used are ones and zeros, the array can be used as a *mask* for the loss function (entirely discarding the contribution of certain samples to the total loss).</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit with sample weight\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.3775 - sparse_categorical_accuracy: 0.8999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15584d29d00>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_weight = np.ones(shape=(len(y_train),))\n",
    "sample_weight[y_train == 5] = 2.0\n",
    "\n",
    "print(\"Fit with sample weight\")\n",
    "model = get_compiled_model()\n",
    "model.fit(x_train, y_train, sample_weight=sample_weight, batch_size=64, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a matching `Dataset` example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 3s 3ms/step - loss: 0.3620 - sparse_categorical_accuracy: 0.9065\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1558d570fa0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_weight = np.ones(shape=(len(y_train),))\n",
    "sample_weight[y_train == 5] = 2.0\n",
    "\n",
    "# Create a Dataset that includes sample weights\n",
    "# (3rd element in the return tuple).\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train, sample_weight))\n",
    "\n",
    "# Shuffle and slice the dataset.\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "model = get_compiled_model()\n",
    "model.fit(train_dataset, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passing data to multi-input, multi-output models\n",
    "\n",
    "In the previous examples, we were considering a model with a single input (a tensor of shape `(764,)`) and a single output (a prediction tensor of shape `(10,)`). But what about models that have multiple inputs or outputs?\n",
    "\n",
    "Consider the following model, which has an image input of shape `(32, 32, 3)` (that's `(height, width, channels)`) and a time series input of shape `(None, 10)` (that's `(timesteps, features)`). Our model will have two outputs computed from the combination of these inputs: a \"score\" (of shape `(1,)`) and a probability distribution over five classes (of shape `(5,)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_input = keras.Input(shape=(32, 32, 3), name=\"img_input\")\n",
    "timeseries_input = keras.Input(shape=(None, 10), name=\"ts_input\")\n",
    "\n",
    "x1 = layers.Conv2D(3, 3)(image_input)\n",
    "x1 = layers.GlobalMaxPooling2D()(x1)\n",
    "\n",
    "x2 = layers.Conv1D(3, 3)(timeseries_input)\n",
    "x2 = layers.GlobalMaxPooling1D()(x2)\n",
    "\n",
    "x = layers.concatenate([x1, x2])\n",
    "\n",
    "score_output = layers.Dense(1, name=\"score_output\")(x)\n",
    "class_output = layers.Dense(5, name=\"class_output\")(x)\n",
    "\n",
    "model = keras.Model(inputs=[image_input, timeseries_input], \n",
    "                    outputs=[score_output, class_output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot this model, so you can clearly see what we're doing here (note that the shapes shown in the plot are batch shapes, rather than per-sample shapes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.utils.plot_model(model, \"plot_model/multi_input_and_output_model.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3 color=maroon>At compilation time, we can specify different losses to different outputs, by passing the loss functions as a list:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "              loss=[keras.losses.MeanSquaredError(), \n",
    "                    keras.losses.CategoricalCrossentropy()],\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3 color=maroon>If we only passed a single loss function to the model, the same loss function would be applied to every output (which is not appropriate here).</font>\n",
    "\n",
    "Likewise for metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "              loss=[keras.losses.MeanSquaredError(), keras.losses.CategoricalCrossentropy()],\n",
    "              metrics=[[keras.metrics.MeanAbsolutePercentageError(),\n",
    "                        keras.metrics.MeanAbsoluteError(),],\n",
    "                       \n",
    "                       [keras.metrics.CategoricalAccuracy()],\n",
    "                      ],\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3 color=maroon>Since we gave names to our output layers, we could also specify per-output losses and metrics via a dict:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "              \n",
    "              loss={\"score_output\": keras.losses.MeanSquaredError(),\n",
    "                    \"class_output\": keras.losses.CategoricalCrossentropy(),\n",
    "                   },\n",
    "              \n",
    "              metrics={\"score_output\": [keras.metrics.MeanAbsolutePercentageError(),\n",
    "                                        keras.metrics.MeanAbsoluteError(),],\n",
    "                       \"class_output\": [keras.metrics.CategoricalAccuracy()],\n",
    "                      },\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3 color=maroon>We recommend the use of explicit names and dicts if you have more than 2 outputs.\n",
    "\n",
    "It's possible to give different weights to different output-specific losses</font> (for instance, one might wish to privilege the \"score\" loss in our example, by giving to 2x the importance of the class loss), <font size=3 color=maroon>using the `loss_weights` argument:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "              loss={\"score_output\": keras.losses.MeanSquaredError(),\n",
    "                    \"class_output\": keras.losses.CategoricalCrossentropy(),\n",
    "                   },\n",
    "              \n",
    "              metrics={\"score_output\": [keras.metrics.MeanAbsolutePercentageError(),\n",
    "                                        keras.metrics.MeanAbsoluteError(),],\n",
    "                       \"class_output\": [keras.metrics.CategoricalAccuracy()],\n",
    "                      },\n",
    "              \n",
    "              loss_weights={\"score_output\": 2.0, \"class_output\": 1.0},\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3 color=maroon>You could also choose not to compute a loss for certain outputs, if these outputs are meant for prediction but not for training:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List loss version\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "              loss=[None, keras.losses.CategoricalCrossentropy()],)\n",
    "\n",
    "# Or dict loss version\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "              loss={\"class_output\": keras.losses.CategoricalCrossentropy()},)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3 color=maroon>Passing data to a multi-input or multi-output model in `fit()` works in a similar way as specifying a loss function in compile: \n",
    "* you can pass **lists of NumPy arrays** (with 1:1 mapping to the outputs that received a loss function) \n",
    "* or **dicts mapping output names to NumPy arrays**.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 4s 20ms/step - loss: 12.0104 - score_output_loss: 0.5911 - class_output_loss: 11.4193\n",
      "4/4 [==============================] - 1s 7ms/step - loss: 11.5518 - score_output_loss: 0.2472 - class_output_loss: 11.3045\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1558f27ebe0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "              loss=[keras.losses.MeanSquaredError(), keras.losses.CategoricalCrossentropy()],)\n",
    "\n",
    "# Generate dummy NumPy data\n",
    "img_data = np.random.random_sample(size=(100, 32, 32, 3))\n",
    "ts_data = np.random.random_sample(size=(100, 20, 10))\n",
    "score_targets = np.random.random_sample(size=(100, 1))\n",
    "class_targets = np.random.random_sample(size=(100, 5))\n",
    "\n",
    "# Fit on lists\n",
    "model.fit([img_data, ts_data], [score_targets, class_targets], batch_size=32, epochs=1)\n",
    "\n",
    "# Alternatively, fit on dicts\n",
    "model.fit({\"img_input\": img_data, \"ts_input\": ts_data},\n",
    "          {\"score_output\": score_targets, \"class_output\": class_targets},\n",
    "          batch_size=32,\n",
    "          epochs=1,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the `Dataset` use case: similarly as what we did for NumPy arrays, the `Dataset` should return a tuple of dicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 45ms/step - loss: 11.4452 - score_output_loss: 0.2102 - class_output_loss: 11.2350\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1553d425a90>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices(({\"img_input\": img_data, \"ts_input\": ts_data},\n",
    "                                                    \n",
    "                                                    {\"score_output\": score_targets,\n",
    "                                                     \"class_output\": class_targets},\n",
    "                                                   ))\n",
    "\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "model.fit(train_dataset, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using callbacks\n",
    "\n",
    "<font size=3 color=maroon>Callbacks in Keras are objects that are called at different points during training (at the start of an epoch, at the end of a batch, at the end of an epoch, etc.). They can be used to implement certain behaviors, such as:\n",
    "\n",
    "- Doing validation at different points during training (beyond the built-in per-epoch validation)\n",
    "- Checkpointing the model at regular intervals or when it exceeds a certain accuracy threshold\n",
    "- Changing the learning rate of the model when training seems to be plateauing\n",
    "- Doing fine-tuning of the top layers when training seems to be plateauing\n",
    "- Sending email or instant message notifications when training ends or where a certain performance threshold is exceeded\n",
    "- Etc.\n",
    "\n",
    "Callbacks can be passed as a list to your call to `fit()`:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "625/625 [==============================] - 3s 4ms/step - loss: 0.3712 - sparse_categorical_accuracy: 0.8961 - val_loss: 0.2337 - val_sparse_categorical_accuracy: 0.9329\n",
      "Epoch 2/20\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.1722 - sparse_categorical_accuracy: 0.9494 - val_loss: 0.2016 - val_sparse_categorical_accuracy: 0.9381\n",
      "Epoch 3/20\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.1264 - sparse_categorical_accuracy: 0.9622 - val_loss: 0.1635 - val_sparse_categorical_accuracy: 0.9521\n",
      "Epoch 4/20\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.0981 - sparse_categorical_accuracy: 0.9706 - val_loss: 0.1533 - val_sparse_categorical_accuracy: 0.9577\n",
      "Epoch 5/20\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.0812 - sparse_categorical_accuracy: 0.9753 - val_loss: 0.1513 - val_sparse_categorical_accuracy: 0.9586\n",
      "Epoch 6/20\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.0686 - sparse_categorical_accuracy: 0.9794 - val_loss: 0.1446 - val_sparse_categorical_accuracy: 0.9593\n",
      "Epoch 6: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1553ccb2e50>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_compiled_model()\n",
    "\n",
    "callbacks = [keras.callbacks.EarlyStopping(# Stop training when `val_loss` is no longer improving\n",
    "                                           monitor=\"val_loss\",\n",
    "                                           # \"no longer improving\" being defined as \"no better than 1e-2 less\"\n",
    "                                           min_delta=1e-2,\n",
    "                                           # \"no longer improving\" being further defined as \"for at least 2 epochs\"\n",
    "                                           patience=2,\n",
    "                                           verbose=1,\n",
    "                                           )\n",
    "            ]\n",
    "\n",
    "\n",
    "model.fit(x_train,\n",
    "          y_train,\n",
    "          epochs=20,\n",
    "          batch_size=64,\n",
    "          callbacks=callbacks,\n",
    "          validation_split=0.2,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Many built-in callbacks are available\n",
    "\n",
    "There are many built-in callbacks already available in Keras, such as:\n",
    "\n",
    "- `ModelCheckpoint`: Periodically save the model.\n",
    "- `EarlyStopping`: Stop training when training is no longer improving the validation metrics.\n",
    "- `TensorBoard`: periodically write model logs that can be visualized in [TensorBoard](https://www.tensorflow.org/tensorboard) (more details in the section \"Visualization\").\n",
    "- `CSVLogger`: streams loss and metrics data to a CSV file.\n",
    "- etc.\n",
    "\n",
    "See the [callbacks documentation](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/) for the complete list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing your own callback\n",
    "\n",
    "<font size=3 color=maroon>You can create a custom callback by extending the base class `keras.callbacks.Callback`. A callback has access to its associated model through the class property `self.model`.\n",
    "\n",
    "Make sure to read the [complete guide to writing custom callbacks](https://www.tensorflow.org/guide/keras/custom_callback/).</font>\n",
    "\n",
    "\n",
    "Here's a simple example saving a list of per-batch loss values during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs):\n",
    "        self.per_batch_losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs):\n",
    "        self.per_batch_losses.append(logs.get(\"loss\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpointing models\n",
    "\n",
    "<font size=3 color=maroon>When you're training model on relatively large datasets, it's crucial to save checkpoints of your model at frequent intervals.</font>\n",
    "\n",
    "The easiest way to achieve this is with the `ModelCheckpoint` callback:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "621/625 [============================>.] - ETA: 0s - loss: 0.3696 - sparse_categorical_accuracy: 0.8956\n",
      "Epoch 1: val_loss improved from inf to 0.24433, saving model to mymodel_1\n",
      "INFO:tensorflow:Assets written to: mymodel_1\\assets\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.3687 - sparse_categorical_accuracy: 0.8959 - val_loss: 0.2443 - val_sparse_categorical_accuracy: 0.9245\n",
      "Epoch 2/2\n",
      "624/625 [============================>.] - ETA: 0s - loss: 0.1698 - sparse_categorical_accuracy: 0.9501\n",
      "Epoch 2: val_loss improved from 0.24433 to 0.17663, saving model to mymodel_2\n",
      "INFO:tensorflow:Assets written to: mymodel_2\\assets\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.1698 - sparse_categorical_accuracy: 0.9500 - val_loss: 0.1766 - val_sparse_categorical_accuracy: 0.9483\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x155395d8b50>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_compiled_model()\n",
    "\n",
    "callbacks = [keras.callbacks.ModelCheckpoint(\n",
    "                                # Path where to save the model.\n",
    "                                # The two parameters below mean that we will overwrite\n",
    "                                # the current checkpoint if and only if the `val_loss` score has improved.\n",
    "                                # The saved model name will include the current epoch.\n",
    "                                filepath=\"mymodel_{epoch}\",\n",
    "                                save_best_only=True,  # Only save a model if `val_loss` has improved.\n",
    "                                monitor=\"val_loss\",\n",
    "                                verbose=1,)\n",
    "            ]\n",
    "\n",
    "\n",
    "model.fit(x_train,\n",
    "          y_train, \n",
    "          epochs=2, \n",
    "          batch_size=64, \n",
    "          callbacks=callbacks, \n",
    "          validation_split=0.2\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3 color=maroon>The `ModelCheckpoint` callback can be used to implement fault-tolerance:<br>\n",
    "the ability to restart training from the last saved state of the model in case training gets randomly interrupted.</font> \n",
    "\n",
    "Here's a basic example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a new model\n",
      "  91/1563 [>.............................] - ETA: 5s - loss: 1.0042 - sparse_categorical_accuracy: 0.7256INFO:tensorflow:Assets written to: ./ckpt\\ckpt-loss=0.95\\assets\n",
      " 191/1563 [==>...........................] - ETA: 8s - loss: 0.7116 - sparse_categorical_accuracy: 0.8033INFO:tensorflow:Assets written to: ./ckpt\\ckpt-loss=0.70\\assets\n",
      " 291/1563 [====>.........................] - ETA: 8s - loss: 0.5916 - sparse_categorical_accuracy: 0.8345INFO:tensorflow:Assets written to: ./ckpt\\ckpt-loss=0.58\\assets\n",
      " 393/1563 [======>.......................] - ETA: 8s - loss: 0.5229 - sparse_categorical_accuracy: 0.8525INFO:tensorflow:Assets written to: ./ckpt\\ckpt-loss=0.52\\assets\n",
      " 499/1563 [========>.....................] - ETA: 8s - loss: 0.4754 - sparse_categorical_accuracy: 0.8655INFO:tensorflow:Assets written to: ./ckpt\\ckpt-loss=0.48\\assets\n",
      " 583/1563 [==========>...................] - ETA: 8s - loss: 0.4450 - sparse_categorical_accuracy: 0.8729INFO:tensorflow:Assets written to: ./ckpt\\ckpt-loss=0.44\\assets\n",
      " 686/1563 [============>.................] - ETA: 7s - loss: 0.4150 - sparse_categorical_accuracy: 0.8813INFO:tensorflow:Assets written to: ./ckpt\\ckpt-loss=0.41\\assets\n",
      " 784/1563 [==============>...............] - ETA: 6s - loss: 0.3948 - sparse_categorical_accuracy: 0.8867INFO:tensorflow:Assets written to: ./ckpt\\ckpt-loss=0.39\\assets\n",
      " 883/1563 [===============>..............] - ETA: 5s - loss: 0.3764 - sparse_categorical_accuracy: 0.8922INFO:tensorflow:Assets written to: ./ckpt\\ckpt-loss=0.37\\assets\n",
      " 988/1563 [=================>............] - ETA: 4s - loss: 0.3592 - sparse_categorical_accuracy: 0.8965INFO:tensorflow:Assets written to: ./ckpt\\ckpt-loss=0.36\\assets\n",
      "1083/1563 [===================>..........] - ETA: 4s - loss: 0.3465 - sparse_categorical_accuracy: 0.8999INFO:tensorflow:Assets written to: ./ckpt\\ckpt-loss=0.34\\assets\n",
      "1188/1563 [=====================>........] - ETA: 3s - loss: 0.3312 - sparse_categorical_accuracy: 0.9040INFO:tensorflow:Assets written to: ./ckpt\\ckpt-loss=0.33\\assets\n",
      "1286/1563 [=======================>......] - ETA: 2s - loss: 0.3201 - sparse_categorical_accuracy: 0.9071INFO:tensorflow:Assets written to: ./ckpt\\ckpt-loss=0.32\\assets\n",
      "1384/1563 [=========================>....] - ETA: 1s - loss: 0.3107 - sparse_categorical_accuracy: 0.9098INFO:tensorflow:Assets written to: ./ckpt\\ckpt-loss=0.31\\assets\n",
      "1485/1563 [===========================>..] - ETA: 0s - loss: 0.3021 - sparse_categorical_accuracy: 0.9122INFO:tensorflow:Assets written to: ./ckpt\\ckpt-loss=0.30\\assets\n",
      "1563/1563 [==============================] - 14s 9ms/step - loss: 0.2956 - sparse_categorical_accuracy: 0.9141\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15596f67640>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Prepare a directory to store all the checkpoints.\n",
    "checkpoint_dir = \"./ckpt\"\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "\n",
    "\n",
    "def make_or_restore_model():\n",
    "    # Either restore the latest model, or create a fresh one\n",
    "    # if there is no checkpoint available.\n",
    "    checkpoints = [checkpoint_dir + \"/\" + name for name in os.listdir(checkpoint_dir)]\n",
    "    \n",
    "    if checkpoints:\n",
    "        latest_checkpoint = max(checkpoints, key=os.path.getctime)\n",
    "        print(\"Restoring from\", latest_checkpoint)\n",
    "        return keras.models.load_model(latest_checkpoint)\n",
    "    \n",
    "    print(\"Creating a new model\")\n",
    "    return get_compiled_model()\n",
    "\n",
    "\n",
    "model = make_or_restore_model()\n",
    "callbacks = [# This callback saves a SavedModel every 100 batches.\n",
    "             # We include the training loss in the saved model name.\n",
    "             keras.callbacks.ModelCheckpoint(filepath=checkpoint_dir + \"/ckpt-loss={loss:.2f}\", \n",
    "                                             save_freq=100\n",
    "                                            )\n",
    "             ]\n",
    "\n",
    "model.fit(x_train, y_train, epochs=1, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3 color=maroon>You call also write your own callback for saving and restoring models.\n",
    "\n",
    "For a complete guide on serialization and saving, see the [guide to saving and serializing Models](https://www.tensorflow.org/guide/keras/save_and_serialize/).</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using learning rate schedules\n",
    "\n",
    "A common pattern when training deep learning models is to gradually reduce the learning as training progresses. This is generally known as \"<font color=blue>**learning rate decay**</font>\".\n",
    "\n",
    "The learning decay schedule could be static (fixed in advance, as a function of the current epoch or the current batch index), or dynamic (responding to the current behavior of the model, in particular the validation loss)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passing a schedule to an optimizer\n",
    "\n",
    "You can easily use a static learning rate decay schedule by passing a schedule object as the `learning_rate` argument in your optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_learning_rate = 0.1\n",
    "\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(initial_learning_rate, \n",
    "                                                          decay_steps=100000, \n",
    "                                                          decay_rate=0.96, \n",
    "                                                          staircase=True)\n",
    "\n",
    "optimizer = keras.optimizers.RMSprop(learning_rate=lr_schedule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3 color=maroon>Several built-in schedules are available: `ExponentialDecay`, `PiecewiseConstantDecay`, `PolynomialDecay`, and `InverseTimeDecay`.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using callbacks to implement a dynamic learning rate schedule\n",
    "\n",
    "A dynamic learning rate schedule (for instance, decreasing the learning rate when the\n",
    "validation loss is no longer improving) cannot be achieved with these schedule objects,\n",
    "since the optimizer does not have access to validation metrics.\n",
    "\n",
    "However, callbacks do have access to all metrics, including validation metrics! You can\n",
    "thus achieve this pattern by using a callback that modifies the current learning rate\n",
    "on the optimizer. In fact, this is even built-in as the `ReduceLROnPlateau` callback."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing loss and metrics during training\n",
    "\n",
    "The best way to keep an eye on your model during training is to use [TensorBoard](https://www.tensorflow.org/tensorboard) -- a browser-based application that you can run locally that provides you with:\n",
    "\n",
    "- Live plots of the loss and metrics for training and evaluation\n",
    "- (optionally) Visualizations of the histograms of your layer activations\n",
    "- (optionally) 3D visualizations of the embedding spaces learned by your `Embedding` layers\n",
    "\n",
    "\n",
    "<font size=3 color=maroon>If you have installed TensorFlow with pip, you should be able to launch TensorBoard from the command line:</font>\n",
    "\n",
    "```cmd\n",
    "tensorboard --logdir=/full_path_to_your_logs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the TensorBoard callback\n",
    "\n",
    "The easiest way to use TensorBoard with a Keras model and the `fit()` method is the `TensorBoard` callback.\n",
    "\n",
    "In the simplest case, just specify where you want the callback to write logs, and you're good to go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.TensorBoard at 0x155399ebf40>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.callbacks.TensorBoard(log_dir=\"./logs_tensorboard\",\n",
    "                            histogram_freq=0,  # How often to log histogram visualizations\n",
    "                            embeddings_freq=0,  # How often to log embedding visualizations\n",
    "                            update_freq=\"epoch\",\n",
    "                           )  # How often to write logs (default: once per epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information, see the [documentation for the `TensorBoard` callback](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/tensorboard/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3 color=maroon>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tfg]",
   "language": "python",
   "name": "conda-env-tfg-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "497px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
